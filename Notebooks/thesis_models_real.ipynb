{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aM2TdxbYGNtJ"
      },
      "outputs": [],
      "source": [
        "##Import required packages\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.utils import shuffle\n",
        "import jax.numpy as jnp\n",
        "import jaxlib\n",
        "from jax import jit\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "from sklearn.metrics import make_scorer, mean_squared_error\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.ensemble import HistGradientBoostingRegressor\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBNSktJNG7Du"
      },
      "source": [
        "# **Pre-Processing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kJBAVk7PG98N"
      },
      "outputs": [],
      "source": [
        "merged_data = pd.read_csv('fully_merged_2020.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9CIwjTImwfxq"
      },
      "outputs": [],
      "source": [
        "merged_data = merged_data.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UjM_BPoOIyjJ",
        "outputId": "56a390b5-9171-4683-f5ea-6bc0e75aaf91"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Unnamed: 0', 'date', 'temp', 'pressure', 'salinity', 'lat', 'lon', 'ice_conc', 'height', 'windspeed', 'precipitation']\n"
          ]
        }
      ],
      "source": [
        "print(merged_data.columns.tolist())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUsoH1wSIru6"
      },
      "source": [
        "**Step 1 - set up arrays**\n",
        "\n",
        "*   Split dataframe into 2 data frames, one for features(temp, pressure, lat, lon, ice_conc, height & windspeed) and for label (salinity).\n",
        "*   Convert to arrays for JAX\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dgRFhz_5I9np"
      },
      "outputs": [],
      "source": [
        "features = merged_data.drop(['salinity', 'date'], axis=1)\n",
        "labels = merged_data['salinity']\n",
        "feature_array = features.to_numpy()\n",
        "label_array = labels.to_numpy()\n",
        "X = feature_array\n",
        "Y = label_array"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbIxXgb_MPUz"
      },
      "source": [
        "**Step 2 - get arrays ready for ML models**\n",
        "\n",
        "* scale so that zero mean and unit variance (i.e. standardised distribution)\n",
        "* split into train, validation, and test data (0.5 train, 0.25 validation, 0.25 test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "li298t4VMzmB"
      },
      "outputs": [],
      "source": [
        "scaler = StandardScaler()\n",
        "x_scaled = scaler.fit_transform(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "muUl6TKFNBzN"
      },
      "outputs": [],
      "source": [
        "x_train, x_temp, y_train, y_temp = train_test_split(x_scaled, Y, test_size=0.5, random_state=42)\n",
        "x_val, x_test, y_val, y_test = train_test_split(x_temp, y_temp, test_size=0.5, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dxz8wyidJ2Yn"
      },
      "source": [
        "**Step 3 - Make new dataframes with one randomised feature each for feature importance testing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jWUzJY9SKCVs"
      },
      "outputs": [],
      "source": [
        "##Shuffled windspeed column\n",
        "x_train_windspeed = np.copy(x_train)\n",
        "shuffled_windspeed = shuffle(x_train_windspeed[:, 7], random_state=42)\n",
        "x_train_windspeed[:, 7] = shuffled_windspeed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-vBg7avPlsm"
      },
      "outputs": [],
      "source": [
        "##Shuffled precip column\n",
        "x_train_precip = np.copy(x_train)\n",
        "shuffled_precip = shuffle(x_train_precip[:, 8], random_state=42)\n",
        "x_train_precip[:, 8] = shuffled_precip\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ag7-kbmLPtLR"
      },
      "outputs": [],
      "source": [
        "##Shuffled ice_conc column\n",
        "x_train_ice_conc = np.copy(x_train)\n",
        "shuffled_ice_conc = shuffle(x_train_ice_conc[:, 5], random_state=42)\n",
        "x_train_ice_conc[:, 5] = shuffled_ice_conc\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d7jmpXGBPuom"
      },
      "outputs": [],
      "source": [
        "##Shuffled height column\n",
        "x_train_height = np.copy(x_train)\n",
        "shuffled_height = shuffle(x_train_height[:, 6], random_state=42)\n",
        "x_train_height[:, 6] = shuffled_height\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ifNdx_iPv5A"
      },
      "outputs": [],
      "source": [
        "##Shuffled lat column\n",
        "x_train_lat = np.copy(x_train)\n",
        "shuffled_lat = shuffle(x_train_lat[:, 3], random_state=42)\n",
        "x_train_lat[:, 3] = shuffled_lat\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jE5JFtoPPxJ6"
      },
      "outputs": [],
      "source": [
        "##Shuffled lon column\n",
        "x_train_lon = np.copy(x_train)\n",
        "shuffled_lon = shuffle(x_train_lon[:, 4], random_state=42)\n",
        "x_train_lon[:, 4] = shuffled_lon\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pOX7meBWPyKG"
      },
      "outputs": [],
      "source": [
        "##Shuffled pressure column\n",
        "x_train_pressure = np.copy(x_train)\n",
        "shuffled_pressure = shuffle(x_train_pressure[:, 2], random_state=42)\n",
        "x_train_pressure[:, 2] = shuffled_pressure\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QwBMCXduPzmP"
      },
      "outputs": [],
      "source": [
        "##Shuffled temp column\n",
        "x_train_temp = np.copy(x_train)\n",
        "shuffled_temp = shuffle(x_train_temp[:, 1], random_state=42)\n",
        "x_train_temp[:, 1] = shuffled_temp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GQn2XqNG1HM"
      },
      "source": [
        "# **Model 1 - KNN**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XrzkW2bpGyUl"
      },
      "outputs": [],
      "source": [
        "##Define the necessary functions\n",
        "\n",
        "@jit\n",
        "def euclidean_distance(x1, x2):\n",
        "  return jnp.sqrt(jnp.sum((x1-x2)**2))\n",
        "\n",
        "def rmse(y_true, y_pred):\n",
        "    return jnp.sqrt(jnp.mean((y_true - y_pred) ** 2))\n",
        "\n",
        "def knn_predict(x_train, y_train, x_test, k=3):\n",
        "  # Calculate distances from each x_test to all x_train\n",
        "  distances = jnp.array([[euclidean_distance(x_test_i, x_train_i) for x_train_i in x_train] for x_test_i in x_test])\n",
        "\n",
        "  # Find the indices of the k nearest neighbors\n",
        "  k_nearest_indices = jnp.argpartition(distances, kth=k, axis=1)[:, :k]\n",
        "\n",
        "  # Predict by averaging the labels of the k nearest neighbors\n",
        "  predictions = jnp.array([jnp.mean(y_train[k_indices]) for k_indices in k_nearest_indices])\n",
        "  return predictions\n",
        "\n",
        "def batched_knn_predict(x_train, y_train, x_test, k=3, batch_size=32):\n",
        "  predictions = []\n",
        "\n",
        "  for i in range(0, len(x_test), batch_size):\n",
        "    batch_x_test = x_test[i:i + batch_size]\n",
        "    batch_distances = jnp.array([[euclidean_distance(x_test_i, x_train_i) for x_train_i in x_train] for x_test_i in batch_x_test])\n",
        "    k_nearest_indices = jnp.argpartition(batch_distances, kth=k, axis=1)[:, :k]\n",
        "    batch_predictions = jnp.array([jnp.mean(y_train[k_indices]) for k_indices in k_nearest_indices])\n",
        "    predictions.append(batch_predictions)\n",
        "    return jnp.concatenate(predictions, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g2jFe0RqQHZU"
      },
      "outputs": [],
      "source": [
        "##Need to do hyperparamter tuning to find the best value for k\n",
        "#involves running the model on validation data, iterating over a range of ks to find the one with lowest mse and then using this k on test data\n",
        "\n",
        "k_values = range(1,5)\n",
        "rmse_values = []\n",
        "\n",
        "for k in k_values:\n",
        "  y_pred = batched_knn_predict(x_train, y_train, x_val, k, batch_size=32)\n",
        "  error = rmse(y_val, y_pred)\n",
        "  rmse_values.append(error)\n",
        "\n",
        "best_k_index = jnp.argmin(rmse_values)\n",
        "best_k = k_values[best_k_index]\n",
        "knn_val_rmse = rmse_values[best_k_index]\n",
        "print(f\"Best Value for k: {best_k} with lowest RMSE: {knn_val_rmse}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_val = batched_knn_predict(x_train, y_train, x_val, k=3, batch_size=32)\n",
        "knn_val_rmse = rmse(y_val, y_pred_val)"
      ],
      "metadata": {
        "id": "lca8mjV7Qgem"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IEB-fK0yONha"
      },
      "outputs": [],
      "source": [
        "##Run model on test data\n",
        "predictions_test = knn_predict(x_train, y_train, x_test, k=best_k)\n",
        "knn_test_rmse = rmse(y_test, predictions_test)\n",
        "\n",
        "#print RMSE\n",
        "print(f\"KNN Validation RMSE: {knn_val_rmse}\")\n",
        "print(f\"KNN Test RMSE: {knn_test_rmse}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKZJ3Nq6ROIi"
      },
      "source": [
        "# **Model 2 - SVR**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Bo3yN5nPfUn"
      },
      "outputs": [],
      "source": [
        "##Define functions for kernel trick & computing kernel matrix K\n",
        "@jit\n",
        "#Define Matern 3/2 Kernel\n",
        "def matern32_kernel(X1, X2, sigma, rho):\n",
        "  #d is Euclidean Distance\n",
        "  d = jnp.sqrt(jnp.sum((X1[:, None, :] - X2[None, :, :]) ** 2, axis=2))\n",
        "  matern = (1 + jnp.sqrt(3) * d / rho) * jnp.exp(-jnp.sqrt(3) * d / rho)\n",
        "  return sigma ** 2 * matern\n",
        "\n",
        "#Define K - make K matrix of K(xi, xj) -> to make k matrix\n",
        "def K(X, sigma, rho):\n",
        "  return matern32_kernel(X, X, sigma, rho)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hxXZGSrFOltT"
      },
      "outputs": [],
      "source": [
        "##Set up the optimisation problem (using JAX and convert to CVXOPT matrixes)\n",
        "#Parameters\n",
        "C = 1.0\n",
        "sigma = 1.0\n",
        "rho = 1.0\n",
        "\n",
        "#Compute kernel matrix k\n",
        "K = K(x_train, sigma, rho)\n",
        "\n",
        "#Form the matrixes required for dual problem\n",
        "n = x_train.shape[0]\n",
        "p = jnp.outer(y_train, y_train) * K\n",
        "q = jnp.ones(n)\n",
        "\n",
        "#Converts from JAX array to numpy array to CVXOPT matrix\n",
        "p = matrix(p.numpy())\n",
        "q = matrix(q.numpy())\n",
        "\n",
        "#Introduce inequality constraints\n",
        "G = matrix(np.vstack((-np.eye(n), np.eye(n))))\n",
        "h = matrix(np.hstack((np.zeros(n), np.ones(n) * C)))\n",
        "\n",
        "#Introduce equality constraints\n",
        "A = matrix(y.reshape(1, -1), tc='d')\n",
        "b = matrix(0.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LLustJjdihRy"
      },
      "outputs": [],
      "source": [
        "##Solve quadratic programming problem -> \\min_{x} \\frac{1}{2} x^T P x + q^T x\n",
        "solution = solvers.qp(P, q, G, h, A, b)\n",
        "alphas = np.array(solution['x']).flatten()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xUyNaqNHmVot"
      },
      "outputs": [],
      "source": [
        "##Build Model\n",
        "#convert to jax arrays\n",
        "x_train_jax = jnp.array(x_train)\n",
        "y_train_jax = jnp.array(y_train)\n",
        "alphas_jax = jnp.array(alphas)\n",
        "\n",
        "#Identify support vectors (i.e. values that had non-zero alpha)\n",
        "threshold = 1e-5\n",
        "support_vector_indices = jnp.where(alphas_jax > threshold)[0]\n",
        "x_sv = x_train_jax[support_vector_indices]\n",
        "y_sv = y_train_jax[support_vector_indices]\n",
        "alphas_sv = alphas_jax[support_vector_indices]\n",
        "\n",
        "#Compute bias term\n",
        "def compute_bias(x_sv, y_sv, alphas_sv, kernel_func, sigma, rho):\n",
        "    # Compute the kernel product for support vectors\n",
        "    K_sv = kernel_func(x_sv, x_sv, sigma, rho)\n",
        "    # Compute the sum for each support vector\n",
        "    sums = jnp.sum(alphas_sv * y_sv * K_sv, axis=1)\n",
        "    # Compute b as the average difference between target and sum\n",
        "    b = jnp.mean(y_sv - sums)\n",
        "    return b\n",
        "\n",
        "b = compute_bias(X_sv, y_sv, alphas_sv, matern32_kernel, sigma, rho)\n",
        "\n",
        "#Define prediction function\n",
        "def predict(x_new, x_sv, y_sv, alphas_sv, b, kernel_func, sigma, rho):\n",
        "    # Compute the kernel between new data points and support vectors\n",
        "    k_new_sv = kernel_func(x_new, x_sv, sigma, rho)\n",
        "    # Compute the decision function\n",
        "    decision_values = jnp.dot((alphas_sv * y_sv), k_new_sv.T) + b\n",
        "    # Return the decision values for SVR or the sign for SVM classification\n",
        "    return decision_values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3i1NXWMMn6EF"
      },
      "outputs": [],
      "source": [
        "##Make predictions for validation & test data\n",
        "val_predictions = predict(x_val, x_sv, y_sv, alphas_sv, b, matern32_kernel, sigma, rho)\n",
        "svr_val_rmse = rmse(y_val, val_predictions)\n",
        "\n",
        "test_predictions = predict(x_test, x_sv, y_sv, alphas_sv, b, matern32_kernel, sigma, rho)\n",
        "svr_test_rmse = rmse(y_test, test_predictions)\n",
        "\n",
        "print(f\"SVR Validation RMSE: {svr_val_rmse}\")\n",
        "print(f\"SVR Test RMSE: {svr_test_rmse}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gY7RdjuXNTE5"
      },
      "outputs": [],
      "source": [
        "svr = SVR(kernel='linear')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iee0lB1V2vyB"
      },
      "outputs": [],
      "source": [
        "sample_size = int(len(x_train) * 0.5)  # Adjust the 0.5 as necessary for the fraction you want\n",
        "\n",
        "# Generate random indices\n",
        "indices = np.random.choice(len(x_train), size=sample_size, replace=False)\n",
        "\n",
        "# Subset the data\n",
        "x_train_sub = x_train[indices]\n",
        "y_train_sub = y_train[indices]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "xWYXdWFVKnoZ",
        "outputId": "e569915b-c118-4047-88aa-c03c6494cba0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n"
          ]
        }
      ],
      "source": [
        "y_train_sub = y_train_sub.ravel()  # Ensuring y_train is a 1D array\n",
        "param_distributions = {\n",
        "    'C': [0.1, 1, 10],\n",
        "    'epsilon': [0.01, 0.1, 1]\n",
        "}\n",
        "random_search = RandomizedSearchCV(estimator = svr, param_distributions=param_distributions, cv=5, scoring='neg_mean_squared_error', verbose=1, n_jobs=-1, random_state=42)\n",
        "random_search.fit(x_train_sub, y_train_sub)\n",
        "\n",
        "print(\"Best parameters:\", random_search.best_params_)\n",
        "best_params = random_search.best_params_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oB0cAgcvSYnO"
      },
      "source": [
        "# **Model 3 - RF**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RIy7N92TSaja"
      },
      "outputs": [],
      "source": [
        "##Set up RFRegressor model from SKLearn\n",
        "rf_regressor = RandomForestRegressor(random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zum0gESlTlfC",
        "outputId": "7c8125b9-6f24-42b3-c421-d817d206617f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/joblib/externals/loky/backend/fork_exec.py:38: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  pid = os.fork()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best No. of Trees: 100\n",
            "Max depth of tree: 10\n",
            "Min no of samples required to split internal node: 5\n"
          ]
        }
      ],
      "source": [
        "##Hyperparameter tuning using GridSearch to find the best no. of trees in the forest, max depth of tree, and min no of samples required to split internal node\n",
        "\n",
        "#Set up the parameter grid for hyperparameters to be tune\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [None, 10, 20],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "\n",
        "#Set up the grid search\n",
        "grid_search = GridSearchCV(estimator=rf_regressor, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error', verbose=1, n_jobs=-1)\n",
        "\n",
        "#Run the grid search\n",
        "grid_search.fit(x_train, y_train)\n",
        "\n",
        "#Find the best parameters\n",
        "best_n_estimators = grid_search.best_params_['n_estimators']\n",
        "best_max_depth = grid_search.best_params_['max_depth']\n",
        "best_min_samples_split = grid_search.best_params_['min_samples_split']\n",
        "\n",
        "print(f\"Best No. of Trees: {best_n_estimators}\")\n",
        "print(f\"Max depth of tree: {best_max_depth}\")\n",
        "print(f\"Min no of samples required to split internal node: {best_min_samples_split}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HtURUu9xqkQF",
        "outputId": "4380b610-ed92-427d-ca38-fd672a386f20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:   29.5s\n",
            "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    0.3s\n",
            "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    0.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest Validation RMSE: 2.9265762350405566e-05\n",
            "Random Forest Test RMSE: 9.943395298250834e-07\n"
          ]
        }
      ],
      "source": [
        "##Run model on validation data and test data\n",
        "#Set up regressor with parameters found in tuning\n",
        "rf_regressor = RandomForestRegressor(n_estimators=best_n_estimators,\n",
        "                                     max_depth=best_max_depth,\n",
        "                                     min_samples_split=best_min_samples_split,\n",
        "                                     random_state=42,\n",
        "                                     verbose=1)\n",
        "#Fit model to training data\n",
        "rf_regressor.fit(x_train, y_train)\n",
        "#Predict on validation data\n",
        "y_val_pred = rf_regressor.predict(x_val)\n",
        "#Predict on test data\n",
        "y_test_pred = rf_regressor.predict(x_test)\n",
        "\n",
        "#Validation & test RMSE (NOTE: uses rmse function defined in KNN section)\n",
        "rf_val_rmse = rmse(y_val, y_val_pred)\n",
        "rf_test_rmse = rmse(y_test, y_test_pred)\n",
        "\n",
        "print(f\"Random Forest Validation RMSE: {rf_val_rmse}\")\n",
        "print(f\"Random Forest Test RMSE: {rf_test_rmse}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Qt_Fyk0QS5p"
      },
      "source": [
        "**Feature Testing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xe2tU5jnQSWT",
        "outputId": "2a655795-85b6-4be8-e43c-749e20122453"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:   37.9s\n",
            "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    0.4s\n",
            "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    0.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest with Randomised Temperature Validation RMSE: 3.706616917042993e-05\n",
            "Random Forest with Randomised Temperature Test RMSE: 2.2426850136980647e-06\n"
          ]
        }
      ],
      "source": [
        "##Test temp feature\n",
        "rf_regressor = RandomForestRegressor(n_estimators=best_n_estimators,\n",
        "                                     max_depth=best_max_depth,\n",
        "                                     min_samples_split=best_min_samples_split,\n",
        "                                     random_state=42,\n",
        "                                     verbose=1)\n",
        "#Fit model to training data\n",
        "rf_regressor.fit(x_train_temp, y_train)\n",
        "#Predict on validation data\n",
        "y_val_pred_temp = rf_regressor.predict(x_val)\n",
        "#Predict on test data\n",
        "y_test_pred_temp = rf_regressor.predict(x_test)\n",
        "\n",
        "#Validation & test RMSE (NOTE: uses rmse function defined in KNN section)\n",
        "rf_val_rmse_temp = rmse(y_val, y_val_pred_temp)\n",
        "rf_test_rmse_temp = rmse(y_test, y_test_pred_temp)\n",
        "\n",
        "print(f\"Random Forest with Randomised Temperature Validation RMSE: {rf_val_rmse_temp}\")\n",
        "print(f\"Random Forest with Randomised Temperature Test RMSE: {rf_test_rmse_temp}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z_Nt1yjQQsuy",
        "outputId": "d1adc01a-777f-46b7-cd71-26e68dd7a052"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:   38.2s\n",
            "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    0.3s\n",
            "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    0.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest with Randomised Pressure Validation RMSE: 4.504089883994311e-05\n",
            "Random Forest with Randomised Pressure Test RMSE: 2.4337655304407235e-06\n"
          ]
        }
      ],
      "source": [
        "##Test pressure feature\n",
        "rf_regressor = RandomForestRegressor(n_estimators=best_n_estimators,\n",
        "                                     max_depth=best_max_depth,\n",
        "                                     min_samples_split=best_min_samples_split,\n",
        "                                     random_state=42,\n",
        "                                     verbose=1)\n",
        "#Fit model to training data\n",
        "rf_regressor.fit(x_train_pressure, y_train)\n",
        "#Predict on validation data\n",
        "y_val_pred_pressure = rf_regressor.predict(x_val)\n",
        "#Predict on test data\n",
        "y_test_pred_pressure = rf_regressor.predict(x_test)\n",
        "\n",
        "#Validation & test RMSE (NOTE: uses rmse function defined in KNN section)\n",
        "rf_val_rmse_pressure = rmse(y_val, y_val_pred_pressure)\n",
        "rf_test_rmse_pressure = rmse(y_test, y_test_pred_pressure)\n",
        "\n",
        "print(f\"Random Forest with Randomised Pressure Validation RMSE: {rf_val_rmse_pressure}\")\n",
        "print(f\"Random Forest with Randomised Pressure Test RMSE: {rf_test_rmse_pressure}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yeV4kT0tRB9O",
        "outputId": "56234451-488d-487a-b204-8a1600ef770e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:   33.6s\n",
            "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    0.3s\n",
            "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    0.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest with Randomised Lat Validation RMSE: 2.5297778847743757e-05\n",
            "Random Forest with Randomised Lat Test RMSE: 1.286071096728847e-06\n"
          ]
        }
      ],
      "source": [
        "##Test lat feature\n",
        "rf_regressor = RandomForestRegressor(n_estimators=best_n_estimators,\n",
        "                                     max_depth=best_max_depth,\n",
        "                                     min_samples_split=best_min_samples_split,\n",
        "                                     random_state=42,\n",
        "                                     verbose=1)\n",
        "#Fit model to training data\n",
        "rf_regressor.fit(x_train_lat, y_train)\n",
        "#Predict on validation data\n",
        "y_val_pred_lat = rf_regressor.predict(x_val)\n",
        "#Predict on test data\n",
        "y_test_pred_lat = rf_regressor.predict(x_test)\n",
        "\n",
        "#Validation & test RMSE (NOTE: uses rmse function defined in KNN section)\n",
        "rf_val_rmse_lat = rmse(y_val, y_val_pred_lat)\n",
        "rf_test_rmse_lat = rmse(y_test, y_test_pred_lat)\n",
        "\n",
        "print(f\"Random Forest with Randomised Lat Validation RMSE: {rf_val_rmse_lat}\")\n",
        "print(f\"Random Forest with Randomised Lat Test RMSE: {rf_test_rmse_lat}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yYRFKZE2RMut",
        "outputId": "e5f70f03-4f7b-4e52-f55c-945d9c9960fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:   32.2s\n",
            "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    0.3s\n",
            "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    0.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest with Randomised Lon Validation RMSE: 2.896838850574568e-05\n",
            "Random Forest with Randomised Lon Test RMSE: 1.872725647444895e-06\n"
          ]
        }
      ],
      "source": [
        "##Test lon feature\n",
        "rf_regressor = RandomForestRegressor(n_estimators=best_n_estimators,\n",
        "                                     max_depth=best_max_depth,\n",
        "                                     min_samples_split=best_min_samples_split,\n",
        "                                     random_state=42,\n",
        "                                     verbose=1)\n",
        "#Fit model to training data\n",
        "rf_regressor.fit(x_train_lon, y_train)\n",
        "#Predict on validation data\n",
        "y_val_pred_lon = rf_regressor.predict(x_val)\n",
        "#Predict on test data\n",
        "y_test_pred_lon = rf_regressor.predict(x_test)\n",
        "\n",
        "#Validation & test RMSE (NOTE: uses rmse function defined in KNN section)\n",
        "rf_val_rmse_lon = rmse(y_val, y_val_pred_lon)\n",
        "rf_test_rmse_lon = rmse(y_test, y_test_pred_lon)\n",
        "\n",
        "print(f\"Random Forest with Randomised Lon Validation RMSE: {rf_val_rmse_lon}\")\n",
        "print(f\"Random Forest with Randomised Lon Test RMSE: {rf_test_rmse_lon}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4H-6ZzR_Rns6",
        "outputId": "ed33a431-5c53-49b7-80e5-c0395620534d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:   29.2s\n",
            "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    0.3s\n",
            "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    0.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest with Randomised Height Validation RMSE: 2.9265762350405566e-05\n",
            "Random Forest with Randomised Height Test RMSE: 9.943395298250834e-07\n"
          ]
        }
      ],
      "source": [
        "##Test height feature\n",
        "rf_regressor = RandomForestRegressor(n_estimators=best_n_estimators,\n",
        "                                     max_depth=best_max_depth,\n",
        "                                     min_samples_split=best_min_samples_split,\n",
        "                                     random_state=42,\n",
        "                                     verbose=1)\n",
        "#Fit model to training data\n",
        "rf_regressor.fit(x_train_height, y_train)\n",
        "#Predict on validation data\n",
        "y_val_pred_height = rf_regressor.predict(x_val)\n",
        "#Predict on test data\n",
        "y_test_pred_height = rf_regressor.predict(x_test)\n",
        "\n",
        "#Validation & test RMSE (NOTE: uses rmse function defined in KNN section)\n",
        "rf_val_rmse_height = rmse(y_val, y_val_pred_height)\n",
        "rf_test_rmse_height = rmse(y_test, y_test_pred_height)\n",
        "\n",
        "print(f\"Random Forest with Randomised Height Validation RMSE: {rf_val_rmse_height}\")\n",
        "print(f\"Random Forest with Randomised Height Test RMSE: {rf_test_rmse_height}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WKGRDdJ9RyOm",
        "outputId": "9d0ff6b7-5f81-4e90-c72d-9549cd234ec6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:   44.5s\n",
            "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    0.3s\n",
            "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    0.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest with Randomised Ice Conc Validation RMSE: 3.099209789070301e-05\n",
            "Random Forest with Randomised Ice Conc Test RMSE: 7.518740972045634e-07\n"
          ]
        }
      ],
      "source": [
        "##Test ice_conc feature\n",
        "rf_regressor = RandomForestRegressor(n_estimators=best_n_estimators,\n",
        "                                     max_depth=best_max_depth,\n",
        "                                     min_samples_split=best_min_samples_split,\n",
        "                                     random_state=42,\n",
        "                                     verbose=1)\n",
        "#Fit model to training data\n",
        "rf_regressor.fit(x_train_ice_conc, y_train)\n",
        "#Predict on validation data\n",
        "y_val_pred_ice_conc = rf_regressor.predict(x_val)\n",
        "#Predict on test data\n",
        "y_test_pred_ice_conc = rf_regressor.predict(x_test)\n",
        "\n",
        "#Validation & test RMSE (NOTE: uses rmse function defined in KNN section)\n",
        "rf_val_rmse_ice_conc = rmse(y_val, y_val_pred_ice_conc)\n",
        "rf_test_rmse_ice_conc = rmse(y_test, y_test_pred_ice_conc)\n",
        "\n",
        "print(f\"Random Forest with Randomised Ice Conc Validation RMSE: {rf_val_rmse_ice_conc}\")\n",
        "print(f\"Random Forest with Randomised Ice Conc Test RMSE: {rf_test_rmse_ice_conc}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XdR-3Y7RR_oz",
        "outputId": "6b290d64-03ad-415c-8b1f-cdea45f0764f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:   34.2s\n",
            "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    0.3s\n",
            "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    0.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest with Randomised Windspeed Validation RMSE: 2.921557825175114e-05\n",
            "Random Forest with Randomised Windspeed Test RMSE: 1.0460299790793215e-06\n"
          ]
        }
      ],
      "source": [
        "##Test windspeed feature\n",
        "rf_regressor = RandomForestRegressor(n_estimators=best_n_estimators,\n",
        "                                     max_depth=best_max_depth,\n",
        "                                     min_samples_split=best_min_samples_split,\n",
        "                                     random_state=42,\n",
        "                                     verbose=1)\n",
        "#Fit model to training data\n",
        "rf_regressor.fit(x_train_windspeed, y_train)\n",
        "#Predict on validation data\n",
        "y_val_pred_windspeed = rf_regressor.predict(x_val)\n",
        "#Predict on test data\n",
        "y_test_pred_windspeed = rf_regressor.predict(x_test)\n",
        "\n",
        "#Validation & test RMSE (NOTE: uses rmse function defined in KNN section)\n",
        "rf_val_rmse_windspeed = rmse(y_val, y_val_pred_windspeed)\n",
        "rf_test_rmse_windspeed = rmse(y_test, y_test_pred_windspeed)\n",
        "\n",
        "print(f\"Random Forest with Randomised Windspeed Validation RMSE: {rf_val_rmse_windspeed}\")\n",
        "print(f\"Random Forest with Randomised Windspeed Test RMSE: {rf_test_rmse_windspeed}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1dh90HpLSXh-",
        "outputId": "5076e87c-8a6e-4ac6-b803-70e0feb484b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:   33.9s\n",
            "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    0.3s\n",
            "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    0.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest with Randomised Precip Validation RMSE: 2.5808571081142873e-05\n",
            "Random Forest with Randomised Precip Test RMSE: 1.4759299347133492e-06\n"
          ]
        }
      ],
      "source": [
        "##Test precip feature\n",
        "rf_regressor = RandomForestRegressor(n_estimators=best_n_estimators,\n",
        "                                     max_depth=best_max_depth,\n",
        "                                     min_samples_split=best_min_samples_split,\n",
        "                                     random_state=42,\n",
        "                                     verbose=1)\n",
        "#Fit model to training data\n",
        "rf_regressor.fit(x_train_precip, y_train)\n",
        "#Predict on validation data\n",
        "y_val_pred_precip = rf_regressor.predict(x_val)\n",
        "#Predict on test data\n",
        "y_test_pred_precip = rf_regressor.predict(x_test)\n",
        "\n",
        "#Validation & test RMSE (NOTE: uses rmse function defined in KNN section)\n",
        "rf_val_rmse_precip = rmse(y_val, y_val_pred_precip)\n",
        "rf_test_rmse_precip = rmse(y_test, y_test_pred_precip)\n",
        "\n",
        "print(f\"Random Forest with Randomised Precip Validation RMSE: {rf_val_rmse_precip}\")\n",
        "print(f\"Random Forest with Randomised Precip Test RMSE: {rf_test_rmse_precip}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5gn9Mbz5SjXo"
      },
      "outputs": [],
      "source": [
        "##RMSE ratings\n",
        "rf_temp_rmse = rf_test_rmse_temp / rf_test_rmse\n",
        "rf_pressure_rmse = rf_test_rmse_pressure / rf_test_rmse\n",
        "rf_lat_rmse = rf_test_rmse_lat / rf_test_rmse\n",
        "rf_lon_rmse = rf_test_rmse_lon / rf_test_rmse\n",
        "rf_height_rmse = rf_test_rmse_height / rf_test_rmse\n",
        "rf_ice_conc_rmse = rf_test_rmse_ice_conc / rf_test_rmse\n",
        "rf_windspeed_rmse = rf_test_rmse_windspeed / rf_test_rmse\n",
        "rf_precip_rmse = rf_test_rmse_precip / rf_test_rmse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wGaxv1HS2ead",
        "outputId": "29885111-66da-449f-ff45-210f8c1fb4eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.255452\n",
            "2.4476202\n",
            "1.2933923\n",
            "1.8833865\n",
            "1.0\n",
            "0.7561543\n",
            "1.0519847\n",
            "1.484332\n"
          ]
        }
      ],
      "source": [
        "print(rf_temp_rmse)\n",
        "print(rf_pressure_rmse)\n",
        "print(rf_lat_rmse)\n",
        "print(rf_lon_rmse)\n",
        "print(rf_height_rmse)\n",
        "print(rf_ice_conc_rmse)\n",
        "print(rf_windspeed_rmse)\n",
        "print(rf_precip_rmse)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yfk2aGh3bWt5"
      },
      "source": [
        "# **Model 4 - LSTM**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YD-2aktTNcXc"
      },
      "source": [
        "https://www.kaggle.com/code/navjindervirdee/lstm-neural-network-from-scratch\n",
        "https://medium.com/@CallMeTwitch/building-a-neural-network-zoo-from-scratch-the-long-short-term-memory-network-1cec5cf31b7\n",
        "https://pub.towardsai.net/building-a-lstm-from-scratch-in-python-1dedd89de8fe\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RsNB5E15bY1A",
        "outputId": "896bb5bd-2a97-470a-bffc-71189f51c72f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "8553/8553 [==============================] - 34s 4ms/step - loss: 59.9169\n",
            "Epoch 2/100\n",
            "8553/8553 [==============================] - 28s 3ms/step - loss: 0.0058\n",
            "Epoch 3/100\n",
            "8553/8553 [==============================] - 28s 3ms/step - loss: 0.0010\n",
            "Epoch 4/100\n",
            "8553/8553 [==============================] - 31s 4ms/step - loss: 5.5427e-04\n",
            "Epoch 5/100\n",
            "8553/8553 [==============================] - 38s 4ms/step - loss: 3.6392e-04\n",
            "Epoch 6/100\n",
            "8553/8553 [==============================] - 28s 3ms/step - loss: 2.6819e-04\n",
            "Epoch 7/100\n",
            "8553/8553 [==============================] - 31s 4ms/step - loss: 2.0361e-04\n",
            "Epoch 8/100\n",
            "8553/8553 [==============================] - 28s 3ms/step - loss: 1.6888e-04\n",
            "Epoch 9/100\n",
            "8553/8553 [==============================] - 37s 4ms/step - loss: 1.4025e-04\n",
            "Epoch 10/100\n",
            "8553/8553 [==============================] - 34s 4ms/step - loss: 1.2666e-04\n",
            "Epoch 11/100\n",
            "8553/8553 [==============================] - 28s 3ms/step - loss: 1.1644e-04\n",
            "Epoch 12/100\n",
            "8553/8553 [==============================] - 33s 4ms/step - loss: 1.0867e-04\n",
            "Epoch 13/100\n",
            "8553/8553 [==============================] - 31s 4ms/step - loss: 9.9256e-05\n",
            "Epoch 14/100\n",
            "8553/8553 [==============================] - 38s 4ms/step - loss: 9.0694e-05\n",
            "Epoch 15/100\n",
            "8553/8553 [==============================] - 32s 4ms/step - loss: 8.6703e-05\n",
            "Epoch 16/100\n",
            "8553/8553 [==============================] - 33s 4ms/step - loss: 8.4995e-05\n",
            "Epoch 17/100\n",
            "8553/8553 [==============================] - 33s 4ms/step - loss: 7.8850e-05\n",
            "Epoch 18/100\n",
            "8553/8553 [==============================] - 39s 5ms/step - loss: 7.5572e-05\n",
            "Epoch 19/100\n",
            "8553/8553 [==============================] - 34s 4ms/step - loss: 7.3108e-05\n",
            "Epoch 20/100\n",
            "8553/8553 [==============================] - 29s 3ms/step - loss: 7.1609e-05\n",
            "Epoch 21/100\n",
            "8553/8553 [==============================] - 33s 4ms/step - loss: 6.9602e-05\n",
            "Epoch 22/100\n",
            "8553/8553 [==============================] - 39s 5ms/step - loss: 6.5548e-05\n",
            "Epoch 23/100\n",
            "8553/8553 [==============================] - 30s 3ms/step - loss: 6.7050e-05\n",
            "Epoch 24/100\n",
            "8553/8553 [==============================] - 30s 3ms/step - loss: 5.7895e-05\n",
            "Epoch 25/100\n",
            "8553/8553 [==============================] - 27s 3ms/step - loss: 6.1727e-05\n",
            "Epoch 26/100\n",
            "8553/8553 [==============================] - 34s 4ms/step - loss: 5.5043e-05\n",
            "Epoch 27/100\n",
            "8553/8553 [==============================] - 33s 4ms/step - loss: 5.4820e-05\n",
            "Epoch 28/100\n",
            "8553/8553 [==============================] - 26s 3ms/step - loss: 5.6718e-05\n",
            "Epoch 29/100\n",
            "8553/8553 [==============================] - 29s 3ms/step - loss: 5.1929e-05\n",
            "Epoch 30/100\n",
            "8553/8553 [==============================] - 29s 3ms/step - loss: 5.0492e-05\n",
            "Epoch 31/100\n",
            "8553/8553 [==============================] - 37s 4ms/step - loss: 5.0592e-05\n",
            "Epoch 32/100\n",
            "8553/8553 [==============================] - 28s 3ms/step - loss: 5.0580e-05\n",
            "Epoch 33/100\n",
            "8553/8553 [==============================] - 31s 4ms/step - loss: 4.7463e-05\n",
            "Epoch 34/100\n",
            "8553/8553 [==============================] - 27s 3ms/step - loss: 5.0969e-05\n",
            "Epoch 35/100\n",
            "8553/8553 [==============================] - 33s 4ms/step - loss: 4.5702e-05\n",
            "Epoch 36/100\n",
            "8553/8553 [==============================] - 34s 4ms/step - loss: 4.2979e-05\n",
            "Epoch 37/100\n",
            "8553/8553 [==============================] - 28s 3ms/step - loss: 4.8443e-05\n",
            "Epoch 38/100\n",
            "8553/8553 [==============================] - 27s 3ms/step - loss: 4.3141e-05\n",
            "Epoch 39/100\n",
            "8553/8553 [==============================] - 30s 4ms/step - loss: 4.1770e-05\n",
            "Epoch 40/100\n",
            "8553/8553 [==============================] - 35s 4ms/step - loss: 4.4695e-05\n",
            "Epoch 41/100\n",
            "8553/8553 [==============================] - 28s 3ms/step - loss: 4.3488e-05\n",
            "Epoch 42/100\n",
            "8553/8553 [==============================] - 30s 3ms/step - loss: 3.8286e-05\n",
            "Epoch 43/100\n",
            "8553/8553 [==============================] - 29s 3ms/step - loss: 3.8665e-05\n",
            "Epoch 44/100\n",
            "8553/8553 [==============================] - 29s 3ms/step - loss: 3.8406e-05\n",
            "Epoch 45/100\n",
            "8553/8553 [==============================] - 36s 4ms/step - loss: 3.9694e-05\n",
            "Epoch 46/100\n",
            "8553/8553 [==============================] - 29s 3ms/step - loss: 3.6885e-05\n",
            "Epoch 47/100\n",
            "8553/8553 [==============================] - 27s 3ms/step - loss: 3.9981e-05\n",
            "Epoch 48/100\n",
            "8553/8553 [==============================] - 31s 4ms/step - loss: 3.5595e-05\n",
            "Epoch 49/100\n",
            "8553/8553 [==============================] - 37s 4ms/step - loss: 3.4583e-05\n",
            "Epoch 50/100\n",
            "8553/8553 [==============================] - 29s 3ms/step - loss: 3.6786e-05\n",
            "Epoch 51/100\n",
            "8553/8553 [==============================] - 30s 4ms/step - loss: 3.5190e-05\n",
            "Epoch 52/100\n",
            "8553/8553 [==============================] - 28s 3ms/step - loss: 3.5888e-05\n",
            "Epoch 53/100\n",
            "8553/8553 [==============================] - 29s 3ms/step - loss: 3.4455e-05\n",
            "Epoch 54/100\n",
            "8553/8553 [==============================] - 36s 4ms/step - loss: 3.6418e-05\n",
            "Epoch 55/100\n",
            "8553/8553 [==============================] - 29s 3ms/step - loss: 3.3124e-05\n",
            "Epoch 56/100\n",
            "8553/8553 [==============================] - 26s 3ms/step - loss: 3.4957e-05\n",
            "Epoch 57/100\n",
            "8553/8553 [==============================] - 29s 3ms/step - loss: 3.2019e-05\n",
            "Epoch 58/100\n",
            "8553/8553 [==============================] - 40s 5ms/step - loss: 2.9822e-05\n",
            "Epoch 59/100\n",
            "8553/8553 [==============================] - 30s 4ms/step - loss: 3.2272e-05\n",
            "Epoch 60/100\n",
            "8553/8553 [==============================] - 31s 4ms/step - loss: 3.0562e-05\n",
            "Epoch 61/100\n",
            "8553/8553 [==============================] - 29s 3ms/step - loss: 3.1561e-05\n",
            "Epoch 62/100\n",
            "8553/8553 [==============================] - 30s 3ms/step - loss: 2.9330e-05\n",
            "Epoch 63/100\n",
            "8553/8553 [==============================] - 35s 4ms/step - loss: 3.0428e-05\n",
            "Epoch 64/100\n",
            "8553/8553 [==============================] - 31s 4ms/step - loss: 3.0635e-05\n",
            "Epoch 65/100\n",
            "8553/8553 [==============================] - 29s 3ms/step - loss: 3.0706e-05\n",
            "Epoch 66/100\n",
            "8553/8553 [==============================] - 32s 4ms/step - loss: 2.7951e-05\n",
            "Epoch 67/100\n",
            "8553/8553 [==============================] - 36s 4ms/step - loss: 3.0863e-05\n",
            "Epoch 68/100\n",
            "8553/8553 [==============================] - 26s 3ms/step - loss: 3.3160e-05\n",
            "Epoch 69/100\n",
            "8553/8553 [==============================] - 28s 3ms/step - loss: 2.8038e-05\n",
            "Epoch 70/100\n",
            "8553/8553 [==============================] - 29s 3ms/step - loss: 2.7775e-05\n",
            "Epoch 71/100\n",
            "8553/8553 [==============================] - 28s 3ms/step - loss: 2.7612e-05\n",
            "Epoch 72/100\n",
            "8553/8553 [==============================] - 35s 4ms/step - loss: 2.7485e-05\n",
            "Epoch 73/100\n",
            "8553/8553 [==============================] - 33s 4ms/step - loss: 2.8012e-05\n",
            "Epoch 74/100\n",
            "8553/8553 [==============================] - 28s 3ms/step - loss: 2.5219e-05\n",
            "Epoch 75/100\n",
            "8553/8553 [==============================] - 30s 3ms/step - loss: 2.7867e-05\n",
            "Epoch 76/100\n",
            "8553/8553 [==============================] - 45s 5ms/step - loss: 2.4591e-05\n",
            "Epoch 77/100\n",
            "8553/8553 [==============================] - 28s 3ms/step - loss: 2.7931e-05\n",
            "Epoch 78/100\n",
            "8553/8553 [==============================] - 33s 4ms/step - loss: 2.5558e-05\n",
            "Epoch 79/100\n",
            "8553/8553 [==============================] - 31s 4ms/step - loss: 2.7037e-05\n",
            "Epoch 80/100\n",
            "8553/8553 [==============================] - 41s 5ms/step - loss: 2.4447e-05\n",
            "Epoch 81/100\n",
            "8553/8553 [==============================] - 30s 3ms/step - loss: 2.4731e-05\n",
            "Epoch 82/100\n",
            "8553/8553 [==============================] - 28s 3ms/step - loss: 2.7228e-05\n",
            "Epoch 83/100\n",
            "8553/8553 [==============================] - 32s 4ms/step - loss: 2.2021e-05\n",
            "Epoch 84/100\n",
            "8553/8553 [==============================] - 36s 4ms/step - loss: 2.8061e-05\n",
            "Epoch 85/100\n",
            "8553/8553 [==============================] - 33s 4ms/step - loss: 2.2408e-05\n",
            "Epoch 86/100\n",
            "8553/8553 [==============================] - 30s 4ms/step - loss: 2.7692e-05\n",
            "Epoch 87/100\n",
            "8553/8553 [==============================] - 30s 4ms/step - loss: 2.1958e-05\n",
            "Epoch 88/100\n",
            "8553/8553 [==============================] - 33s 4ms/step - loss: 2.3999e-05\n",
            "Epoch 89/100\n",
            "8553/8553 [==============================] - 35s 4ms/step - loss: 2.3032e-05\n",
            "Epoch 90/100\n",
            "8553/8553 [==============================] - 30s 4ms/step - loss: 2.3505e-05\n",
            "Epoch 91/100\n",
            "8553/8553 [==============================] - 29s 3ms/step - loss: 2.6633e-05\n",
            "Epoch 92/100\n",
            "8553/8553 [==============================] - 27s 3ms/step - loss: 2.4143e-05\n",
            "Epoch 93/100\n",
            "8553/8553 [==============================] - 39s 5ms/step - loss: 2.0719e-05\n",
            "Epoch 94/100\n",
            "8553/8553 [==============================] - 29s 3ms/step - loss: 2.3959e-05\n",
            "Epoch 95/100\n",
            "8553/8553 [==============================] - 28s 3ms/step - loss: 2.4544e-05\n",
            "Epoch 96/100\n",
            "8553/8553 [==============================] - 32s 4ms/step - loss: 2.0117e-05\n",
            "Epoch 97/100\n",
            "8553/8553 [==============================] - 34s 4ms/step - loss: 2.2633e-05\n",
            "Epoch 98/100\n",
            "8553/8553 [==============================] - 32s 4ms/step - loss: 2.0906e-05\n",
            "Epoch 99/100\n",
            "8553/8553 [==============================] - 31s 4ms/step - loss: 2.3320e-05\n",
            "Epoch 100/100\n",
            "8553/8553 [==============================] - 27s 3ms/step - loss: 2.2968e-05\n",
            "4277/4277 [==============================] - 11s 3ms/step\n",
            "4277/4277 [==============================] - 10s 2ms/step\n",
            "LSTM Validation RMSE: 0.0023484068234861087\n",
            "LSTM Test RMSE: 0.0023513991326912353\n"
          ]
        }
      ],
      "source": [
        "x_train_reshaped = x_train.reshape((x_train.shape[0], 1, x_train.shape[1]))\n",
        "x_val_reshaped = x_val.reshape((x_val.shape[0], 1, x_val.shape[1]))\n",
        "x_test_reshaped = x_test.reshape((x_test.shape[0], 1, x_test.shape[1]))\n",
        "model = Sequential()\n",
        "model.add(LSTM(50, activation='tanh', input_shape=(1, x_train_reshaped.shape[2])))\n",
        "model.add(Dense(1))\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "model.fit(x_train_reshaped, y_train, epochs=100, batch_size=32, verbose=1)\n",
        "# Predict with the model\n",
        "y_val_pred = model.predict(x_val_reshaped)\n",
        "y_test_pred = model.predict(x_test_reshaped)\n",
        "\n",
        "lstm_val_rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))\n",
        "lstm_test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
        "\n",
        "print(f\"LSTM Validation RMSE: {lstm_val_rmse}\")\n",
        "print(f\"LSTM Test RMSE: {lstm_test_rmse}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KkH4sZAnUaQF"
      },
      "source": [
        "**Feature Testing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UoYnkH0IUcE2",
        "outputId": "29ddac37-13fb-4c0c-b1f7-6b684dd93c02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "8553/8553 [==============================] - 30s 3ms/step - loss: 60.5359\n",
            "Epoch 2/100\n",
            "8553/8553 [==============================] - 28s 3ms/step - loss: 0.1668\n",
            "Epoch 3/100\n",
            "8553/8553 [==============================] - 34s 4ms/step - loss: 0.0582\n",
            "Epoch 4/100\n",
            "8553/8553 [==============================] - 28s 3ms/step - loss: 0.0231\n",
            "Epoch 5/100\n",
            "8553/8553 [==============================] - 29s 3ms/step - loss: 0.0098\n",
            "Epoch 6/100\n",
            "8553/8553 [==============================] - 29s 3ms/step - loss: 0.0046\n",
            "Epoch 7/100\n",
            "8553/8553 [==============================] - 31s 4ms/step - loss: 0.0026\n",
            "Epoch 8/100\n",
            "8553/8553 [==============================] - 37s 4ms/step - loss: 0.0017\n",
            "Epoch 9/100\n",
            "8553/8553 [==============================] - 30s 3ms/step - loss: 0.0012\n",
            "Epoch 10/100\n",
            "8553/8553 [==============================] - 28s 3ms/step - loss: 9.2457e-04\n",
            "Epoch 11/100\n",
            "8553/8553 [==============================] - 31s 4ms/step - loss: 7.4568e-04\n",
            "Epoch 12/100\n",
            "8553/8553 [==============================] - 37s 4ms/step - loss: 6.2422e-04\n",
            "Epoch 13/100\n",
            "8553/8553 [==============================] - 29s 3ms/step - loss: 5.3151e-04\n",
            "Epoch 14/100\n",
            "8553/8553 [==============================] - 30s 4ms/step - loss: 4.7221e-04\n",
            "Epoch 15/100\n",
            "8553/8553 [==============================] - 28s 3ms/step - loss: 4.2686e-04\n",
            "Epoch 16/100\n",
            "8553/8553 [==============================] - 37s 4ms/step - loss: 3.9525e-04\n",
            "Epoch 17/100\n",
            "8553/8553 [==============================] - 32s 4ms/step - loss: 3.6526e-04\n",
            "Epoch 18/100\n",
            "8553/8553 [==============================] - 28s 3ms/step - loss: 3.4665e-04\n",
            "Epoch 19/100\n",
            "8553/8553 [==============================] - 32s 4ms/step - loss: 3.3104e-04\n",
            "Epoch 20/100\n",
            "8553/8553 [==============================] - 28s 3ms/step - loss: 3.1467e-04\n",
            "Epoch 21/100\n",
            "8553/8553 [==============================] - 37s 4ms/step - loss: 3.0099e-04\n",
            "Epoch 22/100\n",
            "8553/8553 [==============================] - 32s 4ms/step - loss: 2.9246e-04\n",
            "Epoch 23/100\n",
            "8553/8553 [==============================] - 29s 3ms/step - loss: 2.8496e-04\n",
            "Epoch 24/100\n",
            "8553/8553 [==============================] - 31s 4ms/step - loss: 2.7996e-04\n",
            "Epoch 25/100\n",
            "8553/8553 [==============================] - 39s 5ms/step - loss: 2.7234e-04\n",
            "Epoch 26/100\n",
            "8553/8553 [==============================] - 28s 3ms/step - loss: 2.5662e-04\n",
            "Epoch 27/100\n",
            "8553/8553 [==============================] - 33s 4ms/step - loss: 2.5037e-04\n",
            "Epoch 28/100\n",
            "8553/8553 [==============================] - 29s 3ms/step - loss: 2.4987e-04\n",
            "Epoch 29/100\n",
            "8553/8553 [==============================] - 38s 4ms/step - loss: 2.5011e-04\n",
            "Epoch 30/100\n",
            "8553/8553 [==============================] - 31s 4ms/step - loss: 2.3590e-04\n",
            "Epoch 31/100\n",
            "8553/8553 [==============================] - 28s 3ms/step - loss: 2.3483e-04\n",
            "Epoch 32/100\n",
            "8553/8553 [==============================] - 31s 4ms/step - loss: 2.3140e-04\n",
            "Epoch 33/100\n",
            "8553/8553 [==============================] - 33s 4ms/step - loss: 2.2435e-04\n",
            "Epoch 34/100\n",
            "8553/8553 [==============================] - 33s 4ms/step - loss: 2.2110e-04\n",
            "Epoch 35/100\n",
            "8553/8553 [==============================] - 32s 4ms/step - loss: 2.1162e-04\n",
            "Epoch 36/100\n",
            "8553/8553 [==============================] - 29s 3ms/step - loss: 2.1292e-04\n",
            "Epoch 37/100\n",
            "8553/8553 [==============================] - 32s 4ms/step - loss: 2.0577e-04\n",
            "Epoch 38/100\n",
            "8553/8553 [==============================] - 40s 5ms/step - loss: 2.0309e-04\n",
            "Epoch 39/100\n",
            "8553/8553 [==============================] - 28s 3ms/step - loss: 2.0355e-04\n",
            "Epoch 40/100\n",
            "8553/8553 [==============================] - 31s 4ms/step - loss: 1.9544e-04\n",
            "Epoch 41/100\n",
            "8553/8553 [==============================] - 27s 3ms/step - loss: 1.9182e-04\n",
            "Epoch 42/100\n",
            "8553/8553 [==============================] - 37s 4ms/step - loss: 1.9001e-04\n",
            "Epoch 43/100\n",
            "8553/8553 [==============================] - 32s 4ms/step - loss: 1.8639e-04\n",
            "Epoch 44/100\n",
            "8553/8553 [==============================] - 27s 3ms/step - loss: 1.8399e-04\n",
            "Epoch 45/100\n",
            "8553/8553 [==============================] - 29s 3ms/step - loss: 1.8317e-04\n",
            "Epoch 46/100\n",
            "8553/8553 [==============================] - 31s 4ms/step - loss: 1.7383e-04\n",
            "Epoch 47/100\n",
            "8553/8553 [==============================] - 35s 4ms/step - loss: 1.7415e-04\n",
            "Epoch 48/100\n",
            "8553/8553 [==============================] - 27s 3ms/step - loss: 1.7481e-04\n",
            "Epoch 49/100\n",
            "8553/8553 [==============================] - 30s 3ms/step - loss: 1.7209e-04\n",
            "Epoch 50/100\n",
            "8553/8553 [==============================] - 26s 3ms/step - loss: 1.6761e-04\n",
            "Epoch 51/100\n",
            "8553/8553 [==============================] - 34s 4ms/step - loss: 1.6478e-04\n",
            "Epoch 52/100\n",
            "8553/8553 [==============================] - 31s 4ms/step - loss: 1.6252e-04\n",
            "Epoch 53/100\n",
            "8553/8553 [==============================] - 29s 3ms/step - loss: 1.5801e-04\n",
            "Epoch 54/100\n",
            "8553/8553 [==============================] - 27s 3ms/step - loss: 1.5730e-04\n",
            "Epoch 55/100\n",
            "8553/8553 [==============================] - 29s 3ms/step - loss: 1.5329e-04\n",
            "Epoch 56/100\n",
            "8553/8553 [==============================] - 39s 5ms/step - loss: 1.4570e-04\n",
            "Epoch 57/100\n",
            "8553/8553 [==============================] - 26s 3ms/step - loss: 1.4921e-04\n",
            "Epoch 58/100\n",
            "8553/8553 [==============================] - 28s 3ms/step - loss: 1.4988e-04\n",
            "Epoch 59/100\n",
            "8553/8553 [==============================] - 30s 3ms/step - loss: 1.4369e-04\n",
            "Epoch 60/100\n",
            "8553/8553 [==============================] - 32s 4ms/step - loss: 1.4181e-04\n",
            "Epoch 61/100\n",
            "8553/8553 [==============================] - 31s 4ms/step - loss: 1.4016e-04\n",
            "Epoch 62/100\n",
            "8553/8553 [==============================] - 30s 3ms/step - loss: 1.3844e-04\n",
            "Epoch 63/100\n",
            "8553/8553 [==============================] - 28s 3ms/step - loss: 1.3946e-04\n",
            "Epoch 64/100\n",
            "8553/8553 [==============================] - 29s 3ms/step - loss: 1.3726e-04\n",
            "Epoch 65/100\n",
            "8553/8553 [==============================] - 40s 5ms/step - loss: 1.3394e-04\n",
            "Epoch 66/100\n",
            "8553/8553 [==============================] - 28s 3ms/step - loss: 1.3183e-04\n",
            "Epoch 67/100\n",
            "8553/8553 [==============================] - 29s 3ms/step - loss: 1.3410e-04\n",
            "Epoch 68/100\n",
            "8553/8553 [==============================] - 30s 4ms/step - loss: 1.3025e-04\n",
            "Epoch 69/100\n",
            "8553/8553 [==============================] - 35s 4ms/step - loss: 1.2788e-04\n",
            "Epoch 70/100\n",
            "8553/8553 [==============================] - 35s 4ms/step - loss: 1.2564e-04\n",
            "Epoch 71/100\n",
            "8553/8553 [==============================] - 29s 3ms/step - loss: 1.2404e-04\n",
            "Epoch 72/100\n",
            "8553/8553 [==============================] - 28s 3ms/step - loss: 1.2119e-04\n",
            "Epoch 73/100\n",
            "8553/8553 [==============================] - 30s 4ms/step - loss: 1.1731e-04\n",
            "Epoch 74/100\n",
            "8553/8553 [==============================] - 37s 4ms/step - loss: 1.1901e-04\n",
            "Epoch 75/100\n",
            "8553/8553 [==============================] - 28s 3ms/step - loss: 1.1509e-04\n",
            "Epoch 76/100\n",
            "8553/8553 [==============================] - 32s 4ms/step - loss: 1.1540e-04\n",
            "Epoch 77/100\n",
            "8553/8553 [==============================] - 29s 3ms/step - loss: 1.1717e-04\n",
            "Epoch 78/100\n",
            "8553/8553 [==============================] - 40s 5ms/step - loss: 1.1024e-04\n",
            "Epoch 79/100\n",
            "8553/8553 [==============================] - 30s 4ms/step - loss: 1.1013e-04\n",
            "Epoch 80/100\n",
            "8553/8553 [==============================] - 29s 3ms/step - loss: 1.1184e-04\n",
            "Epoch 81/100\n",
            "8553/8553 [==============================] - 33s 4ms/step - loss: 1.0941e-04\n",
            "Epoch 82/100\n",
            "8553/8553 [==============================] - 36s 4ms/step - loss: 1.0841e-04\n",
            "Epoch 83/100\n",
            "8553/8553 [==============================] - 32s 4ms/step - loss: 1.0595e-04\n",
            "Epoch 84/100\n",
            "8553/8553 [==============================] - 31s 4ms/step - loss: 1.0759e-04\n",
            "Epoch 85/100\n",
            "8553/8553 [==============================] - 30s 3ms/step - loss: 1.0399e-04\n",
            "Epoch 86/100\n",
            "8553/8553 [==============================] - 32s 4ms/step - loss: 1.0464e-04\n",
            "Epoch 87/100\n",
            "8553/8553 [==============================] - 38s 4ms/step - loss: 1.0375e-04\n",
            "Epoch 88/100\n",
            "8553/8553 [==============================] - 31s 4ms/step - loss: 1.0120e-04\n",
            "Epoch 89/100\n",
            "8553/8553 [==============================] - 30s 4ms/step - loss: 1.0100e-04\n",
            "Epoch 90/100\n",
            "8553/8553 [==============================] - 33s 4ms/step - loss: 1.0214e-04\n",
            "Epoch 91/100\n",
            "8553/8553 [==============================] - 41s 5ms/step - loss: 1.0099e-04\n",
            "Epoch 92/100\n",
            "8553/8553 [==============================] - 29s 3ms/step - loss: 9.6927e-05\n",
            "Epoch 93/100\n",
            "8553/8553 [==============================] - 32s 4ms/step - loss: 9.5088e-05\n",
            "Epoch 94/100\n",
            "8553/8553 [==============================] - 29s 3ms/step - loss: 9.7922e-05\n",
            "Epoch 95/100\n",
            "8553/8553 [==============================] - 41s 5ms/step - loss: 9.4706e-05\n",
            "Epoch 96/100\n",
            "8553/8553 [==============================] - 30s 4ms/step - loss: 9.3911e-05\n",
            "Epoch 97/100\n",
            "8553/8553 [==============================] - 30s 4ms/step - loss: 9.2786e-05\n",
            "Epoch 98/100\n",
            "8553/8553 [==============================] - 33s 4ms/step - loss: 9.6763e-05\n",
            "Epoch 99/100\n",
            "8553/8553 [==============================] - 40s 5ms/step - loss: 9.2791e-05\n",
            "Epoch 100/100\n",
            "8553/8553 [==============================] - 34s 4ms/step - loss: 9.2407e-05\n",
            "4277/4277 [==============================] - 10s 2ms/step\n",
            "4277/4277 [==============================] - 9s 2ms/step\n",
            "LSTM Validation on Randomised Temp RMSE: 0.012627889542356576\n",
            "LSTM Test on Randomised Temp RMSE: 0.01260464313326427\n"
          ]
        }
      ],
      "source": [
        "##Test temp feature\n",
        "x_train_temp_reshaped = x_train_temp.reshape((x_train_temp.shape[0], 1, x_train_temp.shape[1]))\n",
        "x_val_reshaped = x_val.reshape((x_val.shape[0], 1, x_val.shape[1]))\n",
        "x_test_reshaped = x_test.reshape((x_test.shape[0], 1, x_test.shape[1]))\n",
        "model = Sequential()\n",
        "model.add(LSTM(50, activation='tanh', input_shape=(1, x_train_temp_reshaped.shape[2])))\n",
        "model.add(Dense(1))\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "model.fit(x_train_temp_reshaped, y_train, epochs=100, batch_size=32, verbose=1)\n",
        "# Predict with the model\n",
        "y_val_temp_pred = model.predict(x_val_reshaped)\n",
        "y_test_temp_pred = model.predict(x_test_reshaped)\n",
        "\n",
        "lstm_val_rmse_temp = np.sqrt(mean_squared_error(y_val, y_val_temp_pred))\n",
        "lstm_test_rmse_temp = np.sqrt(mean_squared_error(y_test, y_test_temp_pred))\n",
        "\n",
        "print(f\"LSTM Validation on Randomised Temp RMSE: {lstm_val_rmse_temp}\")\n",
        "print(f\"LSTM Test on Randomised Temp RMSE: {lstm_test_rmse_temp}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DnQyhRVSUz01",
        "outputId": "2d768cf4-0dbe-4cf3-cb36-b3c9377f8673"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "8553/8553 [==============================] - 44s 5ms/step - loss: 57.6634\n",
            "Epoch 2/100\n",
            "8553/8553 [==============================] - 32s 4ms/step - loss: 0.0104\n",
            "Epoch 3/100\n",
            "8553/8553 [==============================] - 34s 4ms/step - loss: 0.0014\n",
            "Epoch 4/100\n",
            "8553/8553 [==============================] - 30s 4ms/step - loss: 6.7970e-04\n",
            "Epoch 5/100\n",
            "8553/8553 [==============================] - 41s 5ms/step - loss: 4.4859e-04\n",
            "Epoch 6/100\n",
            "8553/8553 [==============================] - 32s 4ms/step - loss: 3.3417e-04\n",
            "Epoch 7/100\n",
            "8553/8553 [==============================] - 30s 4ms/step - loss: 2.6122e-04\n",
            "Epoch 8/100\n",
            "8553/8553 [==============================] - 32s 4ms/step - loss: 2.2307e-04\n",
            "Epoch 9/100\n",
            "8553/8553 [==============================] - 39s 5ms/step - loss: 1.9576e-04\n",
            "Epoch 10/100\n",
            "8553/8553 [==============================] - 33s 4ms/step - loss: 1.7491e-04\n",
            "Epoch 11/100\n",
            "8553/8553 [==============================] - 29s 3ms/step - loss: 1.5575e-04\n",
            "Epoch 12/100\n",
            "8553/8553 [==============================] - 31s 4ms/step - loss: 1.4275e-04\n",
            "Epoch 13/100\n",
            "8553/8553 [==============================] - 37s 4ms/step - loss: 1.3244e-04\n",
            "Epoch 14/100\n",
            "8553/8553 [==============================] - 31s 4ms/step - loss: 1.2140e-04\n",
            "Epoch 15/100\n",
            "8553/8553 [==============================] - 33s 4ms/step - loss: 1.1835e-04\n",
            "Epoch 16/100\n",
            "8553/8553 [==============================] - 28s 3ms/step - loss: 1.0609e-04\n",
            "Epoch 17/100\n",
            "8553/8553 [==============================] - 36s 4ms/step - loss: 1.0714e-04\n",
            "Epoch 18/100\n",
            "8553/8553 [==============================] - 35s 4ms/step - loss: 1.0045e-04\n",
            "Epoch 19/100\n",
            "8553/8553 [==============================] - 28s 3ms/step - loss: 9.4841e-05\n",
            "Epoch 20/100\n",
            "8553/8553 [==============================] - 33s 4ms/step - loss: 9.0692e-05\n",
            "Epoch 21/100\n",
            "8553/8553 [==============================] - 29s 3ms/step - loss: 9.0184e-05\n",
            "Epoch 22/100\n",
            "8553/8553 [==============================] - 38s 4ms/step - loss: 8.3852e-05\n",
            "Epoch 23/100\n",
            "8553/8553 [==============================] - 31s 4ms/step - loss: 7.9006e-05\n",
            "Epoch 24/100\n",
            "8553/8553 [==============================] - 30s 4ms/step - loss: 7.6758e-05\n",
            "Epoch 25/100\n",
            "8553/8553 [==============================] - 33s 4ms/step - loss: 7.8580e-05\n",
            "Epoch 26/100\n",
            "8553/8553 [==============================] - 39s 5ms/step - loss: 7.4191e-05\n",
            "Epoch 27/100\n",
            "8553/8553 [==============================] - 31s 4ms/step - loss: 7.2051e-05\n",
            "Epoch 28/100\n",
            "8553/8553 [==============================] - 31s 4ms/step - loss: 6.9480e-05\n",
            "Epoch 29/100\n",
            "8553/8553 [==============================] - 29s 3ms/step - loss: 6.5544e-05\n",
            "Epoch 30/100\n",
            "8553/8553 [==============================] - 38s 4ms/step - loss: 6.5629e-05\n",
            "Epoch 31/100\n",
            "8553/8553 [==============================] - 31s 4ms/step - loss: 6.2957e-05\n",
            "Epoch 32/100\n",
            "8553/8553 [==============================] - 31s 4ms/step - loss: 6.2191e-05\n",
            "Epoch 33/100\n",
            "8553/8553 [==============================] - 30s 3ms/step - loss: 6.0076e-05\n",
            "Epoch 34/100\n",
            "8553/8553 [==============================] - 30s 3ms/step - loss: 5.6854e-05\n",
            "Epoch 35/100\n",
            "8553/8553 [==============================] - 38s 4ms/step - loss: 5.7555e-05\n",
            "Epoch 36/100\n",
            "8553/8553 [==============================] - 29s 3ms/step - loss: 5.4947e-05\n",
            "Epoch 37/100\n",
            "8553/8553 [==============================] - 28s 3ms/step - loss: 5.8623e-05\n",
            "Epoch 38/100\n",
            "8553/8553 [==============================] - 31s 4ms/step - loss: 5.4136e-05\n",
            "Epoch 39/100\n",
            "8553/8553 [==============================] - 33s 4ms/step - loss: 5.1520e-05\n",
            "Epoch 40/100\n",
            "8553/8553 [==============================] - 32s 4ms/step - loss: 5.0432e-05\n",
            "Epoch 41/100\n",
            "8553/8553 [==============================] - 31s 4ms/step - loss: 5.1388e-05\n",
            "Epoch 42/100\n",
            "8553/8553 [==============================] - 27s 3ms/step - loss: 5.0679e-05\n",
            "Epoch 43/100\n",
            "8553/8553 [==============================] - 31s 4ms/step - loss: 5.0485e-05\n",
            "Epoch 44/100\n",
            "8553/8553 [==============================] - 40s 5ms/step - loss: 4.7514e-05\n",
            "Epoch 45/100\n",
            "8553/8553 [==============================] - 28s 3ms/step - loss: 4.6079e-05\n",
            "Epoch 46/100\n",
            "8553/8553 [==============================] - 32s 4ms/step - loss: 4.6213e-05\n",
            "Epoch 47/100\n",
            "8553/8553 [==============================] - 29s 3ms/step - loss: 4.6488e-05\n",
            "Epoch 48/100\n",
            "8553/8553 [==============================] - 37s 4ms/step - loss: 4.8451e-05\n",
            "Epoch 49/100\n",
            "8553/8553 [==============================] - 33s 4ms/step - loss: 4.1968e-05\n",
            "Epoch 50/100\n",
            "8553/8553 [==============================] - 28s 3ms/step - loss: 4.3111e-05\n",
            "Epoch 51/100\n",
            "8553/8553 [==============================] - 32s 4ms/step - loss: 4.1925e-05\n",
            "Epoch 52/100\n",
            "8553/8553 [==============================] - 29s 3ms/step - loss: 4.2457e-05\n",
            "Epoch 53/100\n",
            "8553/8553 [==============================] - 39s 5ms/step - loss: 4.2048e-05\n",
            "Epoch 54/100\n",
            "8553/8553 [==============================] - 33s 4ms/step - loss: 3.9826e-05\n",
            "Epoch 55/100\n",
            "8553/8553 [==============================] - 29s 3ms/step - loss: 3.9622e-05\n",
            "Epoch 56/100\n",
            "8553/8553 [==============================] - 33s 4ms/step - loss: 4.1039e-05\n",
            "Epoch 57/100\n",
            "8553/8553 [==============================] - 34s 4ms/step - loss: 3.6860e-05\n",
            "Epoch 58/100\n",
            "8553/8553 [==============================] - 32s 4ms/step - loss: 4.0118e-05\n",
            "Epoch 59/100\n",
            "8553/8553 [==============================] - 30s 4ms/step - loss: 3.9633e-05\n",
            "Epoch 60/100\n",
            "8553/8553 [==============================] - 27s 3ms/step - loss: 3.7904e-05\n",
            "Epoch 61/100\n",
            "8553/8553 [==============================] - 28s 3ms/step - loss: 3.7452e-05\n",
            "Epoch 62/100\n",
            "8553/8553 [==============================] - 40s 5ms/step - loss: 3.6979e-05\n",
            "Epoch 63/100\n",
            "8553/8553 [==============================] - 27s 3ms/step - loss: 3.6680e-05\n",
            "Epoch 64/100\n",
            "8553/8553 [==============================] - 27s 3ms/step - loss: 3.7729e-05\n",
            "Epoch 65/100\n",
            "8553/8553 [==============================] - 30s 3ms/step - loss: 3.3266e-05\n",
            "Epoch 66/100\n",
            "8553/8553 [==============================] - 27s 3ms/step - loss: 3.7614e-05\n",
            "Epoch 67/100\n",
            "8553/8553 [==============================] - 36s 4ms/step - loss: 3.4251e-05\n",
            "Epoch 68/100\n",
            "8553/8553 [==============================] - 27s 3ms/step - loss: 3.4332e-05\n",
            "Epoch 69/100\n",
            "8553/8553 [==============================] - 30s 3ms/step - loss: 3.3252e-05\n",
            "Epoch 70/100\n",
            "8553/8553 [==============================] - 26s 3ms/step - loss: 3.2352e-05\n",
            "Epoch 71/100\n",
            "8553/8553 [==============================] - 27s 3ms/step - loss: 3.3070e-05\n",
            "Epoch 72/100\n",
            "8553/8553 [==============================] - 38s 4ms/step - loss: 3.2743e-05\n",
            "Epoch 73/100\n",
            "8553/8553 [==============================] - 28s 3ms/step - loss: 3.3010e-05\n",
            "Epoch 74/100\n",
            "8553/8553 [==============================] - 26s 3ms/step - loss: 3.3441e-05\n",
            "Epoch 75/100\n",
            "8553/8553 [==============================] - 27s 3ms/step - loss: 3.1595e-05\n",
            "Epoch 76/100\n",
            "8553/8553 [==============================] - 30s 3ms/step - loss: 3.5154e-05\n",
            "Epoch 77/100\n",
            "8553/8553 [==============================] - 34s 4ms/step - loss: 3.2013e-05\n",
            "Epoch 78/100\n",
            "8553/8553 [==============================] - 26s 3ms/step - loss: 3.2225e-05\n",
            "Epoch 79/100\n",
            "8553/8553 [==============================] - 29s 3ms/step - loss: 2.9028e-05\n",
            "Epoch 80/100\n",
            "8553/8553 [==============================] - 28s 3ms/step - loss: 3.1240e-05\n",
            "Epoch 81/100\n",
            "8553/8553 [==============================] - 27s 3ms/step - loss: 3.1437e-05\n",
            "Epoch 82/100\n",
            "8553/8553 [==============================] - 37s 4ms/step - loss: 3.0884e-05\n",
            "Epoch 83/100\n",
            "8553/8553 [==============================] - 31s 4ms/step - loss: 3.0023e-05\n",
            "Epoch 84/100\n",
            "8553/8553 [==============================] - 26s 3ms/step - loss: 3.0471e-05\n",
            "Epoch 85/100\n",
            "8553/8553 [==============================] - 26s 3ms/step - loss: 3.0214e-05\n",
            "Epoch 86/100\n",
            "8553/8553 [==============================] - 31s 4ms/step - loss: 3.0846e-05\n",
            "Epoch 87/100\n",
            "8553/8553 [==============================] - 36s 4ms/step - loss: 2.8224e-05\n",
            "Epoch 88/100\n",
            "8553/8553 [==============================] - 26s 3ms/step - loss: 2.9676e-05\n",
            "Epoch 89/100\n",
            "8553/8553 [==============================] - 28s 3ms/step - loss: 2.9520e-05\n",
            "Epoch 90/100\n",
            "8553/8553 [==============================] - 29s 3ms/step - loss: 2.8339e-05\n",
            "Epoch 91/100\n",
            "8553/8553 [==============================] - 31s 4ms/step - loss: 2.8798e-05\n",
            "Epoch 92/100\n",
            "8553/8553 [==============================] - 31s 4ms/step - loss: 2.8713e-05\n",
            "Epoch 93/100\n",
            "8553/8553 [==============================] - 27s 3ms/step - loss: 2.7648e-05\n",
            "Epoch 94/100\n",
            "8553/8553 [==============================] - 29s 3ms/step - loss: 2.9840e-05\n",
            "Epoch 95/100\n",
            "8553/8553 [==============================] - 26s 3ms/step - loss: 2.8059e-05\n",
            "Epoch 96/100\n",
            "8553/8553 [==============================] - 29s 3ms/step - loss: 2.7121e-05\n",
            "Epoch 97/100\n",
            "8553/8553 [==============================] - 35s 4ms/step - loss: 2.7336e-05\n",
            "Epoch 98/100\n",
            "8553/8553 [==============================] - 29s 3ms/step - loss: 2.7110e-05\n",
            "Epoch 99/100\n",
            "8553/8553 [==============================] - 26s 3ms/step - loss: 2.7702e-05\n",
            "Epoch 100/100\n",
            "8553/8553 [==============================] - 25s 3ms/step - loss: 2.5541e-05\n",
            "4277/4277 [==============================] - 11s 2ms/step\n",
            "4277/4277 [==============================] - 7s 2ms/step\n",
            "LSTM Validation on Randomised Pressure RMSE: 0.005044023473700997\n",
            "LSTM Test on Randomised Pressure RMSE: 0.005053760048732205\n"
          ]
        }
      ],
      "source": [
        "##Test pressure feature\n",
        "x_train_pressure_reshaped = x_train_pressure.reshape((x_train_pressure.shape[0], 1, x_train_pressure.shape[1]))\n",
        "x_val_reshaped = x_val.reshape((x_val.shape[0], 1, x_val.shape[1]))\n",
        "x_test_reshaped = x_test.reshape((x_test.shape[0], 1, x_test.shape[1]))\n",
        "model = Sequential()\n",
        "model.add(LSTM(50, activation='tanh', input_shape=(1, x_train_pressure_reshaped.shape[2])))\n",
        "model.add(Dense(1))\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "model.fit(x_train_pressure_reshaped, y_train, epochs=100, batch_size=32, verbose=1)\n",
        "# Predict with the model\n",
        "y_val_pressure_pred = model.predict(x_val_reshaped)\n",
        "y_test_pressure_pred = model.predict(x_test_reshaped)\n",
        "\n",
        "lstm_val_rmse_pressure = np.sqrt(mean_squared_error(y_val, y_val_pressure_pred))\n",
        "lstm_test_rmse_pressure = np.sqrt(mean_squared_error(y_test, y_test_pressure_pred))\n",
        "\n",
        "print(f\"LSTM Validation on Randomised Pressure RMSE: {lstm_val_rmse_pressure}\")\n",
        "print(f\"LSTM Test on Randomised Pressure RMSE: {lstm_test_rmse_pressure}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kmN7XL2UVX7t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7af78c92-b595-4521-9364-f4acff1acbec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "8553/8553 [==============================] - 42s 5ms/step - loss: 61.0832\n",
            "Epoch 2/100\n",
            "8553/8553 [==============================] - 27s 3ms/step - loss: 0.0141\n",
            "Epoch 3/100\n",
            "8553/8553 [==============================] - 26s 3ms/step - loss: 0.0039\n",
            "Epoch 4/100\n",
            "8553/8553 [==============================] - 29s 3ms/step - loss: 0.0017\n",
            "Epoch 5/100\n",
            "8553/8553 [==============================] - 29s 3ms/step - loss: 0.0010\n",
            "Epoch 6/100\n",
            "8553/8553 [==============================] - 36s 4ms/step - loss: 7.3001e-04\n",
            "Epoch 7/100\n",
            "8553/8553 [==============================] - 29s 3ms/step - loss: 5.6925e-04\n",
            "Epoch 8/100\n",
            "8553/8553 [==============================] - 28s 3ms/step - loss: 4.5364e-04\n",
            "Epoch 9/100\n",
            "8553/8553 [==============================] - 26s 3ms/step - loss: 3.8123e-04\n",
            "Epoch 10/100\n",
            "8553/8553 [==============================] - 27s 3ms/step - loss: 3.5110e-04\n",
            "Epoch 11/100\n",
            "8553/8553 [==============================] - 38s 4ms/step - loss: 3.0291e-04\n",
            "Epoch 12/100\n",
            "8553/8553 [==============================] - 27s 3ms/step - loss: 2.6870e-04\n",
            "Epoch 13/100\n",
            "8553/8553 [==============================] - 26s 3ms/step - loss: 2.4384e-04\n",
            "Epoch 14/100\n",
            "8553/8553 [==============================] - 30s 4ms/step - loss: 2.2731e-04\n",
            "Epoch 15/100\n",
            "8553/8553 [==============================] - 28s 3ms/step - loss: 2.0601e-04\n",
            "Epoch 16/100\n",
            "8553/8553 [==============================] - 36s 4ms/step - loss: 1.9521e-04\n",
            "Epoch 17/100\n",
            "8553/8553 [==============================] - 28s 3ms/step - loss: 1.7772e-04\n",
            "Epoch 18/100\n",
            "8553/8553 [==============================] - 29s 3ms/step - loss: 1.7570e-04\n",
            "Epoch 19/100\n",
            "8553/8553 [==============================] - 26s 3ms/step - loss: 1.5719e-04\n",
            "Epoch 20/100\n",
            "8553/8553 [==============================] - 27s 3ms/step - loss: 1.5322e-04\n",
            "Epoch 21/100\n",
            "8553/8553 [==============================] - 37s 4ms/step - loss: 1.4638e-04\n",
            "Epoch 22/100\n",
            "8553/8553 [==============================] - 28s 3ms/step - loss: 1.4194e-04\n",
            "Epoch 23/100\n",
            "8553/8553 [==============================] - 27s 3ms/step - loss: 1.3674e-04\n",
            "Epoch 24/100\n",
            "8553/8553 [==============================] - 30s 3ms/step - loss: 1.3090e-04\n",
            "Epoch 25/100\n",
            "8553/8553 [==============================] - 27s 3ms/step - loss: 1.1711e-04\n",
            "Epoch 26/100\n",
            "8553/8553 [==============================] - 36s 4ms/step - loss: 1.1897e-04\n",
            "Epoch 27/100\n",
            "8553/8553 [==============================] - 26s 3ms/step - loss: 1.1729e-04\n",
            "Epoch 28/100\n",
            "8553/8553 [==============================] - 29s 3ms/step - loss: 1.0573e-04\n",
            "Epoch 29/100\n",
            "8553/8553 [==============================] - 27s 3ms/step - loss: 1.0960e-04\n",
            "Epoch 30/100\n",
            "8553/8553 [==============================] - 26s 3ms/step - loss: 1.0849e-04\n",
            "Epoch 31/100\n",
            "8553/8553 [==============================] - 36s 4ms/step - loss: 1.0066e-04\n",
            "Epoch 32/100\n",
            "8553/8553 [==============================] - 30s 4ms/step - loss: 9.7697e-05\n",
            "Epoch 33/100\n",
            "8553/8553 [==============================] - 26s 3ms/step - loss: 9.7569e-05\n",
            "Epoch 34/100\n",
            "8553/8553 [==============================] - 27s 3ms/step - loss: 9.1894e-05\n",
            "Epoch 35/100\n",
            "8553/8553 [==============================] - 32s 4ms/step - loss: 8.8398e-05\n",
            "Epoch 36/100\n",
            "8553/8553 [==============================] - 35s 4ms/step - loss: 8.9027e-05\n",
            "Epoch 37/100\n",
            "8553/8553 [==============================] - 29s 3ms/step - loss: 8.3083e-05\n",
            "Epoch 38/100\n",
            "8553/8553 [==============================] - 31s 4ms/step - loss: 8.3641e-05\n",
            "Epoch 39/100\n",
            "8553/8553 [==============================] - 27s 3ms/step - loss: 8.3325e-05\n",
            "Epoch 40/100\n",
            "8553/8553 [==============================] - 27s 3ms/step - loss: 8.2183e-05\n",
            "Epoch 41/100\n",
            "8553/8553 [==============================] - 39s 5ms/step - loss: 7.9896e-05\n",
            "Epoch 42/100\n",
            "8553/8553 [==============================] - 28s 3ms/step - loss: 7.6020e-05\n",
            "Epoch 43/100\n",
            "8553/8553 [==============================] - 26s 3ms/step - loss: 7.9606e-05\n",
            "Epoch 44/100\n",
            "8553/8553 [==============================] - 29s 3ms/step - loss: 7.6018e-05\n",
            "Epoch 45/100\n",
            "8553/8553 [==============================] - 29s 3ms/step - loss: 7.1053e-05\n",
            "Epoch 46/100\n",
            "8553/8553 [==============================] - 36s 4ms/step - loss: 7.3241e-05\n",
            "Epoch 47/100\n",
            "8553/8553 [==============================] - 31s 4ms/step - loss: 7.1189e-05\n",
            "Epoch 48/100\n",
            "8553/8553 [==============================] - 29s 3ms/step - loss: 6.9970e-05\n",
            "Epoch 49/100\n",
            "8553/8553 [==============================] - 26s 3ms/step - loss: 7.0826e-05\n",
            "Epoch 50/100\n",
            "8553/8553 [==============================] - 31s 4ms/step - loss: 6.4647e-05\n",
            "Epoch 51/100\n",
            "8553/8553 [==============================] - 39s 5ms/step - loss: 7.2687e-05\n",
            "Epoch 52/100\n",
            "8553/8553 [==============================] - 28s 3ms/step - loss: 6.5885e-05\n",
            "Epoch 53/100\n",
            "8553/8553 [==============================] - 31s 4ms/step - loss: 6.3956e-05\n",
            "Epoch 54/100\n",
            "8553/8553 [==============================] - 28s 3ms/step - loss: 6.5525e-05\n",
            "Epoch 55/100\n",
            "8553/8553 [==============================] - 28s 3ms/step - loss: 6.6466e-05\n",
            "Epoch 56/100\n",
            "8553/8553 [==============================] - 41s 5ms/step - loss: 6.8611e-05\n",
            "Epoch 57/100\n",
            "8553/8553 [==============================] - 27s 3ms/step - loss: 6.3110e-05\n",
            "Epoch 58/100\n",
            "8553/8553 [==============================] - 30s 3ms/step - loss: 6.0640e-05\n",
            "Epoch 59/100\n",
            "8553/8553 [==============================] - 30s 3ms/step - loss: 6.0273e-05\n",
            "Epoch 60/100\n",
            "8553/8553 [==============================] - 30s 4ms/step - loss: 5.9162e-05\n",
            "Epoch 61/100\n",
            "8553/8553 [==============================] - 34s 4ms/step - loss: 5.6527e-05\n",
            "Epoch 62/100\n",
            "8553/8553 [==============================] - 29s 3ms/step - loss: 5.7492e-05\n",
            "Epoch 63/100\n",
            "8553/8553 [==============================] - 26s 3ms/step - loss: 6.0271e-05\n",
            "Epoch 64/100\n",
            "8553/8553 [==============================] - 27s 3ms/step - loss: 5.5135e-05\n",
            "Epoch 65/100\n",
            "8553/8553 [==============================] - 30s 4ms/step - loss: 6.0631e-05\n",
            "Epoch 66/100\n",
            "8553/8553 [==============================] - 36s 4ms/step - loss: 5.5759e-05\n",
            "Epoch 67/100\n",
            "8553/8553 [==============================] - 26s 3ms/step - loss: 5.5330e-05\n",
            "Epoch 68/100\n",
            "8553/8553 [==============================] - 29s 3ms/step - loss: 5.5471e-05\n",
            "Epoch 69/100\n",
            "8553/8553 [==============================] - 28s 3ms/step - loss: 5.5603e-05\n",
            "Epoch 70/100\n",
            "8553/8553 [==============================] - 28s 3ms/step - loss: 5.4389e-05\n",
            "Epoch 71/100\n",
            "8553/8553 [==============================] - 35s 4ms/step - loss: 5.6437e-05\n",
            "Epoch 72/100\n",
            "8553/8553 [==============================] - 31s 4ms/step - loss: 5.4621e-05\n",
            "Epoch 73/100\n",
            "8553/8553 [==============================] - 27s 3ms/step - loss: 5.2930e-05\n",
            "Epoch 74/100\n",
            "8553/8553 [==============================] - 31s 4ms/step - loss: 4.9495e-05\n",
            "Epoch 75/100\n",
            "8553/8553 [==============================] - 32s 4ms/step - loss: 5.4108e-05\n",
            "Epoch 76/100\n",
            "8553/8553 [==============================] - 34s 4ms/step - loss: 5.2810e-05\n",
            "Epoch 77/100\n",
            "8553/8553 [==============================] - 30s 3ms/step - loss: 5.2635e-05\n",
            "Epoch 78/100\n",
            "8553/8553 [==============================] - 30s 4ms/step - loss: 4.7993e-05\n",
            "Epoch 79/100\n",
            "8553/8553 [==============================] - 27s 3ms/step - loss: 4.7727e-05\n",
            "Epoch 80/100\n",
            "8553/8553 [==============================] - 36s 4ms/step - loss: 5.4488e-05\n",
            "Epoch 81/100\n",
            "8553/8553 [==============================] - 33s 4ms/step - loss: 5.1592e-05\n",
            "Epoch 82/100\n",
            "8553/8553 [==============================] - 28s 3ms/step - loss: 4.8541e-05\n",
            "Epoch 83/100\n",
            "8553/8553 [==============================] - 31s 4ms/step - loss: 4.7876e-05\n",
            "Epoch 84/100\n",
            "8553/8553 [==============================] - 28s 3ms/step - loss: 4.8991e-05\n",
            "Epoch 85/100\n",
            "8553/8553 [==============================] - 36s 4ms/step - loss: 4.7918e-05\n",
            "Epoch 86/100\n",
            "8553/8553 [==============================] - 32s 4ms/step - loss: 4.9363e-05\n",
            "Epoch 87/100\n",
            "8553/8553 [==============================] - 28s 3ms/step - loss: 4.7508e-05\n",
            "Epoch 88/100\n",
            "8553/8553 [==============================] - 28s 3ms/step - loss: 4.3485e-05\n",
            "Epoch 89/100\n",
            "8553/8553 [==============================] - 31s 4ms/step - loss: 4.4114e-05\n",
            "Epoch 90/100\n",
            "8553/8553 [==============================] - 36s 4ms/step - loss: 4.5481e-05\n",
            "Epoch 91/100\n",
            "8553/8553 [==============================] - 29s 3ms/step - loss: 4.2872e-05\n",
            "Epoch 92/100\n",
            "8553/8553 [==============================] - 32s 4ms/step - loss: 4.5285e-05\n",
            "Epoch 93/100\n",
            "8553/8553 [==============================] - 27s 3ms/step - loss: 4.6023e-05\n",
            "Epoch 94/100\n",
            "8553/8553 [==============================] - 30s 4ms/step - loss: 4.2707e-05\n",
            "Epoch 95/100\n",
            "8553/8553 [==============================] - 40s 5ms/step - loss: 4.6949e-05\n",
            "Epoch 96/100\n",
            "8553/8553 [==============================] - 27s 3ms/step - loss: 4.4866e-05\n",
            "Epoch 97/100\n",
            "8553/8553 [==============================] - 31s 4ms/step - loss: 4.0364e-05\n",
            "Epoch 98/100\n",
            "8553/8553 [==============================] - 30s 3ms/step - loss: 4.2261e-05\n",
            "Epoch 99/100\n",
            "8553/8553 [==============================] - 33s 4ms/step - loss: 4.3226e-05\n",
            "Epoch 100/100\n",
            "8553/8553 [==============================] - 35s 4ms/step - loss: 4.1009e-05\n",
            "4277/4277 [==============================] - 8s 2ms/step\n",
            "4277/4277 [==============================] - 10s 2ms/step\n",
            "LSTM Validation on Randomised Lat RMSE: 0.003423123880057231\n",
            "LSTM Test on Randomised Lat RMSE: 0.0034471933991493315\n"
          ]
        }
      ],
      "source": [
        "##Test lat feature\n",
        "x_train_lat_reshaped = x_train_lat.reshape((x_train_lat.shape[0], 1, x_train_lat.shape[1]))\n",
        "x_val_reshaped = x_val.reshape((x_val.shape[0], 1, x_val.shape[1]))\n",
        "x_test_reshaped = x_test.reshape((x_test.shape[0], 1, x_test.shape[1]))\n",
        "model = Sequential()\n",
        "model.add(LSTM(50, activation='tanh', input_shape=(1, x_train_lat_reshaped.shape[2])))\n",
        "model.add(Dense(1))\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "model.fit(x_train_lat_reshaped, y_train, epochs=100, batch_size=32, verbose=1)\n",
        "# Predict with the model\n",
        "y_val_lat_pred = model.predict(x_val_reshaped)\n",
        "y_test_lat_pred = model.predict(x_test_reshaped)\n",
        "\n",
        "lstm_val_rmse_lat = np.sqrt(mean_squared_error(y_val, y_val_lat_pred))\n",
        "lstm_test_rmse_lat = np.sqrt(mean_squared_error(y_test, y_test_lat_pred))\n",
        "\n",
        "print(f\"LSTM Validation on Randomised Lat RMSE: {lstm_val_rmse_lat}\")\n",
        "print(f\"LSTM Test on Randomised Lat RMSE: {lstm_test_rmse_lat}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RlaQbWQAVrOv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7792a52e-e404-4901-c0a3-a0df8799c0f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "8553/8553 [==============================] - 30s 3ms/step - loss: 61.5771\n",
            "Epoch 2/100\n",
            "8553/8553 [==============================] - 33s 4ms/step - loss: 0.0085\n",
            "Epoch 3/100\n",
            "8553/8553 [==============================] - 34s 4ms/step - loss: 0.0017\n",
            "Epoch 4/100\n",
            "8553/8553 [==============================] - 35s 4ms/step - loss: 7.5126e-04\n",
            "Epoch 5/100\n",
            "8553/8553 [==============================] - 31s 4ms/step - loss: 4.6101e-04\n",
            "Epoch 6/100\n",
            "8553/8553 [==============================] - 29s 3ms/step - loss: 3.3203e-04\n",
            "Epoch 7/100\n",
            "8553/8553 [==============================] - 31s 4ms/step - loss: 2.6756e-04\n",
            "Epoch 8/100\n",
            "8553/8553 [==============================] - 38s 4ms/step - loss: 2.3032e-04\n",
            "Epoch 9/100\n",
            "8553/8553 [==============================] - 31s 4ms/step - loss: 2.0674e-04\n",
            "Epoch 10/100\n",
            "8553/8553 [==============================] - 30s 4ms/step - loss: 1.8856e-04\n",
            "Epoch 11/100\n",
            "8553/8553 [==============================] - 31s 4ms/step - loss: 1.6974e-04\n",
            "Epoch 12/100\n",
            "8553/8553 [==============================] - 35s 4ms/step - loss: 1.6510e-04\n",
            "Epoch 13/100\n",
            "8553/8553 [==============================] - 36s 4ms/step - loss: 1.4910e-04\n",
            "Epoch 14/100\n",
            "8553/8553 [==============================] - 34s 4ms/step - loss: 1.4423e-04\n",
            "Epoch 15/100\n",
            "8553/8553 [==============================] - 32s 4ms/step - loss: 1.3447e-04\n",
            "Epoch 16/100\n",
            "8553/8553 [==============================] - 32s 4ms/step - loss: 1.3139e-04\n",
            "Epoch 17/100\n",
            "8553/8553 [==============================] - 40s 5ms/step - loss: 1.2434e-04\n",
            "Epoch 18/100\n",
            "8553/8553 [==============================] - 37s 4ms/step - loss: 1.1917e-04\n",
            "Epoch 19/100\n",
            "8553/8553 [==============================] - 32s 4ms/step - loss: 1.1816e-04\n",
            "Epoch 20/100\n",
            "8553/8553 [==============================] - 33s 4ms/step - loss: 1.1409e-04\n",
            "Epoch 21/100\n",
            "8553/8553 [==============================] - 42s 5ms/step - loss: 1.0959e-04\n",
            "Epoch 22/100\n",
            "8553/8553 [==============================] - 32s 4ms/step - loss: 1.0621e-04\n",
            "Epoch 23/100\n",
            "8553/8553 [==============================] - 33s 4ms/step - loss: 1.0022e-04\n",
            "Epoch 24/100\n",
            "8553/8553 [==============================] - 31s 4ms/step - loss: 9.7453e-05\n",
            "Epoch 25/100\n",
            "8553/8553 [==============================] - 37s 4ms/step - loss: 9.3839e-05\n",
            "Epoch 26/100\n",
            "8553/8553 [==============================] - 38s 4ms/step - loss: 9.7549e-05\n",
            "Epoch 27/100\n",
            "8553/8553 [==============================] - 33s 4ms/step - loss: 9.0975e-05\n",
            "Epoch 28/100\n",
            "8553/8553 [==============================] - 30s 4ms/step - loss: 9.2902e-05\n",
            "Epoch 29/100\n",
            "8553/8553 [==============================] - 32s 4ms/step - loss: 9.1350e-05\n",
            "Epoch 30/100\n",
            "8553/8553 [==============================] - 40s 5ms/step - loss: 8.5168e-05\n",
            "Epoch 31/100\n",
            "8553/8553 [==============================] - 30s 3ms/step - loss: 8.4019e-05\n",
            "Epoch 32/100\n",
            "8553/8553 [==============================] - 33s 4ms/step - loss: 8.3526e-05\n",
            "Epoch 33/100\n",
            "8553/8553 [==============================] - 29s 3ms/step - loss: 8.0791e-05\n",
            "Epoch 34/100\n",
            "8553/8553 [==============================] - 37s 4ms/step - loss: 8.1924e-05\n",
            "Epoch 35/100\n",
            "8553/8553 [==============================] - 34s 4ms/step - loss: 7.7392e-05\n",
            "Epoch 36/100\n",
            "8553/8553 [==============================] - 31s 4ms/step - loss: 7.9465e-05\n",
            "Epoch 37/100\n",
            "8553/8553 [==============================] - 31s 4ms/step - loss: 7.8633e-05\n",
            "Epoch 38/100\n",
            "8553/8553 [==============================] - 30s 3ms/step - loss: 7.6170e-05\n",
            "Epoch 39/100\n",
            "8553/8553 [==============================] - 43s 5ms/step - loss: 7.0621e-05\n",
            "Epoch 40/100\n",
            "8553/8553 [==============================] - 29s 3ms/step - loss: 7.3968e-05\n",
            "Epoch 41/100\n",
            "8553/8553 [==============================] - 33s 4ms/step - loss: 7.2546e-05\n",
            "Epoch 42/100\n",
            "8553/8553 [==============================] - 29s 3ms/step - loss: 7.0513e-05\n",
            "Epoch 43/100\n",
            "8553/8553 [==============================] - 36s 4ms/step - loss: 7.0740e-05\n",
            "Epoch 44/100\n",
            "8553/8553 [==============================] - 39s 5ms/step - loss: 6.9880e-05\n",
            "Epoch 45/100\n",
            "8553/8553 [==============================] - 34s 4ms/step - loss: 6.6567e-05\n",
            "Epoch 46/100\n",
            "8553/8553 [==============================] - 30s 3ms/step - loss: 6.6104e-05\n",
            "Epoch 47/100\n",
            "8553/8553 [==============================] - 35s 4ms/step - loss: 6.3197e-05\n",
            "Epoch 48/100\n",
            "8553/8553 [==============================] - 39s 5ms/step - loss: 6.3514e-05\n",
            "Epoch 49/100\n",
            "8553/8553 [==============================] - 31s 4ms/step - loss: 6.3951e-05\n",
            "Epoch 50/100\n",
            "8553/8553 [==============================] - 31s 4ms/step - loss: 6.5992e-05\n",
            "Epoch 51/100\n",
            "8553/8553 [==============================] - 31s 4ms/step - loss: 6.0908e-05\n",
            "Epoch 52/100\n",
            "8553/8553 [==============================] - 36s 4ms/step - loss: 6.0283e-05\n",
            "Epoch 53/100\n",
            "8553/8553 [==============================] - 35s 4ms/step - loss: 6.2848e-05\n",
            "Epoch 54/100\n",
            "8553/8553 [==============================] - 33s 4ms/step - loss: 6.3171e-05\n",
            "Epoch 55/100\n",
            "8553/8553 [==============================] - 28s 3ms/step - loss: 5.4578e-05\n",
            "Epoch 56/100\n",
            "8553/8553 [==============================] - 29s 3ms/step - loss: 5.9249e-05\n",
            "Epoch 57/100\n",
            "8553/8553 [==============================] - 36s 4ms/step - loss: 6.0820e-05\n",
            "Epoch 58/100\n",
            "8553/8553 [==============================] - 28s 3ms/step - loss: 6.0360e-05\n",
            "Epoch 59/100\n",
            "8553/8553 [==============================] - 29s 3ms/step - loss: 5.9278e-05\n",
            "Epoch 60/100\n",
            "8553/8553 [==============================] - 31s 4ms/step - loss: 5.3732e-05\n",
            "Epoch 61/100\n",
            "8553/8553 [==============================] - 30s 4ms/step - loss: 5.6030e-05\n",
            "Epoch 62/100\n",
            "8553/8553 [==============================] - 43s 5ms/step - loss: 5.5968e-05\n",
            "Epoch 63/100\n",
            "8553/8553 [==============================] - 29s 3ms/step - loss: 5.6395e-05\n",
            "Epoch 64/100\n",
            "8553/8553 [==============================] - 31s 4ms/step - loss: 5.4220e-05\n",
            "Epoch 65/100\n",
            "8553/8553 [==============================] - 29s 3ms/step - loss: 5.5497e-05\n",
            "Epoch 66/100\n",
            "8553/8553 [==============================] - 28s 3ms/step - loss: 5.1459e-05\n",
            "Epoch 67/100\n",
            "8553/8553 [==============================] - 39s 5ms/step - loss: 5.1839e-05\n",
            "Epoch 68/100\n",
            "8553/8553 [==============================] - 31s 4ms/step - loss: 5.1797e-05\n",
            "Epoch 69/100\n",
            "8553/8553 [==============================] - 28s 3ms/step - loss: 5.0795e-05\n",
            "Epoch 70/100\n",
            "8553/8553 [==============================] - 32s 4ms/step - loss: 5.0578e-05\n",
            "Epoch 71/100\n",
            "8553/8553 [==============================] - 34s 4ms/step - loss: 5.1486e-05\n",
            "Epoch 72/100\n",
            "8553/8553 [==============================] - 36s 4ms/step - loss: 4.7540e-05\n",
            "Epoch 73/100\n",
            "8553/8553 [==============================] - 31s 4ms/step - loss: 5.1032e-05\n",
            "Epoch 74/100\n",
            "8553/8553 [==============================] - 28s 3ms/step - loss: 5.0610e-05\n",
            "Epoch 75/100\n",
            "8553/8553 [==============================] - 33s 4ms/step - loss: 4.9791e-05\n",
            "Epoch 76/100\n",
            "8553/8553 [==============================] - 38s 4ms/step - loss: 4.8218e-05\n",
            "Epoch 77/100\n",
            "8553/8553 [==============================] - 36s 4ms/step - loss: 4.8614e-05\n",
            "Epoch 78/100\n",
            "8553/8553 [==============================] - 29s 3ms/step - loss: 4.7490e-05\n",
            "Epoch 79/100\n",
            "8553/8553 [==============================] - 35s 4ms/step - loss: 4.7451e-05\n",
            "Epoch 80/100\n",
            "8553/8553 [==============================] - 35s 4ms/step - loss: 4.6195e-05\n",
            "Epoch 81/100\n",
            "8553/8553 [==============================] - 37s 4ms/step - loss: 4.7185e-05\n",
            "Epoch 82/100\n",
            "8553/8553 [==============================] - 31s 4ms/step - loss: 4.7083e-05\n",
            "Epoch 83/100\n",
            "8553/8553 [==============================] - 32s 4ms/step - loss: 4.6650e-05\n",
            "Epoch 84/100\n",
            "8553/8553 [==============================] - 31s 4ms/step - loss: 4.5638e-05\n",
            "Epoch 85/100\n",
            "8553/8553 [==============================] - 38s 4ms/step - loss: 4.6218e-05\n",
            "Epoch 86/100\n",
            "8553/8553 [==============================] - 33s 4ms/step - loss: 4.2453e-05\n",
            "Epoch 87/100\n",
            "8553/8553 [==============================] - 29s 3ms/step - loss: 4.3338e-05\n",
            "Epoch 88/100\n",
            "8553/8553 [==============================] - 32s 4ms/step - loss: 4.2300e-05\n",
            "Epoch 89/100\n",
            "8553/8553 [==============================] - 33s 4ms/step - loss: 4.4307e-05\n",
            "Epoch 90/100\n",
            "8553/8553 [==============================] - 38s 4ms/step - loss: 4.2070e-05\n",
            "Epoch 91/100\n",
            "8553/8553 [==============================] - 33s 4ms/step - loss: 4.3840e-05\n",
            "Epoch 92/100\n",
            "8553/8553 [==============================] - 32s 4ms/step - loss: 4.2545e-05\n",
            "Epoch 93/100\n",
            "8553/8553 [==============================] - 32s 4ms/step - loss: 4.4733e-05\n",
            "Epoch 94/100\n",
            "8553/8553 [==============================] - 38s 4ms/step - loss: 3.9566e-05\n",
            "Epoch 95/100\n",
            "8553/8553 [==============================] - 34s 4ms/step - loss: 4.4983e-05\n",
            "Epoch 96/100\n",
            "8553/8553 [==============================] - 30s 3ms/step - loss: 4.2814e-05\n",
            "Epoch 97/100\n",
            "8553/8553 [==============================] - 35s 4ms/step - loss: 4.3171e-05\n",
            "Epoch 98/100\n",
            "8553/8553 [==============================] - 32s 4ms/step - loss: 3.9362e-05\n",
            "Epoch 99/100\n",
            "8553/8553 [==============================] - 40s 5ms/step - loss: 4.4770e-05\n",
            "Epoch 100/100\n",
            "8553/8553 [==============================] - 30s 4ms/step - loss: 3.9987e-05\n",
            "4277/4277 [==============================] - 12s 3ms/step\n",
            "4277/4277 [==============================] - 10s 2ms/step\n",
            "LSTM Validation on Randomised Lon RMSE: 0.007456834958833353\n",
            "LSTM Test on Randomised Lon RMSE: 0.007441779995265137\n"
          ]
        }
      ],
      "source": [
        "##Test lon feature\n",
        "x_train_lon_reshaped = x_train_lon.reshape((x_train_lon.shape[0], 1, x_train_lon.shape[1]))\n",
        "x_val_reshaped = x_val.reshape((x_val.shape[0], 1, x_val.shape[1]))\n",
        "x_test_reshaped = x_test.reshape((x_test.shape[0], 1, x_test.shape[1]))\n",
        "model = Sequential()\n",
        "model.add(LSTM(50, activation='tanh', input_shape=(1, x_train_lon_reshaped.shape[2])))\n",
        "model.add(Dense(1))\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "model.fit(x_train_lon_reshaped, y_train, epochs=100, batch_size=32, verbose=1)\n",
        "# Predict with the model\n",
        "y_val_lon_pred = model.predict(x_val_reshaped)\n",
        "y_test_lon_pred = model.predict(x_test_reshaped)\n",
        "\n",
        "lstm_val_rmse_lon = np.sqrt(mean_squared_error(y_val, y_val_lon_pred))\n",
        "lstm_test_rmse_lon = np.sqrt(mean_squared_error(y_test, y_test_lon_pred))\n",
        "\n",
        "print(f\"LSTM Validation on Randomised Lon RMSE: {lstm_val_rmse_lon}\")\n",
        "print(f\"LSTM Test on Randomised Lon RMSE: {lstm_test_rmse_lon}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FZurENQGV5lq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e591b56b-4b9c-4246-a7c7-b0c14acd6ee8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "8553/8553 [==============================] - 30s 3ms/step - loss: 60.3476\n",
            "Epoch 2/100\n",
            "8553/8553 [==============================] - 40s 5ms/step - loss: 0.0065\n",
            "Epoch 3/100\n",
            "8553/8553 [==============================] - 35s 4ms/step - loss: 0.0013\n",
            "Epoch 4/100\n",
            "8553/8553 [==============================] - 32s 4ms/step - loss: 6.6766e-04\n",
            "Epoch 5/100\n",
            "8553/8553 [==============================] - 31s 4ms/step - loss: 4.2855e-04\n",
            "Epoch 6/100\n",
            "8553/8553 [==============================] - 36s 4ms/step - loss: 3.2410e-04\n",
            "Epoch 7/100\n",
            "8553/8553 [==============================] - 41s 5ms/step - loss: 2.6609e-04\n",
            "Epoch 8/100\n",
            "8553/8553 [==============================] - 31s 4ms/step - loss: 2.3346e-04\n",
            "Epoch 9/100\n",
            "8553/8553 [==============================] - 34s 4ms/step - loss: 2.0479e-04\n",
            "Epoch 10/100\n",
            "8553/8553 [==============================] - 30s 4ms/step - loss: 1.7953e-04\n",
            "Epoch 11/100\n",
            "8553/8553 [==============================] - 44s 5ms/step - loss: 1.6040e-04\n",
            "Epoch 12/100\n",
            "8553/8553 [==============================] - 34s 4ms/step - loss: 1.4256e-04\n",
            "Epoch 13/100\n",
            "8553/8553 [==============================] - 33s 4ms/step - loss: 1.4029e-04\n",
            "Epoch 14/100\n",
            "8553/8553 [==============================] - 33s 4ms/step - loss: 1.2838e-04\n",
            "Epoch 15/100\n",
            "8553/8553 [==============================] - 43s 5ms/step - loss: 1.2150e-04\n",
            "Epoch 16/100\n",
            "8553/8553 [==============================] - 35s 4ms/step - loss: 1.1327e-04\n",
            "Epoch 17/100\n",
            "8553/8553 [==============================] - 30s 4ms/step - loss: 1.0983e-04\n",
            "Epoch 18/100\n",
            "8553/8553 [==============================] - 31s 4ms/step - loss: 1.0063e-04\n",
            "Epoch 19/100\n",
            "8553/8553 [==============================] - 29s 3ms/step - loss: 9.8730e-05\n",
            "Epoch 20/100\n",
            "8553/8553 [==============================] - 41s 5ms/step - loss: 9.7226e-05\n",
            "Epoch 21/100\n",
            "8553/8553 [==============================] - 29s 3ms/step - loss: 9.1265e-05\n",
            "Epoch 22/100\n",
            "8553/8553 [==============================] - 29s 3ms/step - loss: 8.4253e-05\n",
            "Epoch 23/100\n",
            "8553/8553 [==============================] - 31s 4ms/step - loss: 8.5861e-05\n",
            "Epoch 24/100\n",
            "8553/8553 [==============================] - 32s 4ms/step - loss: 8.2157e-05\n",
            "Epoch 25/100\n",
            "8553/8553 [==============================] - 37s 4ms/step - loss: 7.7881e-05\n",
            "Epoch 26/100\n",
            "8553/8553 [==============================] - 28s 3ms/step - loss: 7.3541e-05\n",
            "Epoch 27/100\n",
            "8553/8553 [==============================] - 32s 4ms/step - loss: 7.7274e-05\n",
            "Epoch 28/100\n",
            "8553/8553 [==============================] - 27s 3ms/step - loss: 7.2472e-05\n",
            "Epoch 29/100\n",
            "8553/8553 [==============================] - 33s 4ms/step - loss: 6.8882e-05\n",
            "Epoch 30/100\n",
            "8553/8553 [==============================] - 35s 4ms/step - loss: 6.5974e-05\n",
            "Epoch 31/100\n",
            "8553/8553 [==============================] - 28s 3ms/step - loss: 6.5677e-05\n",
            "Epoch 32/100\n",
            "8553/8553 [==============================] - 32s 4ms/step - loss: 6.6256e-05\n",
            "Epoch 33/100\n",
            "8553/8553 [==============================] - 28s 3ms/step - loss: 6.2633e-05\n",
            "Epoch 34/100\n",
            "8553/8553 [==============================] - 33s 4ms/step - loss: 6.0435e-05\n",
            "Epoch 35/100\n",
            "8553/8553 [==============================] - 35s 4ms/step - loss: 5.8733e-05\n",
            "Epoch 36/100\n",
            "8553/8553 [==============================] - 29s 3ms/step - loss: 5.9399e-05\n",
            "Epoch 37/100\n",
            "8553/8553 [==============================] - 30s 4ms/step - loss: 5.5966e-05\n",
            "Epoch 38/100\n",
            "8553/8553 [==============================] - 28s 3ms/step - loss: 5.9301e-05\n",
            "Epoch 39/100\n",
            "8553/8553 [==============================] - 36s 4ms/step - loss: 5.3820e-05\n",
            "Epoch 40/100\n",
            "8553/8553 [==============================] - 35s 4ms/step - loss: 5.3161e-05\n",
            "Epoch 41/100\n",
            "8553/8553 [==============================] - 32s 4ms/step - loss: 5.3986e-05\n",
            "Epoch 42/100\n",
            "8553/8553 [==============================] - 28s 3ms/step - loss: 5.3210e-05\n",
            "Epoch 43/100\n",
            "8553/8553 [==============================] - 28s 3ms/step - loss: 4.9349e-05\n",
            "Epoch 44/100\n",
            "8553/8553 [==============================] - 36s 4ms/step - loss: 5.0343e-05\n",
            "Epoch 45/100\n",
            "8553/8553 [==============================] - 31s 4ms/step - loss: 4.9625e-05\n",
            "Epoch 46/100\n",
            "8553/8553 [==============================] - 32s 4ms/step - loss: 4.7933e-05\n",
            "Epoch 47/100\n",
            "8553/8553 [==============================] - 27s 3ms/step - loss: 4.5858e-05\n",
            "Epoch 48/100\n",
            "8553/8553 [==============================] - 28s 3ms/step - loss: 4.6613e-05\n",
            "Epoch 49/100\n",
            "8553/8553 [==============================] - 36s 4ms/step - loss: 4.6789e-05\n",
            "Epoch 50/100\n",
            "8553/8553 [==============================] - 33s 4ms/step - loss: 4.3261e-05\n",
            "Epoch 51/100\n",
            "8553/8553 [==============================] - 33s 4ms/step - loss: 4.5262e-05\n",
            "Epoch 52/100\n",
            "8553/8553 [==============================] - 29s 3ms/step - loss: 4.4510e-05\n",
            "Epoch 53/100\n",
            "8553/8553 [==============================] - 32s 4ms/step - loss: 4.3914e-05\n",
            "Epoch 54/100\n",
            "8553/8553 [==============================] - 36s 4ms/step - loss: 4.2383e-05\n",
            "Epoch 55/100\n",
            "8553/8553 [==============================] - 32s 4ms/step - loss: 4.4459e-05\n",
            "Epoch 56/100\n",
            "8553/8553 [==============================] - 29s 3ms/step - loss: 4.1733e-05\n",
            "Epoch 57/100\n",
            "8553/8553 [==============================] - 27s 3ms/step - loss: 4.0827e-05\n",
            "Epoch 58/100\n",
            "8553/8553 [==============================] - 32s 4ms/step - loss: 3.9447e-05\n",
            "Epoch 59/100\n",
            "8553/8553 [==============================] - 36s 4ms/step - loss: 3.8318e-05\n",
            "Epoch 60/100\n",
            "8553/8553 [==============================] - 33s 4ms/step - loss: 3.9381e-05\n",
            "Epoch 61/100\n",
            "8553/8553 [==============================] - 31s 4ms/step - loss: 3.9165e-05\n",
            "Epoch 62/100\n",
            "8553/8553 [==============================] - 31s 4ms/step - loss: 3.5838e-05\n",
            "Epoch 63/100\n",
            "8553/8553 [==============================] - 31s 4ms/step - loss: 3.7295e-05\n",
            "Epoch 64/100\n",
            "8553/8553 [==============================] - 40s 5ms/step - loss: 3.7340e-05\n",
            "Epoch 65/100\n",
            "8553/8553 [==============================] - 31s 4ms/step - loss: 3.6304e-05\n",
            "Epoch 66/100\n",
            "8553/8553 [==============================] - 28s 3ms/step - loss: 3.7001e-05\n",
            "Epoch 67/100\n",
            "8553/8553 [==============================] - 32s 4ms/step - loss: 3.5120e-05\n",
            "Epoch 68/100\n",
            "8553/8553 [==============================] - 28s 3ms/step - loss: 3.6848e-05\n",
            "Epoch 69/100\n",
            "8553/8553 [==============================] - 40s 5ms/step - loss: 3.6314e-05\n",
            "Epoch 70/100\n",
            "8553/8553 [==============================] - 30s 4ms/step - loss: 3.4368e-05\n",
            "Epoch 71/100\n",
            "8553/8553 [==============================] - 29s 3ms/step - loss: 3.2393e-05\n",
            "Epoch 72/100\n",
            "8553/8553 [==============================] - 32s 4ms/step - loss: 3.4493e-05\n",
            "Epoch 73/100\n",
            "8553/8553 [==============================] - 31s 4ms/step - loss: 3.4305e-05\n",
            "Epoch 74/100\n",
            "8553/8553 [==============================] - 40s 5ms/step - loss: 3.3543e-05\n",
            "Epoch 75/100\n",
            "8553/8553 [==============================] - 29s 3ms/step - loss: 3.6528e-05\n",
            "Epoch 76/100\n",
            "8553/8553 [==============================] - 32s 4ms/step - loss: 3.2386e-05\n",
            "Epoch 77/100\n",
            "8553/8553 [==============================] - 29s 3ms/step - loss: 3.1963e-05\n",
            "Epoch 78/100\n",
            "8553/8553 [==============================] - 33s 4ms/step - loss: 3.4249e-05\n",
            "Epoch 79/100\n",
            "8553/8553 [==============================] - 38s 4ms/step - loss: 3.2785e-05\n",
            "Epoch 80/100\n",
            "8553/8553 [==============================] - 30s 4ms/step - loss: 3.0560e-05\n",
            "Epoch 81/100\n",
            "8553/8553 [==============================] - 32s 4ms/step - loss: 3.1514e-05\n",
            "Epoch 82/100\n",
            "8553/8553 [==============================] - 28s 3ms/step - loss: 3.2458e-05\n",
            "Epoch 83/100\n",
            "8553/8553 [==============================] - 36s 4ms/step - loss: 3.0814e-05\n",
            "Epoch 84/100\n",
            "8553/8553 [==============================] - 35s 4ms/step - loss: 2.9700e-05\n",
            "Epoch 85/100\n",
            "8553/8553 [==============================] - 32s 4ms/step - loss: 2.9107e-05\n",
            "Epoch 86/100\n",
            "8553/8553 [==============================] - 31s 4ms/step - loss: 3.0640e-05\n",
            "Epoch 87/100\n",
            "8553/8553 [==============================] - 33s 4ms/step - loss: 3.1312e-05\n",
            "Epoch 88/100\n",
            "8553/8553 [==============================] - 37s 4ms/step - loss: 2.8363e-05\n",
            "Epoch 89/100\n",
            "8553/8553 [==============================] - 35s 4ms/step - loss: 3.0506e-05\n",
            "Epoch 90/100\n",
            "8553/8553 [==============================] - 33s 4ms/step - loss: 3.1062e-05\n",
            "Epoch 91/100\n",
            "8553/8553 [==============================] - 30s 4ms/step - loss: 3.0462e-05\n",
            "Epoch 92/100\n",
            "8553/8553 [==============================] - 32s 4ms/step - loss: 2.8726e-05\n",
            "Epoch 93/100\n",
            "8553/8553 [==============================] - 38s 4ms/step - loss: 2.8736e-05\n",
            "Epoch 94/100\n",
            "8553/8553 [==============================] - 32s 4ms/step - loss: 2.7515e-05\n",
            "Epoch 95/100\n",
            "8553/8553 [==============================] - 30s 3ms/step - loss: 2.9265e-05\n",
            "Epoch 96/100\n",
            "8553/8553 [==============================] - 31s 4ms/step - loss: 2.8699e-05\n",
            "Epoch 97/100\n",
            "8553/8553 [==============================] - 32s 4ms/step - loss: 2.6692e-05\n",
            "Epoch 98/100\n",
            "8553/8553 [==============================] - 39s 5ms/step - loss: 2.6623e-05\n",
            "Epoch 99/100\n",
            "8553/8553 [==============================] - 33s 4ms/step - loss: 2.9306e-05\n",
            "Epoch 100/100\n",
            "8553/8553 [==============================] - 29s 3ms/step - loss: 2.6936e-05\n",
            "4277/4277 [==============================] - 8s 2ms/step\n",
            "4277/4277 [==============================] - 11s 3ms/step\n",
            "LSTM Validation on Randomised Height RMSE: 0.003052648910021576\n",
            "LSTM Test on Randomised Height RMSE: 0.0030425407220521002\n"
          ]
        }
      ],
      "source": [
        "##Test height feature\n",
        "x_train_height_reshaped = x_train_height.reshape((x_train_height.shape[0], 1, x_train_height.shape[1]))\n",
        "x_val_reshaped = x_val.reshape((x_val.shape[0], 1, x_val.shape[1]))\n",
        "x_test_reshaped = x_test.reshape((x_test.shape[0], 1, x_test.shape[1]))\n",
        "model = Sequential()\n",
        "model.add(LSTM(50, activation='tanh', input_shape=(1, x_train_height_reshaped.shape[2])))\n",
        "model.add(Dense(1))\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "model.fit(x_train_height_reshaped, y_train, epochs=100, batch_size=32, verbose=1)\n",
        "# Predict with the model\n",
        "y_val_height_pred = model.predict(x_val_reshaped)\n",
        "y_test_height_pred = model.predict(x_test_reshaped)\n",
        "\n",
        "lstm_val_rmse_height = np.sqrt(mean_squared_error(y_val, y_val_height_pred))\n",
        "lstm_test_rmse_height = np.sqrt(mean_squared_error(y_test, y_test_height_pred))\n",
        "\n",
        "print(f\"LSTM Validation on Randomised Height RMSE: {lstm_val_rmse_height}\")\n",
        "print(f\"LSTM Test on Randomised Height RMSE: {lstm_test_rmse_height}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uzIiPqaGWNSz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d1f9236-f352-4acc-9996-d8b681cbf9ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "8553/8553 [==============================] - 35s 4ms/step - loss: 59.0694\n",
            "Epoch 2/100\n",
            "8553/8553 [==============================] - 36s 4ms/step - loss: 0.0111\n",
            "Epoch 3/100\n",
            "8553/8553 [==============================] - 29s 3ms/step - loss: 0.0023\n",
            "Epoch 4/100\n",
            "8553/8553 [==============================] - 31s 4ms/step - loss: 0.0011\n",
            "Epoch 5/100\n",
            "8553/8553 [==============================] - 27s 3ms/step - loss: 6.6009e-04\n",
            "Epoch 6/100\n",
            "8553/8553 [==============================] - 33s 4ms/step - loss: 4.4689e-04\n",
            "Epoch 7/100\n",
            "8553/8553 [==============================] - 37s 4ms/step - loss: 3.3093e-04\n",
            "Epoch 8/100\n",
            "8553/8553 [==============================] - 27s 3ms/step - loss: 2.6388e-04\n",
            "Epoch 9/100\n",
            "8553/8553 [==============================] - 31s 4ms/step - loss: 2.1847e-04\n",
            "Epoch 10/100\n",
            "8553/8553 [==============================] - 28s 3ms/step - loss: 1.9654e-04\n",
            "Epoch 11/100\n",
            "8553/8553 [==============================] - 28s 3ms/step - loss: 1.6537e-04\n",
            "Epoch 12/100\n",
            "8553/8553 [==============================] - 38s 4ms/step - loss: 1.5405e-04\n",
            "Epoch 13/100\n",
            "8553/8553 [==============================] - 30s 4ms/step - loss: 1.3855e-04\n",
            "Epoch 14/100\n",
            "8553/8553 [==============================] - 27s 3ms/step - loss: 1.2799e-04\n",
            "Epoch 15/100\n",
            "8553/8553 [==============================] - 30s 3ms/step - loss: 1.2335e-04\n",
            "Epoch 16/100\n",
            "8553/8553 [==============================] - 29s 3ms/step - loss: 1.1396e-04\n",
            "Epoch 17/100\n",
            "8553/8553 [==============================] - 33s 4ms/step - loss: 1.0818e-04\n",
            "Epoch 18/100\n",
            "8553/8553 [==============================] - 33s 4ms/step - loss: 1.0394e-04\n",
            "Epoch 19/100\n",
            "8553/8553 [==============================] - 31s 4ms/step - loss: 9.7577e-05\n",
            "Epoch 20/100\n",
            "8553/8553 [==============================] - 28s 3ms/step - loss: 9.4987e-05\n",
            "Epoch 21/100\n",
            "8553/8553 [==============================] - 32s 4ms/step - loss: 9.9547e-05\n",
            "Epoch 22/100\n",
            "8553/8553 [==============================] - 35s 4ms/step - loss: 8.7706e-05\n",
            "Epoch 23/100\n",
            "8553/8553 [==============================] - 34s 4ms/step - loss: 8.9858e-05\n",
            "Epoch 24/100\n",
            "8553/8553 [==============================] - 32s 4ms/step - loss: 8.3089e-05\n",
            "Epoch 25/100\n",
            "8553/8553 [==============================] - 29s 3ms/step - loss: 8.0829e-05\n",
            "Epoch 26/100\n",
            "8553/8553 [==============================] - 31s 4ms/step - loss: 7.7627e-05\n",
            "Epoch 27/100\n",
            "8553/8553 [==============================] - 35s 4ms/step - loss: 8.0499e-05\n",
            "Epoch 28/100\n",
            "8553/8553 [==============================] - 33s 4ms/step - loss: 8.0442e-05\n",
            "Epoch 29/100\n",
            "8553/8553 [==============================] - 33s 4ms/step - loss: 7.3456e-05\n",
            "Epoch 30/100\n",
            "8553/8553 [==============================] - 29s 3ms/step - loss: 7.1769e-05\n",
            "Epoch 31/100\n",
            "8553/8553 [==============================] - 32s 4ms/step - loss: 7.7842e-05\n",
            "Epoch 32/100\n",
            "8553/8553 [==============================] - 34s 4ms/step - loss: 6.9212e-05\n",
            "Epoch 33/100\n",
            "8553/8553 [==============================] - 33s 4ms/step - loss: 6.7287e-05\n",
            "Epoch 34/100\n",
            "8553/8553 [==============================] - 30s 3ms/step - loss: 6.8953e-05\n",
            "Epoch 35/100\n",
            "8553/8553 [==============================] - 30s 4ms/step - loss: 6.4724e-05\n",
            "Epoch 36/100\n",
            "8553/8553 [==============================] - 29s 3ms/step - loss: 6.7758e-05\n",
            "Epoch 37/100\n",
            "8553/8553 [==============================] - 32s 4ms/step - loss: 6.4968e-05\n",
            "Epoch 38/100\n",
            "8553/8553 [==============================] - 37s 4ms/step - loss: 5.8799e-05\n",
            "Epoch 39/100\n",
            "8553/8553 [==============================] - 29s 3ms/step - loss: 6.6935e-05\n",
            "Epoch 40/100\n",
            "8553/8553 [==============================] - 26s 3ms/step - loss: 5.8227e-05\n",
            "Epoch 41/100\n",
            "8553/8553 [==============================] - 27s 3ms/step - loss: 6.0272e-05\n",
            "Epoch 42/100\n",
            "8553/8553 [==============================] - 31s 4ms/step - loss: 6.7734e-05\n",
            "Epoch 43/100\n",
            "8553/8553 [==============================] - 32s 4ms/step - loss: 5.9745e-05\n",
            "Epoch 44/100\n",
            "8553/8553 [==============================] - 34s 4ms/step - loss: 5.5884e-05\n",
            "Epoch 45/100\n",
            "8553/8553 [==============================] - 34s 4ms/step - loss: 5.8019e-05\n",
            "Epoch 46/100\n",
            "8553/8553 [==============================] - 28s 3ms/step - loss: 5.4215e-05\n",
            "Epoch 47/100\n",
            "8553/8553 [==============================] - 29s 3ms/step - loss: 5.6662e-05\n",
            "Epoch 48/100\n",
            "8553/8553 [==============================] - 33s 4ms/step - loss: 5.3151e-05\n",
            "Epoch 49/100\n",
            "8553/8553 [==============================] - 34s 4ms/step - loss: 5.5528e-05\n",
            "Epoch 50/100\n",
            "8553/8553 [==============================] - 29s 3ms/step - loss: 5.0536e-05\n",
            "Epoch 51/100\n",
            "8553/8553 [==============================] - 31s 4ms/step - loss: 5.1233e-05\n",
            "Epoch 52/100\n",
            "8553/8553 [==============================] - 28s 3ms/step - loss: 5.0450e-05\n",
            "Epoch 53/100\n",
            "8553/8553 [==============================] - 31s 4ms/step - loss: 5.2545e-05\n",
            "Epoch 54/100\n",
            "8553/8553 [==============================] - 38s 4ms/step - loss: 4.6370e-05\n",
            "Epoch 55/100\n",
            "8553/8553 [==============================] - 29s 3ms/step - loss: 4.6597e-05\n",
            "Epoch 56/100\n",
            "8553/8553 [==============================] - 29s 3ms/step - loss: 5.1731e-05\n",
            "Epoch 57/100\n",
            "8553/8553 [==============================] - 31s 4ms/step - loss: 4.7726e-05\n",
            "Epoch 58/100\n",
            "8553/8553 [==============================] - 28s 3ms/step - loss: 4.8113e-05\n",
            "Epoch 59/100\n",
            "8553/8553 [==============================] - 37s 4ms/step - loss: 4.2710e-05\n",
            "Epoch 60/100\n",
            "8553/8553 [==============================] - 32s 4ms/step - loss: 4.1968e-05\n",
            "Epoch 61/100\n",
            "8553/8553 [==============================] - 28s 3ms/step - loss: 4.5608e-05\n",
            "Epoch 62/100\n",
            "8553/8553 [==============================] - 30s 4ms/step - loss: 4.6304e-05\n",
            "Epoch 63/100\n",
            "8553/8553 [==============================] - 26s 3ms/step - loss: 5.0207e-05\n",
            "Epoch 64/100\n",
            "8553/8553 [==============================] - 32s 4ms/step - loss: 4.0990e-05\n",
            "Epoch 65/100\n",
            "8553/8553 [==============================] - 38s 4ms/step - loss: 4.9935e-05\n",
            "Epoch 66/100\n",
            "8553/8553 [==============================] - 29s 3ms/step - loss: 3.8911e-05\n",
            "Epoch 67/100\n",
            "8553/8553 [==============================] - 31s 4ms/step - loss: 4.1566e-05\n",
            "Epoch 68/100\n",
            "8553/8553 [==============================] - 27s 3ms/step - loss: 4.1115e-05\n",
            "Epoch 69/100\n",
            "8553/8553 [==============================] - 30s 3ms/step - loss: 4.3538e-05\n",
            "Epoch 70/100\n",
            "8553/8553 [==============================] - 37s 4ms/step - loss: 3.8865e-05\n",
            "Epoch 71/100\n",
            "8553/8553 [==============================] - 30s 4ms/step - loss: 4.0034e-05\n",
            "Epoch 72/100\n",
            "8553/8553 [==============================] - 28s 3ms/step - loss: 4.0187e-05\n",
            "Epoch 73/100\n",
            "8553/8553 [==============================] - 32s 4ms/step - loss: 4.1480e-05\n",
            "Epoch 74/100\n",
            "8553/8553 [==============================] - 27s 3ms/step - loss: 3.7364e-05\n",
            "Epoch 75/100\n",
            "8553/8553 [==============================] - 37s 4ms/step - loss: 3.7971e-05\n",
            "Epoch 76/100\n",
            "8553/8553 [==============================] - 32s 4ms/step - loss: 3.8214e-05\n",
            "Epoch 77/100\n",
            "8553/8553 [==============================] - 32s 4ms/step - loss: 3.7118e-05\n",
            "Epoch 78/100\n",
            "8553/8553 [==============================] - 28s 3ms/step - loss: 3.8535e-05\n",
            "Epoch 79/100\n",
            "8553/8553 [==============================] - 29s 3ms/step - loss: 3.5006e-05\n",
            "Epoch 80/100\n",
            "8553/8553 [==============================] - 32s 4ms/step - loss: 3.7361e-05\n",
            "Epoch 81/100\n",
            "8553/8553 [==============================] - 35s 4ms/step - loss: 3.7253e-05\n",
            "Epoch 82/100\n",
            "8553/8553 [==============================] - 27s 3ms/step - loss: 3.2658e-05\n",
            "Epoch 83/100\n",
            "8553/8553 [==============================] - 33s 4ms/step - loss: 3.4367e-05\n",
            "Epoch 84/100\n",
            "8553/8553 [==============================] - 28s 3ms/step - loss: 3.5179e-05\n",
            "Epoch 85/100\n",
            "8553/8553 [==============================] - 30s 4ms/step - loss: 3.4072e-05\n",
            "Epoch 86/100\n",
            "8553/8553 [==============================] - 38s 4ms/step - loss: 3.4097e-05\n",
            "Epoch 87/100\n",
            "8553/8553 [==============================] - 30s 4ms/step - loss: 3.2872e-05\n",
            "Epoch 88/100\n",
            "8553/8553 [==============================] - 29s 3ms/step - loss: 3.1937e-05\n",
            "Epoch 89/100\n",
            "8553/8553 [==============================] - 31s 4ms/step - loss: 3.1574e-05\n",
            "Epoch 90/100\n",
            "8553/8553 [==============================] - 28s 3ms/step - loss: 3.3762e-05\n",
            "Epoch 91/100\n",
            "8553/8553 [==============================] - 39s 5ms/step - loss: 3.0758e-05\n",
            "Epoch 92/100\n",
            "8553/8553 [==============================] - 33s 4ms/step - loss: 3.8080e-05\n",
            "Epoch 93/100\n",
            "8553/8553 [==============================] - 30s 4ms/step - loss: 3.3427e-05\n",
            "Epoch 94/100\n",
            "8553/8553 [==============================] - 27s 3ms/step - loss: 3.0713e-05\n",
            "Epoch 95/100\n",
            "8553/8553 [==============================] - 31s 4ms/step - loss: 3.0629e-05\n",
            "Epoch 96/100\n",
            "8553/8553 [==============================] - 32s 4ms/step - loss: 3.0777e-05\n",
            "Epoch 97/100\n",
            "8553/8553 [==============================] - 34s 4ms/step - loss: 3.2956e-05\n",
            "Epoch 98/100\n",
            "8553/8553 [==============================] - 34s 4ms/step - loss: 3.1432e-05\n",
            "Epoch 99/100\n",
            "8553/8553 [==============================] - 28s 3ms/step - loss: 3.0706e-05\n",
            "Epoch 100/100\n",
            "8553/8553 [==============================] - 31s 4ms/step - loss: 3.5028e-05\n",
            "4277/4277 [==============================] - 9s 2ms/step\n",
            "4277/4277 [==============================] - 12s 3ms/step\n",
            "LSTM Validation on Randomised Ice Conc RMSE: 0.002733139330880865\n",
            "LSTM Test on Randomised Ice Conc RMSE: 0.0027644770218497217\n"
          ]
        }
      ],
      "source": [
        "##Test ice_conc feature\n",
        "x_train_ice_conc_reshaped = x_train_ice_conc.reshape((x_train_ice_conc.shape[0], 1, x_train_ice_conc.shape[1]))\n",
        "x_val_reshaped = x_val.reshape((x_val.shape[0], 1, x_val.shape[1]))\n",
        "x_test_reshaped = x_test.reshape((x_test.shape[0], 1, x_test.shape[1]))\n",
        "model = Sequential()\n",
        "model.add(LSTM(50, activation='tanh', input_shape=(1, x_train_ice_conc_reshaped.shape[2])))\n",
        "model.add(Dense(1))\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "model.fit(x_train_ice_conc_reshaped, y_train, epochs=100, batch_size=32, verbose=1)\n",
        "# Predict with the model\n",
        "y_val_ice_conc_pred = model.predict(x_val_reshaped)\n",
        "y_test_ice_conc_pred = model.predict(x_test_reshaped)\n",
        "\n",
        "lstm_val_rmse_ice_conc = np.sqrt(mean_squared_error(y_val, y_val_ice_conc_pred))\n",
        "lstm_test_rmse_ice_conc = np.sqrt(mean_squared_error(y_test, y_test_ice_conc_pred))\n",
        "\n",
        "print(f\"LSTM Validation on Randomised Ice Conc RMSE: {lstm_val_rmse_ice_conc}\")\n",
        "print(f\"LSTM Test on Randomised Ice Conc RMSE: {lstm_test_rmse_ice_conc}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gqJTy1GuXzvL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd7fae81-4370-492c-c753-a096c7419d8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "8553/8553 [==============================] - 28s 3ms/step - loss: 60.6654\n",
            "Epoch 2/100\n",
            "8553/8553 [==============================] - 24s 3ms/step - loss: 0.0279\n",
            "Epoch 3/100\n",
            "8553/8553 [==============================] - 24s 3ms/step - loss: 0.0032\n",
            "Epoch 4/100\n",
            "8553/8553 [==============================] - 25s 3ms/step - loss: 0.0014\n",
            "Epoch 5/100\n",
            "8553/8553 [==============================] - 25s 3ms/step - loss: 7.8335e-04\n",
            "Epoch 6/100\n",
            "8553/8553 [==============================] - 23s 3ms/step - loss: 5.2282e-04\n",
            "Epoch 7/100\n",
            "8553/8553 [==============================] - 29s 3ms/step - loss: 3.8213e-04\n",
            "Epoch 8/100\n",
            "8553/8553 [==============================] - 25s 3ms/step - loss: 3.2024e-04\n",
            "Epoch 9/100\n",
            "8553/8553 [==============================] - 25s 3ms/step - loss: 2.6541e-04\n",
            "Epoch 10/100\n",
            "8553/8553 [==============================] - 23s 3ms/step - loss: 2.3620e-04\n",
            "Epoch 11/100\n",
            "8553/8553 [==============================] - 25s 3ms/step - loss: 2.1593e-04\n",
            "Epoch 12/100\n",
            "8553/8553 [==============================] - 24s 3ms/step - loss: 2.0004e-04\n",
            "Epoch 13/100\n",
            "8553/8553 [==============================] - 25s 3ms/step - loss: 1.8249e-04\n",
            "Epoch 14/100\n",
            "8553/8553 [==============================] - 25s 3ms/step - loss: 1.6836e-04\n",
            "Epoch 15/100\n",
            "8553/8553 [==============================] - 28s 3ms/step - loss: 1.6384e-04\n",
            "Epoch 16/100\n",
            "8553/8553 [==============================] - 25s 3ms/step - loss: 1.5025e-04\n",
            "Epoch 17/100\n",
            "8553/8553 [==============================] - 25s 3ms/step - loss: 1.4101e-04\n",
            "Epoch 18/100\n",
            "8553/8553 [==============================] - 25s 3ms/step - loss: 1.4152e-04\n",
            "Epoch 19/100\n",
            "8553/8553 [==============================] - 28s 3ms/step - loss: 1.3422e-04\n",
            "Epoch 20/100\n",
            "8553/8553 [==============================] - 25s 3ms/step - loss: 1.3241e-04\n",
            "Epoch 21/100\n",
            "8553/8553 [==============================] - 29s 3ms/step - loss: 1.1959e-04\n",
            "Epoch 22/100\n",
            "8553/8553 [==============================] - 29s 3ms/step - loss: 1.1786e-04\n",
            "Epoch 23/100\n",
            "8553/8553 [==============================] - 36s 4ms/step - loss: 1.1698e-04\n",
            "Epoch 24/100\n",
            "8553/8553 [==============================] - 29s 3ms/step - loss: 1.0731e-04\n",
            "Epoch 25/100\n",
            "8553/8553 [==============================] - 29s 3ms/step - loss: 1.1007e-04\n",
            "Epoch 26/100\n",
            "8553/8553 [==============================] - 29s 3ms/step - loss: 1.0304e-04\n",
            "Epoch 27/100\n",
            "8553/8553 [==============================] - 28s 3ms/step - loss: 1.0212e-04\n",
            "Epoch 28/100\n",
            "8553/8553 [==============================] - 28s 3ms/step - loss: 9.8781e-05\n",
            "Epoch 29/100\n",
            "8553/8553 [==============================] - 29s 3ms/step - loss: 9.6916e-05\n",
            "Epoch 30/100\n",
            "8553/8553 [==============================] - 34s 4ms/step - loss: 9.3709e-05\n",
            "Epoch 31/100\n",
            "8553/8553 [==============================] - 27s 3ms/step - loss: 9.1981e-05\n",
            "Epoch 32/100\n",
            "8553/8553 [==============================] - 28s 3ms/step - loss: 9.0064e-05\n",
            "Epoch 33/100\n",
            "8553/8553 [==============================] - 27s 3ms/step - loss: 8.2252e-05\n",
            "Epoch 34/100\n",
            "8553/8553 [==============================] - 29s 3ms/step - loss: 8.6728e-05\n",
            "Epoch 35/100\n",
            "8553/8553 [==============================] - 26s 3ms/step - loss: 8.3357e-05\n",
            "Epoch 36/100\n",
            "8553/8553 [==============================] - 29s 3ms/step - loss: 8.2831e-05\n",
            "Epoch 37/100\n",
            "8553/8553 [==============================] - 37s 4ms/step - loss: 8.3464e-05\n",
            "Epoch 38/100\n",
            "8553/8553 [==============================] - 27s 3ms/step - loss: 7.6665e-05\n",
            "Epoch 39/100\n",
            "8553/8553 [==============================] - 30s 4ms/step - loss: 8.0788e-05\n",
            "Epoch 40/100\n",
            "8553/8553 [==============================] - 27s 3ms/step - loss: 7.4220e-05\n",
            "Epoch 41/100\n",
            "8553/8553 [==============================] - 28s 3ms/step - loss: 7.3629e-05\n",
            "Epoch 42/100\n",
            "8553/8553 [==============================] - 30s 3ms/step - loss: 7.2422e-05\n",
            "Epoch 43/100\n",
            "8553/8553 [==============================] - 36s 4ms/step - loss: 6.6089e-05\n",
            "Epoch 44/100\n",
            "8553/8553 [==============================] - 34s 4ms/step - loss: 7.1343e-05\n",
            "Epoch 45/100\n",
            "8553/8553 [==============================] - 32s 4ms/step - loss: 7.1395e-05\n",
            "Epoch 46/100\n",
            "8553/8553 [==============================] - 27s 3ms/step - loss: 6.9884e-05\n",
            "Epoch 47/100\n",
            "8553/8553 [==============================] - 28s 3ms/step - loss: 6.7595e-05\n",
            "Epoch 48/100\n",
            "8553/8553 [==============================] - 30s 4ms/step - loss: 6.5733e-05\n",
            "Epoch 49/100\n",
            "8553/8553 [==============================] - 27s 3ms/step - loss: 6.4759e-05\n",
            "Epoch 50/100\n",
            "8553/8553 [==============================] - 38s 4ms/step - loss: 6.1910e-05\n",
            "Epoch 51/100\n",
            "8553/8553 [==============================] - 30s 3ms/step - loss: 6.3931e-05\n",
            "Epoch 52/100\n",
            "8553/8553 [==============================] - 26s 3ms/step - loss: 6.3285e-05\n",
            "Epoch 53/100\n",
            "8553/8553 [==============================] - 31s 4ms/step - loss: 5.7929e-05\n",
            "Epoch 54/100\n",
            "8553/8553 [==============================] - 30s 4ms/step - loss: 5.9476e-05\n",
            "Epoch 55/100\n",
            "8553/8553 [==============================] - 30s 3ms/step - loss: 5.7907e-05\n",
            "Epoch 56/100\n",
            "8553/8553 [==============================] - 35s 4ms/step - loss: 5.6372e-05\n",
            "Epoch 57/100\n",
            "8553/8553 [==============================] - 35s 4ms/step - loss: 5.5942e-05\n",
            "Epoch 58/100\n",
            "8553/8553 [==============================] - 31s 4ms/step - loss: 5.8202e-05\n",
            "Epoch 59/100\n",
            "8553/8553 [==============================] - 29s 3ms/step - loss: 5.4402e-05\n",
            "Epoch 60/100\n",
            "8553/8553 [==============================] - 28s 3ms/step - loss: 5.6054e-05\n",
            "Epoch 61/100\n",
            "8553/8553 [==============================] - 32s 4ms/step - loss: 5.2932e-05\n",
            "Epoch 62/100\n",
            "8553/8553 [==============================] - 28s 3ms/step - loss: 5.3186e-05\n",
            "Epoch 63/100\n",
            "8553/8553 [==============================] - 43s 5ms/step - loss: 5.0425e-05\n",
            "Epoch 64/100\n",
            "8553/8553 [==============================] - 31s 4ms/step - loss: 5.1454e-05\n",
            "Epoch 65/100\n",
            "8553/8553 [==============================] - 28s 3ms/step - loss: 4.9884e-05\n",
            "Epoch 66/100\n",
            "8553/8553 [==============================] - 31s 4ms/step - loss: 4.8234e-05\n",
            "Epoch 67/100\n",
            "8553/8553 [==============================] - 28s 3ms/step - loss: 4.9346e-05\n",
            "Epoch 68/100\n",
            "8553/8553 [==============================] - 28s 3ms/step - loss: 4.6757e-05\n",
            "Epoch 69/100\n",
            "8553/8553 [==============================] - 42s 5ms/step - loss: 5.0461e-05\n",
            "Epoch 70/100\n",
            "8553/8553 [==============================] - 28s 3ms/step - loss: 4.8488e-05\n",
            "Epoch 71/100\n",
            "8553/8553 [==============================] - 33s 4ms/step - loss: 4.7903e-05\n",
            "Epoch 72/100\n",
            "8553/8553 [==============================] - 29s 3ms/step - loss: 4.5688e-05\n",
            "Epoch 73/100\n",
            "8553/8553 [==============================] - 30s 3ms/step - loss: 4.6860e-05\n",
            "Epoch 74/100\n",
            "8553/8553 [==============================] - 30s 3ms/step - loss: 4.4866e-05\n",
            "Epoch 75/100\n",
            "8553/8553 [==============================] - 34s 4ms/step - loss: 4.6135e-05\n",
            "Epoch 76/100\n",
            "8553/8553 [==============================] - 33s 4ms/step - loss: 4.2788e-05\n",
            "Epoch 77/100\n",
            "8553/8553 [==============================] - 31s 4ms/step - loss: 4.1326e-05\n",
            "Epoch 78/100\n",
            "8553/8553 [==============================] - 29s 3ms/step - loss: 4.0656e-05\n",
            "Epoch 79/100\n",
            "8553/8553 [==============================] - 31s 4ms/step - loss: 4.0955e-05\n",
            "Epoch 80/100\n",
            "8553/8553 [==============================] - 27s 3ms/step - loss: 4.1028e-05\n",
            "Epoch 81/100\n",
            "8553/8553 [==============================] - 32s 4ms/step - loss: 4.0734e-05\n",
            "Epoch 82/100\n",
            "8553/8553 [==============================] - 36s 4ms/step - loss: 4.0896e-05\n",
            "Epoch 83/100\n",
            "8553/8553 [==============================] - 28s 3ms/step - loss: 4.2680e-05\n",
            "Epoch 84/100\n",
            "8553/8553 [==============================] - 28s 3ms/step - loss: 3.8653e-05\n",
            "Epoch 85/100\n",
            "8553/8553 [==============================] - 31s 4ms/step - loss: 3.9603e-05\n",
            "Epoch 86/100\n",
            "8553/8553 [==============================] - 28s 3ms/step - loss: 3.9244e-05\n",
            "Epoch 87/100\n",
            "8553/8553 [==============================] - 30s 4ms/step - loss: 3.9432e-05\n",
            "Epoch 88/100\n",
            "8553/8553 [==============================] - 38s 4ms/step - loss: 3.8970e-05\n",
            "Epoch 89/100\n",
            "8553/8553 [==============================] - 27s 3ms/step - loss: 3.5548e-05\n",
            "Epoch 90/100\n",
            "8553/8553 [==============================] - 30s 4ms/step - loss: 3.9308e-05\n",
            "Epoch 91/100\n",
            "8553/8553 [==============================] - 29s 3ms/step - loss: 3.4490e-05\n",
            "Epoch 92/100\n",
            "8553/8553 [==============================] - 27s 3ms/step - loss: 3.3499e-05\n",
            "Epoch 93/100\n",
            "8553/8553 [==============================] - 32s 4ms/step - loss: 3.4460e-05\n",
            "Epoch 94/100\n",
            "8553/8553 [==============================] - 35s 4ms/step - loss: 3.5647e-05\n",
            "Epoch 95/100\n",
            "8553/8553 [==============================] - 31s 4ms/step - loss: 3.5641e-05\n",
            "Epoch 96/100\n",
            "8553/8553 [==============================] - 33s 4ms/step - loss: 3.2162e-05\n",
            "Epoch 97/100\n",
            "8553/8553 [==============================] - 28s 3ms/step - loss: 3.3856e-05\n",
            "Epoch 98/100\n",
            "8553/8553 [==============================] - 34s 4ms/step - loss: 3.5435e-05\n",
            "Epoch 99/100\n",
            "8553/8553 [==============================] - 30s 4ms/step - loss: 3.1403e-05\n",
            "Epoch 100/100\n",
            "8553/8553 [==============================] - 42s 5ms/step - loss: 3.2299e-05\n",
            "4277/4277 [==============================] - 7s 2ms/step\n",
            "4277/4277 [==============================] - 10s 2ms/step\n",
            "LSTM Validation on Randomised Windspeed RMSE: 0.004874041447153838\n",
            "LSTM Test on Randomised Windspeed RMSE: 0.004895030846810138\n"
          ]
        }
      ],
      "source": [
        "##Test windspeed feature\n",
        "x_train_windspeed_reshaped = x_train_windspeed.reshape((x_train_windspeed.shape[0], 1, x_train_windspeed.shape[1]))\n",
        "x_val_reshaped = x_val.reshape((x_val.shape[0], 1, x_val.shape[1]))\n",
        "x_test_reshaped = x_test.reshape((x_test.shape[0], 1, x_test.shape[1]))\n",
        "model = Sequential()\n",
        "model.add(LSTM(50, activation='tanh', input_shape=(1, x_train_windspeed_reshaped.shape[2])))\n",
        "model.add(Dense(1))\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "model.fit(x_train_windspeed_reshaped, y_train, epochs=100, batch_size=32, verbose=1)\n",
        "# Predict with the model\n",
        "y_val_windspeed_pred = model.predict(x_val_reshaped)\n",
        "y_test_windspeed_pred = model.predict(x_test_reshaped)\n",
        "\n",
        "lstm_val_rmse_windspeed = np.sqrt(mean_squared_error(y_val, y_val_windspeed_pred))\n",
        "lstm_test_rmse_windspeed = np.sqrt(mean_squared_error(y_test, y_test_windspeed_pred))\n",
        "\n",
        "print(f\"LSTM Validation on Randomised Windspeed RMSE: {lstm_val_rmse_windspeed}\")\n",
        "print(f\"LSTM Test on Randomised Windspeed RMSE: {lstm_test_rmse_windspeed}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kKpZx9lSYx8B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c20b09c0-1656-428e-bf17-e7d4ff0d24d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "8553/8553 [==============================] - 32s 3ms/step - loss: 58.6092\n",
            "Epoch 2/100\n",
            "8553/8553 [==============================] - 26s 3ms/step - loss: 0.0107\n",
            "Epoch 3/100\n",
            "8553/8553 [==============================] - 35s 4ms/step - loss: 0.0016\n",
            "Epoch 4/100\n",
            "8553/8553 [==============================] - 35s 4ms/step - loss: 7.3738e-04\n",
            "Epoch 5/100\n",
            "8553/8553 [==============================] - 28s 3ms/step - loss: 4.6130e-04\n",
            "Epoch 6/100\n",
            "8553/8553 [==============================] - 33s 4ms/step - loss: 3.4952e-04\n",
            "Epoch 7/100\n",
            "8553/8553 [==============================] - 28s 3ms/step - loss: 2.8322e-04\n",
            "Epoch 8/100\n",
            "8553/8553 [==============================] - 33s 4ms/step - loss: 2.6598e-04\n",
            "Epoch 9/100\n",
            "8553/8553 [==============================] - 38s 4ms/step - loss: 2.1957e-04\n",
            "Epoch 10/100\n",
            "8553/8553 [==============================] - 32s 4ms/step - loss: 1.8989e-04\n",
            "Epoch 11/100\n",
            "8553/8553 [==============================] - 30s 3ms/step - loss: 1.7687e-04\n",
            "Epoch 12/100\n",
            "8553/8553 [==============================] - 27s 3ms/step - loss: 1.7925e-04\n",
            "Epoch 13/100\n",
            "8553/8553 [==============================] - 31s 4ms/step - loss: 1.7155e-04\n",
            "Epoch 14/100\n",
            "8553/8553 [==============================] - 28s 3ms/step - loss: 1.6796e-04\n",
            "Epoch 15/100\n",
            "8553/8553 [==============================] - 39s 5ms/step - loss: 1.5304e-04\n",
            "Epoch 16/100\n",
            "8553/8553 [==============================] - 31s 4ms/step - loss: 1.3638e-04\n",
            "Epoch 17/100\n",
            "8553/8553 [==============================] - 28s 3ms/step - loss: 1.2391e-04\n",
            "Epoch 18/100\n",
            "8553/8553 [==============================] - 32s 4ms/step - loss: 1.3982e-04\n",
            "Epoch 19/100\n",
            "8553/8553 [==============================] - 29s 3ms/step - loss: 1.2787e-04\n",
            "Epoch 20/100\n",
            "8553/8553 [==============================] - 34s 4ms/step - loss: 1.2394e-04\n",
            "Epoch 21/100\n",
            "8553/8553 [==============================] - 38s 4ms/step - loss: 1.0750e-04\n",
            "Epoch 22/100\n",
            "8553/8553 [==============================] - 30s 4ms/step - loss: 1.1047e-04\n",
            "Epoch 23/100\n",
            "8553/8553 [==============================] - 32s 4ms/step - loss: 1.0991e-04\n",
            "Epoch 24/100\n",
            "8553/8553 [==============================] - 31s 4ms/step - loss: 1.0811e-04\n",
            "Epoch 25/100\n",
            "8553/8553 [==============================] - 32s 4ms/step - loss: 9.6751e-05\n",
            "Epoch 26/100\n",
            "8553/8553 [==============================] - 29s 3ms/step - loss: 1.0263e-04\n",
            "Epoch 27/100\n",
            "8553/8553 [==============================] - 42s 5ms/step - loss: 9.7954e-05\n",
            "Epoch 28/100\n",
            "8553/8553 [==============================] - 28s 3ms/step - loss: 9.8613e-05\n",
            "Epoch 29/100\n",
            "8553/8553 [==============================] - 32s 4ms/step - loss: 8.9690e-05\n",
            "Epoch 30/100\n",
            "8553/8553 [==============================] - 29s 3ms/step - loss: 9.0026e-05\n",
            "Epoch 31/100\n",
            "8553/8553 [==============================] - 33s 4ms/step - loss: 8.0987e-05\n",
            "Epoch 32/100\n",
            "8553/8553 [==============================] - 33s 4ms/step - loss: 8.6264e-05\n",
            "Epoch 33/100\n",
            "8553/8553 [==============================] - 33s 4ms/step - loss: 7.5722e-05\n",
            "Epoch 34/100\n",
            "8553/8553 [==============================] - 31s 4ms/step - loss: 8.1888e-05\n",
            "Epoch 35/100\n",
            "8553/8553 [==============================] - 26s 3ms/step - loss: 7.9521e-05\n",
            "Epoch 36/100\n",
            "8553/8553 [==============================] - 27s 3ms/step - loss: 7.6094e-05\n",
            "Epoch 37/100\n",
            "8553/8553 [==============================] - 29s 3ms/step - loss: 8.2164e-05\n",
            "Epoch 38/100\n",
            "8553/8553 [==============================] - 26s 3ms/step - loss: 6.9650e-05\n",
            "Epoch 39/100\n",
            "8553/8553 [==============================] - 35s 4ms/step - loss: 7.0531e-05\n",
            "Epoch 40/100\n",
            "8553/8553 [==============================] - 27s 3ms/step - loss: 7.2033e-05\n",
            "Epoch 41/100\n",
            "8553/8553 [==============================] - 32s 4ms/step - loss: 8.2552e-05\n",
            "Epoch 42/100\n",
            "8553/8553 [==============================] - 28s 3ms/step - loss: 7.3340e-05\n",
            "Epoch 43/100\n",
            "8553/8553 [==============================] - 29s 3ms/step - loss: 5.9765e-05\n",
            "Epoch 44/100\n",
            "8553/8553 [==============================] - 29s 3ms/step - loss: 7.1716e-05\n",
            "Epoch 45/100\n",
            "8553/8553 [==============================] - 36s 4ms/step - loss: 6.9852e-05\n",
            "Epoch 46/100\n",
            "8553/8553 [==============================] - 28s 3ms/step - loss: 7.8869e-05\n",
            "Epoch 47/100\n",
            "8553/8553 [==============================] - 33s 4ms/step - loss: 7.7055e-05\n",
            "Epoch 48/100\n",
            "8553/8553 [==============================] - 27s 3ms/step - loss: 5.7812e-05\n",
            "Epoch 49/100\n",
            "8553/8553 [==============================] - 29s 3ms/step - loss: 6.3947e-05\n",
            "Epoch 50/100\n",
            "8553/8553 [==============================] - 32s 4ms/step - loss: 6.0260e-05\n",
            "Epoch 51/100\n",
            "8553/8553 [==============================] - 38s 4ms/step - loss: 5.4494e-05\n",
            "Epoch 52/100\n",
            "8553/8553 [==============================] - 27s 3ms/step - loss: 6.1936e-05\n",
            "Epoch 53/100\n",
            "8553/8553 [==============================] - 31s 4ms/step - loss: 5.4746e-05\n",
            "Epoch 54/100\n",
            "8553/8553 [==============================] - 30s 4ms/step - loss: 5.2870e-05\n",
            "Epoch 55/100\n",
            "8553/8553 [==============================] - 29s 3ms/step - loss: 5.8975e-05\n",
            "Epoch 56/100\n",
            "8553/8553 [==============================] - 31s 4ms/step - loss: 5.4506e-05\n",
            "Epoch 57/100\n",
            "8553/8553 [==============================] - 38s 4ms/step - loss: 5.9640e-05\n",
            "Epoch 58/100\n",
            "8553/8553 [==============================] - 28s 3ms/step - loss: 5.6808e-05\n",
            "Epoch 59/100\n",
            "8553/8553 [==============================] - 28s 3ms/step - loss: 6.1103e-05\n",
            "Epoch 60/100\n",
            "8553/8553 [==============================] - 31s 4ms/step - loss: 4.3821e-05\n",
            "Epoch 61/100\n",
            "8553/8553 [==============================] - 28s 3ms/step - loss: 5.0375e-05\n",
            "Epoch 62/100\n",
            "8553/8553 [==============================] - 33s 4ms/step - loss: 5.0957e-05\n",
            "Epoch 63/100\n",
            "8553/8553 [==============================] - 38s 4ms/step - loss: 6.5502e-05\n",
            "Epoch 64/100\n",
            "8553/8553 [==============================] - 28s 3ms/step - loss: 5.0217e-05\n",
            "Epoch 65/100\n",
            "8553/8553 [==============================] - 28s 3ms/step - loss: 4.9238e-05\n",
            "Epoch 66/100\n",
            "8553/8553 [==============================] - 29s 3ms/step - loss: 4.8700e-05\n",
            "Epoch 67/100\n",
            "8553/8553 [==============================] - 28s 3ms/step - loss: 5.0405e-05\n",
            "Epoch 68/100\n",
            "8553/8553 [==============================] - 35s 4ms/step - loss: 5.0544e-05\n",
            "Epoch 69/100\n",
            "8553/8553 [==============================] - 31s 4ms/step - loss: 4.7501e-05\n",
            "Epoch 70/100\n",
            "8553/8553 [==============================] - 29s 3ms/step - loss: 4.8289e-05\n",
            "Epoch 71/100\n",
            "8553/8553 [==============================] - 27s 3ms/step - loss: 4.2659e-05\n",
            "Epoch 72/100\n",
            "8553/8553 [==============================] - 29s 3ms/step - loss: 4.6486e-05\n",
            "Epoch 73/100\n",
            "8553/8553 [==============================] - 29s 3ms/step - loss: 4.6388e-05\n",
            "Epoch 74/100\n",
            "8553/8553 [==============================] - 34s 4ms/step - loss: 4.2323e-05\n",
            "Epoch 75/100\n",
            "8553/8553 [==============================] - 33s 4ms/step - loss: 3.8205e-05\n",
            "Epoch 76/100\n",
            "8553/8553 [==============================] - 30s 3ms/step - loss: 4.3789e-05\n",
            "Epoch 77/100\n",
            "8553/8553 [==============================] - 27s 3ms/step - loss: 4.1506e-05\n",
            "Epoch 78/100\n",
            "8553/8553 [==============================] - 27s 3ms/step - loss: 4.9440e-05\n",
            "Epoch 79/100\n",
            "8553/8553 [==============================] - 30s 3ms/step - loss: 4.0478e-05\n",
            "Epoch 80/100\n",
            "8553/8553 [==============================] - 36s 4ms/step - loss: 3.9300e-05\n",
            "Epoch 81/100\n",
            "8553/8553 [==============================] - 27s 3ms/step - loss: 3.8548e-05\n",
            "Epoch 82/100\n",
            "8553/8553 [==============================] - 29s 3ms/step - loss: 3.7690e-05\n",
            "Epoch 83/100\n",
            "8553/8553 [==============================] - 28s 3ms/step - loss: 4.4606e-05\n",
            "Epoch 84/100\n",
            "8553/8553 [==============================] - 26s 3ms/step - loss: 4.0257e-05\n",
            "Epoch 85/100\n",
            "8553/8553 [==============================] - 26s 3ms/step - loss: 3.8924e-05\n",
            "Epoch 86/100\n",
            "8553/8553 [==============================] - 38s 4ms/step - loss: 4.0731e-05\n",
            "Epoch 87/100\n",
            "8553/8553 [==============================] - 29s 3ms/step - loss: 3.9922e-05\n",
            "Epoch 88/100\n",
            "8553/8553 [==============================] - 26s 3ms/step - loss: 4.2700e-05\n",
            "Epoch 89/100\n",
            "8553/8553 [==============================] - 27s 3ms/step - loss: 3.4655e-05\n",
            "Epoch 90/100\n",
            "8553/8553 [==============================] - 29s 3ms/step - loss: 4.3509e-05\n",
            "Epoch 91/100\n",
            "8553/8553 [==============================] - 27s 3ms/step - loss: 3.6375e-05\n",
            "Epoch 92/100\n",
            "8553/8553 [==============================] - 35s 4ms/step - loss: 4.2612e-05\n",
            "Epoch 93/100\n",
            "8553/8553 [==============================] - 28s 3ms/step - loss: 3.2633e-05\n",
            "Epoch 94/100\n",
            "8553/8553 [==============================] - 29s 3ms/step - loss: 3.5448e-05\n",
            "Epoch 95/100\n",
            "8553/8553 [==============================] - 27s 3ms/step - loss: 3.4219e-05\n",
            "Epoch 96/100\n",
            "8553/8553 [==============================] - 28s 3ms/step - loss: 3.9417e-05\n",
            "Epoch 97/100\n",
            "8553/8553 [==============================] - 30s 3ms/step - loss: 3.9165e-05\n",
            "Epoch 98/100\n",
            "8553/8553 [==============================] - 35s 4ms/step - loss: 3.7243e-05\n",
            "Epoch 99/100\n",
            "8553/8553 [==============================] - 27s 3ms/step - loss: 3.2862e-05\n",
            "Epoch 100/100\n",
            "8553/8553 [==============================] - 28s 3ms/step - loss: 3.7005e-05\n",
            "4277/4277 [==============================] - 10s 2ms/step\n",
            "4277/4277 [==============================] - 7s 2ms/step\n",
            "LSTM Validation on Randomised Precip RMSE: 0.0033112246249207104\n",
            "LSTM Test on Randomised Precip RMSE: 0.0033065700595167795\n"
          ]
        }
      ],
      "source": [
        "##Test precip feature\n",
        "x_train_precip_reshaped = x_train_precip.reshape((x_train_precip.shape[0], 1, x_train_precip.shape[1]))\n",
        "x_val_reshaped = x_val.reshape((x_val.shape[0], 1, x_val.shape[1]))\n",
        "x_test_reshaped = x_test.reshape((x_test.shape[0], 1, x_test.shape[1]))\n",
        "model = Sequential()\n",
        "model.add(LSTM(50, activation='tanh', input_shape=(1, x_train_precip_reshaped.shape[2])))\n",
        "model.add(Dense(1))\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "model.fit(x_train_precip_reshaped, y_train, epochs=100, batch_size=32, verbose=1)\n",
        "# Predict with the model\n",
        "y_val_precip_pred = model.predict(x_val_reshaped)\n",
        "y_test_precip_pred = model.predict(x_test_reshaped)\n",
        "\n",
        "lstm_val_rmse_precip = np.sqrt(mean_squared_error(y_val, y_val_precip_pred))\n",
        "lstm_test_rmse_precip = np.sqrt(mean_squared_error(y_test, y_test_precip_pred))\n",
        "\n",
        "print(f\"LSTM Validation on Randomised Precip RMSE: {lstm_val_rmse_precip}\")\n",
        "print(f\"LSTM Test on Randomised Precip RMSE: {lstm_test_rmse_precip}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_test_rmse = 0.0023513991326912353\n",
        "lstm_test_rmse_temp = 0.0126046431332642\n",
        "lstm_test_rmse_pressure = 0.005053670048732205\n",
        "lstm_test_rmse_lat = 0.0034471933991493315\n",
        "lstm_test_rmse_lon = 0.007441779995265137\n",
        "lstm_test_rmse_height = 0.0030425407220521002\n",
        "lstm_test_rmse_ice_conc = 0.0027644770218497217\n",
        "lstm_test_rmse_windspeed = 0.004895030846810138\n",
        "lstm_test_rmse_precip = 0.0033065700595167795"
      ],
      "metadata": {
        "id": "RQx6l0uPEgf3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JI5DMYj4ZdYr"
      },
      "outputs": [],
      "source": [
        "##RMSE ratings\n",
        "lstm_temp_rmse = lstm_test_rmse_temp / lstm_test_rmse\n",
        "lstm_pressure_rmse = lstm_test_rmse_pressure / lstm_test_rmse\n",
        "lstm_lat_rmse = lstm_test_rmse_lat / lstm_test_rmse\n",
        "lstm_lon_rmse = lstm_test_rmse_lon / lstm_test_rmse\n",
        "lstm_height_rmse = lstm_test_rmse_height / lstm_test_rmse\n",
        "lstm_ice_conc_rmse = lstm_test_rmse_ice_conc / lstm_test_rmse\n",
        "lstm_windspeed_rmse = lstm_test_rmse_windspeed / lstm_test_rmse\n",
        "lstm_precip_rmse = lstm_test_rmse_precip / lstm_test_rmse"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(lstm_temp_rmse)\n",
        "print(lstm_pressure_rmse)\n",
        "print(lstm_lat_rmse)\n",
        "print(lstm_lon_rmse)\n",
        "print(lstm_height_rmse)\n",
        "print(lstm_ice_conc_rmse)\n",
        "print(lstm_windspeed_rmse)\n",
        "print(lstm_precip_rmse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eLst-xLcFkof",
        "outputId": "ab1811e9-3aef-4a97-92f7-2de73c1d75ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5.360486426155083\n",
            "2.14921830091353\n",
            "1.46601797679662\n",
            "3.1648306286257037\n",
            "1.2939278065352673\n",
            "1.1756732336146218\n",
            "2.081752425079639\n",
            "1.4062138637145483\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "keBx3Fp8MI3j"
      },
      "source": [
        "# **Model 5 - MLP**\n",
        "https://www.kaggle.com/code/vitorgamalemos/multilayer-perceptron-from-scratch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vtQzWr8kMK8n"
      },
      "outputs": [],
      "source": [
        "model = Sequential()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8XtgZf1AMgn2"
      },
      "outputs": [],
      "source": [
        "model.add(Dense(128, activation='relu', input_shape=(x_train.shape[1],)))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(1, activation='linear'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1cC21VPkMkjC",
        "outputId": "7f42adef-2a36-4450-88fd-9cab28c13100"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 128)               1152      \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 128)               16512     \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 128)               16512     \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 128)               16512     \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 128)               16512     \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 128)               16512     \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 128)               16512     \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 128)               16512     \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 128)               16512     \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 128)               16512     \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 128)               16512     \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 128)               16512     \n",
            "                                                                 \n",
            " dense_12 (Dense)            (None, 128)               16512     \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 128)               16512     \n",
            "                                                                 \n",
            " dense_14 (Dense)            (None, 1)                 129       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 215937 (843.50 KB)\n",
            "Trainable params: 215937 (843.50 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UYbrVf_fMn84"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam', loss='mean_squared_error')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NNkxYUrsMxAN",
        "outputId": "5070d747-90ca-4bb7-cdb1-b3bd60a6d492"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/21\n",
            "4277/4277 [==============================] - 40s 9ms/step - loss: 3.9090 - val_loss: 0.0666\n",
            "Epoch 2/21\n",
            "4277/4277 [==============================] - 40s 9ms/step - loss: 0.0846 - val_loss: 0.0255\n",
            "Epoch 3/21\n",
            "4277/4277 [==============================] - 42s 10ms/step - loss: 0.2327 - val_loss: 0.0026\n",
            "Epoch 4/21\n",
            "4277/4277 [==============================] - 49s 11ms/step - loss: 0.0236 - val_loss: 0.0019\n",
            "Epoch 5/21\n",
            "4277/4277 [==============================] - 34s 8ms/step - loss: 0.0143 - val_loss: 0.0021\n",
            "Epoch 6/21\n",
            "4277/4277 [==============================] - 36s 8ms/step - loss: 0.3423 - val_loss: 0.0021\n",
            "Epoch 7/21\n",
            "4277/4277 [==============================] - 36s 8ms/step - loss: 0.0244 - val_loss: 0.0034\n",
            "Epoch 8/21\n",
            "4277/4277 [==============================] - 40s 9ms/step - loss: 0.0429 - val_loss: 0.1031\n",
            "Epoch 9/21\n",
            "4277/4277 [==============================] - 40s 9ms/step - loss: 0.0118 - val_loss: 0.0066\n",
            "Epoch 10/21\n",
            "4277/4277 [==============================] - 37s 9ms/step - loss: 0.0089 - val_loss: 7.5880e-05\n",
            "Epoch 11/21\n",
            "4277/4277 [==============================] - 34s 8ms/step - loss: 0.0046 - val_loss: 7.3370e-05\n",
            "Epoch 12/21\n",
            "4277/4277 [==============================] - 33s 8ms/step - loss: 0.0030 - val_loss: 2.0077e-05\n",
            "Epoch 13/21\n",
            "4277/4277 [==============================] - 46s 11ms/step - loss: 0.0314 - val_loss: 4.4613e-06\n",
            "Epoch 14/21\n",
            "4277/4277 [==============================] - 33s 8ms/step - loss: 0.0015 - val_loss: 5.5729e-06\n",
            "Epoch 15/21\n",
            "4277/4277 [==============================] - 35s 8ms/step - loss: 0.0072 - val_loss: 0.0022\n",
            "Epoch 16/21\n",
            "4277/4277 [==============================] - 38s 9ms/step - loss: 9.4801e-04 - val_loss: 2.5073e-04\n",
            "Epoch 17/21\n",
            "4277/4277 [==============================] - 40s 9ms/step - loss: 7.3364e-04 - val_loss: 0.0021\n",
            "Epoch 18/21\n",
            "4277/4277 [==============================] - 45s 11ms/step - loss: 0.0113 - val_loss: 1.2797e-04\n",
            "Epoch 19/21\n",
            "4277/4277 [==============================] - 37s 9ms/step - loss: 4.1651e-04 - val_loss: 1.5628e-05\n",
            "Epoch 20/21\n",
            "4277/4277 [==============================] - 38s 9ms/step - loss: 4.8263e-04 - val_loss: 1.2896e-06\n",
            "Epoch 21/21\n",
            "4277/4277 [==============================] - 41s 10ms/step - loss: 4.5042e-04 - val_loss: 2.8731e-04\n"
          ]
        }
      ],
      "source": [
        "training_results = model.fit(x_train,\n",
        "                             y_train,\n",
        "                             epochs=21,\n",
        "                             batch_size=64,\n",
        "                             validation_data=(x_val, y_val))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LDMbanULM7BN",
        "outputId": "86f0c456-469b-4793-eb16-6fdcc5499e52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4277/4277 [==============================] - 15s 3ms/step\n",
            "4277/4277 [==============================] - 14s 3ms/step\n",
            "MLP Validation RMSE: 0.016950407228873195\n",
            "MLP Test RMSE: 0.016949189685648076\n"
          ]
        }
      ],
      "source": [
        "##Run model\n",
        "y_val_pred = model.predict(x_val)\n",
        "y_test_pred = model.predict(x_test)\n",
        "\n",
        "mlp_val_rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))\n",
        "mlp_test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
        "\n",
        "print(f\"MLP Validation RMSE: {mlp_val_rmse}\")\n",
        "print(f\"MLP Test RMSE: {mlp_test_rmse}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Feature Testing**"
      ],
      "metadata": {
        "id": "Ftksg_LlEBXO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IhYvy0TMe8H_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40cd7c5e-3526-4487-8d71-06489816f0c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/21\n",
            "4277/4277 [==============================] - 36s 8ms/step - loss: 0.4994 - val_loss: 0.0014\n",
            "Epoch 2/21\n",
            "4277/4277 [==============================] - 38s 9ms/step - loss: 0.0315 - val_loss: 0.0051\n",
            "Epoch 3/21\n",
            "4277/4277 [==============================] - 35s 8ms/step - loss: 0.0459 - val_loss: 0.0041\n",
            "Epoch 4/21\n",
            "4277/4277 [==============================] - 42s 10ms/step - loss: 0.0045 - val_loss: 0.0015\n",
            "Epoch 5/21\n",
            "4277/4277 [==============================] - 35s 8ms/step - loss: 0.0077 - val_loss: 0.0012\n",
            "Epoch 6/21\n",
            "4277/4277 [==============================] - 37s 9ms/step - loss: 0.1228 - val_loss: 0.0019\n",
            "Epoch 7/21\n",
            "4277/4277 [==============================] - 31s 7ms/step - loss: 0.0078 - val_loss: 0.0033\n",
            "Epoch 8/21\n",
            "4277/4277 [==============================] - 46s 11ms/step - loss: 0.0129 - val_loss: 7.9536e-04\n",
            "Epoch 9/21\n",
            "4277/4277 [==============================] - 38s 9ms/step - loss: 0.0069 - val_loss: 3.2275e-04\n",
            "Epoch 10/21\n",
            "4277/4277 [==============================] - 32s 8ms/step - loss: 0.0322 - val_loss: 0.0055\n",
            "Epoch 11/21\n",
            "4277/4277 [==============================] - 36s 8ms/step - loss: 0.0043 - val_loss: 2.3729e-04\n",
            "Epoch 12/21\n",
            "4277/4277 [==============================] - 36s 8ms/step - loss: 0.0043 - val_loss: 1.0951e-04\n",
            "Epoch 13/21\n",
            "4277/4277 [==============================] - 41s 10ms/step - loss: 0.0038 - val_loss: 0.0200\n",
            "Epoch 14/21\n",
            "4277/4277 [==============================] - 35s 8ms/step - loss: 0.0578 - val_loss: 5.7616e-04\n",
            "Epoch 15/21\n",
            "4277/4277 [==============================] - 33s 8ms/step - loss: 0.0036 - val_loss: 2.5065e-04\n",
            "Epoch 16/21\n",
            "4277/4277 [==============================] - 35s 8ms/step - loss: 0.0025 - val_loss: 0.0171\n",
            "Epoch 17/21\n",
            "4277/4277 [==============================] - 48s 11ms/step - loss: 0.0064 - val_loss: 0.0011\n",
            "Epoch 18/21\n",
            "4277/4277 [==============================] - 33s 8ms/step - loss: 1.0458 - val_loss: 0.0035\n",
            "Epoch 19/21\n",
            "4277/4277 [==============================] - 35s 8ms/step - loss: 0.0275 - val_loss: 0.1069\n",
            "Epoch 20/21\n",
            "4277/4277 [==============================] - 35s 8ms/step - loss: 0.0851 - val_loss: 0.0011\n",
            "Epoch 21/21\n",
            "4277/4277 [==============================] - 44s 10ms/step - loss: 0.0017 - val_loss: 0.0021\n",
            "4277/4277 [==============================] - 11s 2ms/step\n",
            "4277/4277 [==============================] - 11s 2ms/step\n",
            "MLP Validation on Randomised Temp RMSE: 0.045873789757715125\n",
            "MLP Test on Randomised Temp RMSE: 0.046101674375602766\n"
          ]
        }
      ],
      "source": [
        "##Test temp feature\n",
        "training_results_temp = model.fit(x_train_temp,\n",
        "                             y_train,\n",
        "                             epochs=21,\n",
        "                             batch_size=64,\n",
        "                             validation_data=(x_val, y_val))\n",
        "\n",
        "y_val_pred_temp = model.predict(x_val)\n",
        "y_test_pred_temp = model.predict(x_test)\n",
        "\n",
        "mlp_val_rmse_temp = np.sqrt(mean_squared_error(y_val, y_val_pred_temp))\n",
        "mlp_test_rmse_temp = np.sqrt(mean_squared_error(y_test, y_test_pred_temp))\n",
        "\n",
        "print(f\"MLP Validation on Randomised Temp RMSE: {mlp_val_rmse_temp}\")\n",
        "print(f\"MLP Test on Randomised Temp RMSE: {mlp_test_rmse_temp}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##Test pressure feature\n",
        "training_results_pressure = model.fit(x_train_pressure,\n",
        "                             y_train,\n",
        "                             epochs=21,\n",
        "                             batch_size=64,\n",
        "                             validation_data=(x_val, y_val))\n",
        "\n",
        "y_val_pred_pressure = model.predict(x_val)\n",
        "y_test_pred_pressure = model.predict(x_test)\n",
        "\n",
        "mlp_val_rmse_pressure = np.sqrt(mean_squared_error(y_val, y_val_pred_pressure))\n",
        "mlp_test_rmse_pressure = np.sqrt(mean_squared_error(y_test, y_test_pred_pressure))\n",
        "\n",
        "print(f\"MLP Validation on Randomised Pressure RMSE: {mlp_val_rmse_pressure}\")\n",
        "print(f\"MLP Test on Randomised Pressure RMSE: {mlp_test_rmse_pressure}\")"
      ],
      "metadata": {
        "id": "_xGYM8MPHTTD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1826585-d754-4f83-92a9-32d3dbd9aad7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/21\n",
            "4277/4277 [==============================] - 36s 8ms/step - loss: 0.0025 - val_loss: 0.0034\n",
            "Epoch 2/21\n",
            "4277/4277 [==============================] - 34s 8ms/step - loss: 0.0011 - val_loss: 2.7510e-04\n",
            "Epoch 3/21\n",
            "4277/4277 [==============================] - 33s 8ms/step - loss: 0.0476 - val_loss: 2.5852e-04\n",
            "Epoch 4/21\n",
            "4277/4277 [==============================] - 47s 11ms/step - loss: 0.0033 - val_loss: 0.0025\n",
            "Epoch 5/21\n",
            "4277/4277 [==============================] - 39s 9ms/step - loss: 7.3047e-04 - val_loss: 8.7279e-05\n",
            "Epoch 6/21\n",
            "4277/4277 [==============================] - 32s 7ms/step - loss: 5.7022e-04 - val_loss: 3.0349e-04\n",
            "Epoch 7/21\n",
            "4277/4277 [==============================] - 35s 8ms/step - loss: 5.9879e-04 - val_loss: 5.6218e-05\n",
            "Epoch 8/21\n",
            "4277/4277 [==============================] - 43s 10ms/step - loss: 0.0129 - val_loss: 5.6098e-05\n",
            "Epoch 9/21\n",
            "4277/4277 [==============================] - 37s 9ms/step - loss: 4.4786e-04 - val_loss: 1.6004e-04\n",
            "Epoch 10/21\n",
            "4277/4277 [==============================] - 37s 9ms/step - loss: 0.0049 - val_loss: 3.2016e-05\n",
            "Epoch 11/21\n",
            "4277/4277 [==============================] - 35s 8ms/step - loss: 0.0041 - val_loss: 4.3651e-05\n",
            "Epoch 12/21\n",
            "4277/4277 [==============================] - 43s 10ms/step - loss: 4.1963e-04 - val_loss: 2.2627e-04\n",
            "Epoch 13/21\n",
            "4277/4277 [==============================] - 37s 9ms/step - loss: 3.1522e-04 - val_loss: 3.2467e-04\n",
            "Epoch 14/21\n",
            "4277/4277 [==============================] - 40s 9ms/step - loss: 3.4686e-04 - val_loss: 0.0112\n",
            "Epoch 15/21\n",
            "4277/4277 [==============================] - 39s 9ms/step - loss: 0.0140 - val_loss: 0.0011\n",
            "Epoch 16/21\n",
            "4277/4277 [==============================] - 50s 12ms/step - loss: 0.0016 - val_loss: 0.0016\n",
            "Epoch 17/21\n",
            "4277/4277 [==============================] - 39s 9ms/step - loss: 0.0017 - val_loss: 0.0017\n",
            "Epoch 18/21\n",
            "4277/4277 [==============================] - 34s 8ms/step - loss: 0.0017 - val_loss: 0.0023\n",
            "Epoch 19/21\n",
            "4277/4277 [==============================] - 36s 9ms/step - loss: 0.0017 - val_loss: 0.0015\n",
            "Epoch 20/21\n",
            "4277/4277 [==============================] - 53s 12ms/step - loss: 0.0026 - val_loss: 0.0012\n",
            "Epoch 21/21\n",
            "4277/4277 [==============================] - 39s 9ms/step - loss: 0.0017 - val_loss: 0.0030\n",
            "4277/4277 [==============================] - 12s 3ms/step\n",
            "4277/4277 [==============================] - 9s 2ms/step\n",
            "MLP Validation on Randomised Pressure RMSE: 0.05493083817861914\n",
            "MLP Test on Randomised Pressure RMSE: 0.0549060780470108\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##Test lat feature\n",
        "training_results_lat = model.fit(x_train_lat,\n",
        "                             y_train,\n",
        "                             epochs=21,\n",
        "                             batch_size=64,\n",
        "                             validation_data=(x_val, y_val))\n",
        "\n",
        "y_val_pred_lat = model.predict(x_val)\n",
        "y_test_pred_lat = model.predict(x_test)\n",
        "\n",
        "mlp_val_rmse_lat = np.sqrt(mean_squared_error(y_val, y_val_pred_lat))\n",
        "mlp_test_rmse_lat = np.sqrt(mean_squared_error(y_test, y_test_pred_lat))\n",
        "\n",
        "print(f\"MLP Validation on Randomised Lat RMSE: {mlp_val_rmse_lat}\")\n",
        "print(f\"MLP Test on Randomised Lat RMSE: {mlp_test_rmse_lat}\")"
      ],
      "metadata": {
        "id": "J2vVS0Y0IAAC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbbabb21-119c-44a2-ad48-a54fe4dbf5c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/21\n",
            "4277/4277 [==============================] - 42s 10ms/step - loss: 0.0410 - val_loss: 0.0014\n",
            "Epoch 2/21\n",
            "4277/4277 [==============================] - 37s 9ms/step - loss: 0.0017 - val_loss: 0.0027\n",
            "Epoch 3/21\n",
            "4277/4277 [==============================] - 41s 10ms/step - loss: 0.0019 - val_loss: 0.0167\n",
            "Epoch 4/21\n",
            "4277/4277 [==============================] - 42s 10ms/step - loss: 0.0180 - val_loss: 0.0074\n",
            "Epoch 5/21\n",
            "4277/4277 [==============================] - 43s 10ms/step - loss: 0.0131 - val_loss: 0.0032\n",
            "Epoch 6/21\n",
            "4277/4277 [==============================] - 36s 8ms/step - loss: 0.4772 - val_loss: 0.0042\n",
            "Epoch 7/21\n",
            "4277/4277 [==============================] - 37s 9ms/step - loss: 0.0022 - val_loss: 6.8538e-04\n",
            "Epoch 8/21\n",
            "4277/4277 [==============================] - 48s 11ms/step - loss: 0.0021 - val_loss: 0.0010\n",
            "Epoch 9/21\n",
            "4277/4277 [==============================] - 38s 9ms/step - loss: 0.0280 - val_loss: 3.9411e-04\n",
            "Epoch 10/21\n",
            "4277/4277 [==============================] - 40s 9ms/step - loss: 9.1591e-04 - val_loss: 0.0014\n",
            "Epoch 11/21\n",
            "4277/4277 [==============================] - 35s 8ms/step - loss: 0.0012 - val_loss: 1.5672e-04\n",
            "Epoch 12/21\n",
            "4277/4277 [==============================] - 48s 11ms/step - loss: 0.0011 - val_loss: 5.8047e-05\n",
            "Epoch 13/21\n",
            "4277/4277 [==============================] - 37s 9ms/step - loss: 8.5708e-04 - val_loss: 3.0270e-04\n",
            "Epoch 14/21\n",
            "4277/4277 [==============================] - 41s 10ms/step - loss: 8.1469e-04 - val_loss: 2.9529e-05\n",
            "Epoch 15/21\n",
            "4277/4277 [==============================] - 41s 10ms/step - loss: 0.0035 - val_loss: 4.1311e-05\n",
            "Epoch 16/21\n",
            "4277/4277 [==============================] - 40s 9ms/step - loss: 6.0593e-04 - val_loss: 3.2953e-04\n",
            "Epoch 17/21\n",
            "4277/4277 [==============================] - 36s 8ms/step - loss: 6.2419e-04 - val_loss: 6.7325e-04\n",
            "Epoch 18/21\n",
            "4277/4277 [==============================] - 37s 9ms/step - loss: 5.9826e-04 - val_loss: 0.0011\n",
            "Epoch 19/21\n",
            "4277/4277 [==============================] - 43s 10ms/step - loss: 6.4398e-04 - val_loss: 1.7815e-05\n",
            "Epoch 20/21\n",
            "4277/4277 [==============================] - 36s 8ms/step - loss: 5.5078e-04 - val_loss: 4.4027e-05\n",
            "Epoch 21/21\n",
            "4277/4277 [==============================] - 37s 9ms/step - loss: 5.4682e-04 - val_loss: 1.1302e-04\n",
            "4277/4277 [==============================] - 9s 2ms/step\n",
            "4277/4277 [==============================] - 10s 2ms/step\n",
            "MLP Validation on Randomised Lat RMSE: 0.010631077179936553\n",
            "MLP Test on Randomised Lat RMSE: 0.010647562778071676\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##Test lon feature\n",
        "training_results_lon = model.fit(x_train_lon,\n",
        "                             y_train,\n",
        "                             epochs=21,\n",
        "                             batch_size=64,\n",
        "                             validation_data=(x_val, y_val))\n",
        "\n",
        "y_val_pred_lon = model.predict(x_val)\n",
        "y_test_pred_lon = model.predict(x_test)\n",
        "\n",
        "mlp_val_rmse_lon = np.sqrt(mean_squared_error(y_val, y_val_pred_lon))\n",
        "mlp_test_rmse_lon = np.sqrt(mean_squared_error(y_test, y_test_pred_lon))\n",
        "\n",
        "print(f\"MLP Validation on Randomised Lon RMSE: {mlp_val_rmse_lon}\")\n",
        "print(f\"MLP Test on Randomised Lon RMSE: {mlp_test_rmse_lon}\")"
      ],
      "metadata": {
        "id": "jrJMfSfqILuN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fdf54ec-1536-40aa-cc66-038c497d1482"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/21\n",
            "4277/4277 [==============================] - 43s 10ms/step - loss: 0.1004 - val_loss: 0.0014\n",
            "Epoch 2/21\n",
            "4277/4277 [==============================] - 43s 10ms/step - loss: 5.2554e-04 - val_loss: 4.5069e-04\n",
            "Epoch 3/21\n",
            "4277/4277 [==============================] - 37s 9ms/step - loss: 5.3496e-04 - val_loss: 5.5885e-05\n",
            "Epoch 4/21\n",
            "4277/4277 [==============================] - 34s 8ms/step - loss: 5.3534e-04 - val_loss: 3.7677e-05\n",
            "Epoch 5/21\n",
            "4277/4277 [==============================] - 46s 11ms/step - loss: 0.0105 - val_loss: 2.1154e-04\n",
            "Epoch 6/21\n",
            "4277/4277 [==============================] - 39s 9ms/step - loss: 4.4208e-04 - val_loss: 6.4273e-05\n",
            "Epoch 7/21\n",
            "4277/4277 [==============================] - 39s 9ms/step - loss: 4.3791e-04 - val_loss: 4.0699e-04\n",
            "Epoch 8/21\n",
            "4277/4277 [==============================] - 39s 9ms/step - loss: 4.1662e-04 - val_loss: 8.4383e-04\n",
            "Epoch 9/21\n",
            "4277/4277 [==============================] - 44s 10ms/step - loss: 4.0002e-04 - val_loss: 9.9545e-04\n",
            "Epoch 10/21\n",
            "4277/4277 [==============================] - 36s 8ms/step - loss: 0.0065 - val_loss: 3.8377e-04\n",
            "Epoch 11/21\n",
            "4277/4277 [==============================] - 41s 10ms/step - loss: 3.0795e-04 - val_loss: 0.0013\n",
            "Epoch 12/21\n",
            "4277/4277 [==============================] - 40s 9ms/step - loss: 3.1309e-04 - val_loss: 9.6642e-05\n",
            "Epoch 13/21\n",
            "4277/4277 [==============================] - 40s 9ms/step - loss: 3.2307e-04 - val_loss: 6.9092e-05\n",
            "Epoch 14/21\n",
            "4277/4277 [==============================] - 36s 8ms/step - loss: 3.2259e-04 - val_loss: 6.2084e-05\n",
            "Epoch 15/21\n",
            "4277/4277 [==============================] - 40s 9ms/step - loss: 3.0829e-04 - val_loss: 6.2227e-06\n",
            "Epoch 16/21\n",
            "4277/4277 [==============================] - 37s 9ms/step - loss: 2.9044e-04 - val_loss: 5.2987e-04\n",
            "Epoch 17/21\n",
            "4277/4277 [==============================] - 42s 10ms/step - loss: 2.9465e-04 - val_loss: 1.4872e-05\n",
            "Epoch 18/21\n",
            "4277/4277 [==============================] - 37s 9ms/step - loss: 2.6451e-04 - val_loss: 3.5980e-06\n",
            "Epoch 19/21\n",
            "4277/4277 [==============================] - 39s 9ms/step - loss: 2.8041e-04 - val_loss: 0.0025\n",
            "Epoch 20/21\n",
            "4277/4277 [==============================] - 43s 10ms/step - loss: 2.4925e-04 - val_loss: 5.4953e-05\n",
            "Epoch 21/21\n",
            "4277/4277 [==============================] - 37s 9ms/step - loss: 0.0017 - val_loss: 6.2743e-04\n",
            "4277/4277 [==============================] - 8s 2ms/step\n",
            "4277/4277 [==============================] - 12s 3ms/step\n",
            "MLP Validation on Randomised Lon RMSE: 0.025048729709281636\n",
            "MLP Test on Randomised Lon RMSE: 0.025053657221873\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##Test height feature\n",
        "training_results_height = model.fit(x_train_height,\n",
        "                             y_train,\n",
        "                             epochs=21,\n",
        "                             batch_size=64,\n",
        "                             validation_data=(x_val, y_val))\n",
        "\n",
        "y_val_pred_height = model.predict(x_val)\n",
        "y_test_pred_height = model.predict(x_test)\n",
        "\n",
        "mlp_val_rmse_height = np.sqrt(mean_squared_error(y_val, y_val_pred_height))\n",
        "mlp_test_rmse_height = np.sqrt(mean_squared_error(y_test, y_test_pred_height))\n",
        "\n",
        "print(f\"MLP Validation on Randomised Height RMSE: {mlp_val_rmse_height}\")\n",
        "print(f\"MLP Test on Randomised Height RMSE: {mlp_test_rmse_height}\")"
      ],
      "metadata": {
        "id": "ZQgeUdQXIttM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0c4007a-fe6f-40e6-b17d-e46bc8f0e4af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/21\n",
            "4277/4277 [==============================] - 37s 9ms/step - loss: 2.4840e-04 - val_loss: 3.9153e-05\n",
            "Epoch 2/21\n",
            "4277/4277 [==============================] - 38s 9ms/step - loss: 2.3956e-04 - val_loss: 9.2493e-04\n",
            "Epoch 3/21\n",
            "4277/4277 [==============================] - 33s 8ms/step - loss: 2.4984e-04 - val_loss: 1.0956e-05\n",
            "Epoch 4/21\n",
            "4277/4277 [==============================] - 48s 11ms/step - loss: 2.3609e-04 - val_loss: 9.8531e-06\n",
            "Epoch 5/21\n",
            "4277/4277 [==============================] - 37s 9ms/step - loss: 2.3746e-04 - val_loss: 2.0323e-04\n",
            "Epoch 6/21\n",
            "4277/4277 [==============================] - 34s 8ms/step - loss: 2.4754e-04 - val_loss: 7.6307e-07\n",
            "Epoch 7/21\n",
            "4277/4277 [==============================] - 35s 8ms/step - loss: 2.1183e-04 - val_loss: 8.8246e-04\n",
            "Epoch 8/21\n",
            "4277/4277 [==============================] - 47s 11ms/step - loss: 2.2393e-04 - val_loss: 4.3215e-05\n",
            "Epoch 9/21\n",
            "4277/4277 [==============================] - 38s 9ms/step - loss: 2.2125e-04 - val_loss: 4.2395e-06\n",
            "Epoch 10/21\n",
            "4277/4277 [==============================] - 33s 8ms/step - loss: 2.1941e-04 - val_loss: 5.1619e-05\n",
            "Epoch 11/21\n",
            "4277/4277 [==============================] - 42s 10ms/step - loss: 2.0422e-04 - val_loss: 8.9236e-04\n",
            "Epoch 12/21\n",
            "4277/4277 [==============================] - 39s 9ms/step - loss: 2.1810e-04 - val_loss: 2.7895e-06\n",
            "Epoch 13/21\n",
            "4277/4277 [==============================] - 34s 8ms/step - loss: 2.0209e-04 - val_loss: 2.0115e-06\n",
            "Epoch 14/21\n",
            "4277/4277 [==============================] - 33s 8ms/step - loss: 2.0134e-04 - val_loss: 1.0665e-06\n",
            "Epoch 15/21\n",
            "4277/4277 [==============================] - 41s 10ms/step - loss: 2.0800e-04 - val_loss: 1.5168e-06\n",
            "Epoch 16/21\n",
            "4277/4277 [==============================] - 43s 10ms/step - loss: 1.8613e-04 - val_loss: 2.6404e-05\n",
            "Epoch 17/21\n",
            "4277/4277 [==============================] - 38s 9ms/step - loss: 1.9739e-04 - val_loss: 2.4117e-04\n",
            "Epoch 18/21\n",
            "4277/4277 [==============================] - 34s 8ms/step - loss: 1.9014e-04 - val_loss: 2.9000e-05\n",
            "Epoch 19/21\n",
            "4277/4277 [==============================] - 48s 11ms/step - loss: 2.0456e-04 - val_loss: 7.1530e-07\n",
            "Epoch 20/21\n",
            "4277/4277 [==============================] - 38s 9ms/step - loss: 1.7512e-04 - val_loss: 2.5482e-05\n",
            "Epoch 21/21\n",
            "4277/4277 [==============================] - 41s 10ms/step - loss: 1.7453e-04 - val_loss: 3.3696e-06\n",
            "4277/4277 [==============================] - 10s 2ms/step\n",
            "4277/4277 [==============================] - 12s 3ms/step\n",
            "MLP Validation on Randomised Height RMSE: 0.0018357519303910381\n",
            "MLP Test on Randomised Height RMSE: 0.0018383198369107076\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##Test ice_conc feature\n",
        "training_results_ice_conc = model.fit(x_train_ice_conc,\n",
        "                             y_train,\n",
        "                             epochs=21,\n",
        "                             batch_size=64,\n",
        "                             validation_data=(x_val, y_val))\n",
        "\n",
        "y_val_pred_ice_conc = model.predict(x_val)\n",
        "y_test_pred_ice_conc = model.predict(x_test)\n",
        "\n",
        "mlp_val_rmse_ice_conc = np.sqrt(mean_squared_error(y_val, y_val_pred_ice_conc))\n",
        "mlp_test_rmse_ice_conc = np.sqrt(mean_squared_error(y_test, y_test_pred_ice_conc))\n",
        "\n",
        "print(f\"MLP Validation on Randomised Ice Conc RMSE: {mlp_val_rmse_ice_conc}\")\n",
        "print(f\"MLP Test on Randomised Ice Conc RMSE: {mlp_test_rmse_ice_conc}\")"
      ],
      "metadata": {
        "id": "m2r2L1WGI8Nb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0cedfb36-7e48-4157-de93-bed2e2851ac9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/21\n",
            "4277/4277 [==============================] - 39s 9ms/step - loss: 0.6277 - val_loss: 9.7297e-05\n",
            "Epoch 2/21\n",
            "4277/4277 [==============================] - 37s 9ms/step - loss: 3.7065e-04 - val_loss: 0.0019\n",
            "Epoch 3/21\n",
            "4277/4277 [==============================] - 44s 10ms/step - loss: 3.5497e-04 - val_loss: 1.2488e-04\n",
            "Epoch 4/21\n",
            "4277/4277 [==============================] - 35s 8ms/step - loss: 3.0512e-04 - val_loss: 3.4935e-05\n",
            "Epoch 5/21\n",
            "4277/4277 [==============================] - 38s 9ms/step - loss: 0.2329 - val_loss: 0.0364\n",
            "Epoch 6/21\n",
            "4277/4277 [==============================] - 41s 9ms/step - loss: 0.0773 - val_loss: 2.2204e-05\n",
            "Epoch 7/21\n",
            "4277/4277 [==============================] - 50s 12ms/step - loss: 2.6611e-04 - val_loss: 8.0208e-06\n",
            "Epoch 8/21\n",
            "4277/4277 [==============================] - 40s 9ms/step - loss: 2.4590e-04 - val_loss: 0.0014\n",
            "Epoch 9/21\n",
            "4277/4277 [==============================] - 40s 9ms/step - loss: 2.4376e-04 - val_loss: 2.7961e-05\n",
            "Epoch 10/21\n",
            "4277/4277 [==============================] - 48s 11ms/step - loss: 2.3412e-04 - val_loss: 4.3018e-05\n",
            "Epoch 11/21\n",
            "4277/4277 [==============================] - 37s 9ms/step - loss: 2.2470e-04 - val_loss: 4.4704e-05\n",
            "Epoch 12/21\n",
            "4277/4277 [==============================] - 39s 9ms/step - loss: 2.1908e-04 - val_loss: 1.5178e-05\n",
            "Epoch 13/21\n",
            "4277/4277 [==============================] - 42s 10ms/step - loss: 2.1575e-04 - val_loss: 1.0545e-04\n",
            "Epoch 14/21\n",
            "4277/4277 [==============================] - 46s 11ms/step - loss: 0.0046 - val_loss: 9.8482e-07\n",
            "Epoch 15/21\n",
            "4277/4277 [==============================] - 33s 8ms/step - loss: 1.1096e-04 - val_loss: 6.1552e-06\n",
            "Epoch 16/21\n",
            "4277/4277 [==============================] - 37s 9ms/step - loss: 1.9796e-04 - val_loss: 5.3845e-07\n",
            "Epoch 17/21\n",
            "4277/4277 [==============================] - 51s 12ms/step - loss: 1.8911e-04 - val_loss: 1.5299e-06\n",
            "Epoch 18/21\n",
            "4277/4277 [==============================] - 38s 9ms/step - loss: 1.8727e-04 - val_loss: 4.8540e-05\n",
            "Epoch 19/21\n",
            "4277/4277 [==============================] - 34s 8ms/step - loss: 2.0131e-04 - val_loss: 4.2344e-07\n",
            "Epoch 20/21\n",
            "4277/4277 [==============================] - 43s 10ms/step - loss: 1.7644e-04 - val_loss: 5.6365e-04\n",
            "Epoch 21/21\n",
            "4277/4277 [==============================] - 42s 10ms/step - loss: 1.7718e-04 - val_loss: 4.4452e-05\n",
            "4277/4277 [==============================] - 9s 2ms/step\n",
            "4277/4277 [==============================] - 8s 2ms/step\n",
            "MLP Validation on Randomised Ice Conc RMSE: 0.006667009677699084\n",
            "MLP Test on Randomised Ice Conc RMSE: 0.006666484594917919\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##Test windspeed feature\n",
        "training_results_windspeed = model.fit(x_train_windspeed,\n",
        "                             y_train,\n",
        "                             epochs=21,\n",
        "                             batch_size=64,\n",
        "                             validation_data=(x_val, y_val))\n",
        "\n",
        "y_val_pred_windspeed = model.predict(x_val)\n",
        "y_test_pred_windspeed = model.predict(x_test)\n",
        "\n",
        "mlp_val_rmse_windspeed = np.sqrt(mean_squared_error(y_val, y_val_pred_windspeed))\n",
        "mlp_test_rmse_windspeed = np.sqrt(mean_squared_error(y_test, y_test_pred_windspeed))\n",
        "\n",
        "print(f\"MLP Validation on Randomised Windspeed RMSE: {mlp_val_rmse_windspeed}\")\n",
        "print(f\"MLP Test on Randomised Windspeed RMSE: {mlp_test_rmse_windspeed}\")"
      ],
      "metadata": {
        "id": "5p5rwTwEJI4s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cfeaa775-8b7f-4096-9dfc-e817ba463f44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/21\n",
            "4277/4277 [==============================] - 36s 8ms/step - loss: 0.7682 - val_loss: 3.1068e-04\n",
            "Epoch 2/21\n",
            "4277/4277 [==============================] - 53s 12ms/step - loss: 3.8868e-04 - val_loss: 1.6824e-04\n",
            "Epoch 3/21\n",
            "4277/4277 [==============================] - 38s 9ms/step - loss: 3.0395e-04 - val_loss: 0.0020\n",
            "Epoch 4/21\n",
            "4277/4277 [==============================] - 36s 8ms/step - loss: 2.6079e-04 - val_loss: 3.7740e-05\n",
            "Epoch 5/21\n",
            "4277/4277 [==============================] - 39s 9ms/step - loss: 2.2547e-04 - val_loss: 6.2510e-05\n",
            "Epoch 6/21\n",
            "4277/4277 [==============================] - 42s 10ms/step - loss: 1.0190 - val_loss: 3.5819e-04\n",
            "Epoch 7/21\n",
            "4277/4277 [==============================] - 41s 10ms/step - loss: 1.3762e-04 - val_loss: 1.5526e-04\n",
            "Epoch 8/21\n",
            "4277/4277 [==============================] - 39s 9ms/step - loss: 3.2380e-04 - val_loss: 1.1566e-05\n",
            "Epoch 9/21\n",
            "4277/4277 [==============================] - 44s 10ms/step - loss: 3.2294e-04 - val_loss: 7.3878e-04\n",
            "Epoch 10/21\n",
            "4277/4277 [==============================] - 39s 9ms/step - loss: 0.5546 - val_loss: 4.7677e-05\n",
            "Epoch 11/21\n",
            "4277/4277 [==============================] - 38s 9ms/step - loss: 0.1414 - val_loss: 1.7160e-04\n",
            "Epoch 12/21\n",
            "4277/4277 [==============================] - 39s 9ms/step - loss: 3.4321e-04 - val_loss: 1.0886e-04\n",
            "Epoch 13/21\n",
            "4277/4277 [==============================] - 43s 10ms/step - loss: 3.3436e-04 - val_loss: 6.3574e-04\n",
            "Epoch 14/21\n",
            "4277/4277 [==============================] - 40s 9ms/step - loss: 2.9725e-04 - val_loss: 0.0011\n",
            "Epoch 15/21\n",
            "4277/4277 [==============================] - 40s 9ms/step - loss: 2.8988e-04 - val_loss: 9.3617e-06\n",
            "Epoch 16/21\n",
            "4277/4277 [==============================] - 49s 11ms/step - loss: 2.5926e-04 - val_loss: 5.0588e-04\n",
            "Epoch 17/21\n",
            "4277/4277 [==============================] - 40s 9ms/step - loss: 2.5893e-04 - val_loss: 1.0153e-04\n",
            "Epoch 18/21\n",
            "4277/4277 [==============================] - 35s 8ms/step - loss: 2.4965e-04 - val_loss: 5.0407e-05\n",
            "Epoch 19/21\n",
            "4277/4277 [==============================] - 46s 11ms/step - loss: 2.3217e-04 - val_loss: 9.6763e-06\n",
            "Epoch 20/21\n",
            "4277/4277 [==============================] - 40s 9ms/step - loss: 0.0782 - val_loss: 1.8672e-05\n",
            "Epoch 21/21\n",
            "4277/4277 [==============================] - 43s 10ms/step - loss: 6.9815e-05 - val_loss: 1.1327e-04\n",
            "4277/4277 [==============================] - 13s 3ms/step\n",
            "4277/4277 [==============================] - 13s 3ms/step\n",
            "MLP Validation on Randomised Windspeed RMSE: 0.010643138408271852\n",
            "MLP Test on Randomised Windspeed RMSE: 0.010646009687240308\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##Test precip feature\n",
        "training_results_precip = model.fit(x_train_precip,\n",
        "                             y_train,\n",
        "                             epochs=21,\n",
        "                             batch_size=64,\n",
        "                             validation_data=(x_val, y_val))\n",
        "\n",
        "y_val_pred_precip = model.predict(x_val)\n",
        "y_test_pred_precip = model.predict(x_test)\n",
        "\n",
        "mlp_val_rmse_precip = np.sqrt(mean_squared_error(y_val, y_val_pred_precip))\n",
        "mlp_test_rmse_precip = np.sqrt(mean_squared_error(y_test, y_test_pred_precip))\n",
        "\n",
        "print(f\"MLP Validation on Randomised Precip RMSE: {mlp_val_rmse_precip}\")\n",
        "print(f\"MLP Test on Randomised Precip RMSE: {mlp_test_rmse_precip}\")"
      ],
      "metadata": {
        "id": "EwOjgc6UJXbY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c35585a7-23a9-4d5e-9e1e-f24b82d3e469"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/21\n",
            "4277/4277 [==============================] - 44s 10ms/step - loss: 0.8697 - val_loss: 0.0182\n",
            "Epoch 2/21\n",
            "4277/4277 [==============================] - 40s 9ms/step - loss: 0.7566 - val_loss: 0.0042\n",
            "Epoch 3/21\n",
            "4277/4277 [==============================] - 36s 9ms/step - loss: 0.2811 - val_loss: 3.7872e-05\n",
            "Epoch 4/21\n",
            "4277/4277 [==============================] - 48s 11ms/step - loss: 0.3589 - val_loss: 9.7501e-04\n",
            "Epoch 5/21\n",
            "4277/4277 [==============================] - 38s 9ms/step - loss: 1.7572e-04 - val_loss: 2.6539e-04\n",
            "Epoch 6/21\n",
            "4277/4277 [==============================] - 40s 9ms/step - loss: 3.4112e-04 - val_loss: 1.0973e-04\n",
            "Epoch 7/21\n",
            "4277/4277 [==============================] - 42s 10ms/step - loss: 3.8020e-04 - val_loss: 7.6738e-05\n",
            "Epoch 8/21\n",
            "4277/4277 [==============================] - 39s 9ms/step - loss: 3.0888e-04 - val_loss: 1.0878e-04\n",
            "Epoch 9/21\n",
            "4277/4277 [==============================] - 38s 9ms/step - loss: 2.7680e-04 - val_loss: 1.5972e-05\n",
            "Epoch 10/21\n",
            "4277/4277 [==============================] - 38s 9ms/step - loss: 3.2404e-04 - val_loss: 7.7878e-05\n",
            "Epoch 11/21\n",
            "4277/4277 [==============================] - 49s 12ms/step - loss: 2.4263e-04 - val_loss: 1.0842e-04\n",
            "Epoch 12/21\n",
            "4277/4277 [==============================] - 33s 8ms/step - loss: 2.2924e-04 - val_loss: 4.4034e-05\n",
            "Epoch 13/21\n",
            "4277/4277 [==============================] - 37s 9ms/step - loss: 2.1130e-04 - val_loss: 5.7277e-04\n",
            "Epoch 14/21\n",
            "4277/4277 [==============================] - 42s 10ms/step - loss: 2.2098e-04 - val_loss: 3.3045e-06\n",
            "Epoch 15/21\n",
            "4277/4277 [==============================] - 44s 10ms/step - loss: 1.5545 - val_loss: 0.0035\n",
            "Epoch 16/21\n",
            "4277/4277 [==============================] - 39s 9ms/step - loss: 2.2658e-04 - val_loss: 6.4999e-06\n",
            "Epoch 17/21\n",
            "4277/4277 [==============================] - 39s 9ms/step - loss: 2.7037e-04 - val_loss: 3.0398e-05\n",
            "Epoch 18/21\n",
            "4277/4277 [==============================] - 45s 10ms/step - loss: 2.5844e-04 - val_loss: 7.8190e-06\n",
            "Epoch 19/21\n",
            "4277/4277 [==============================] - 38s 9ms/step - loss: 2.2310e-04 - val_loss: 2.4554e-04\n",
            "Epoch 20/21\n",
            "4277/4277 [==============================] - 41s 10ms/step - loss: 2.1343e-04 - val_loss: 4.3118e-06\n",
            "Epoch 21/21\n",
            "4277/4277 [==============================] - 43s 10ms/step - loss: 2.0747e-04 - val_loss: 2.5599e-05\n",
            "4277/4277 [==============================] - 11s 3ms/step\n",
            "4277/4277 [==============================] - 8s 2ms/step\n",
            "MLP Validation on Randomised Precip RMSE: 0.005059791185778849\n",
            "MLP Test on Randomised Precip RMSE: 0.005047179159307311\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##RMSE ratings\n",
        "mlp_temp_rmse = mlp_test_rmse_temp / mlp_test_rmse\n",
        "mlp_pressure_rmse =  mlp_test_rmse_pressure / mlp_test_rmse\n",
        "mlp_lat_rmse =  mlp_test_rmse_lat / mlp_test_rmse\n",
        "mlp_lon_rmse =  mlp_test_rmse_lon / mlp_test_rmse\n",
        "mlp_height_rmse =  mlp_test_rmse_height / mlp_test_rmse\n",
        "mlp_ice_conc_rmse =  mlp_test_rmse_ice_conc / mlp_test_rmse\n",
        "mlp_windspeed_rmse =  mlp_test_rmse_windspeed / mlp_test_rmse\n",
        "mlp_precip_rmse =  mlp_test_rmse_precip / mlp_test_rmse\n"
      ],
      "metadata": {
        "id": "Tj0PUBHEJjpL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(mlp_temp_rmse)\n",
        "print(mlp_pressure_rmse)\n",
        "print(mlp_lat_rmse)\n",
        "print(mlp_lon_rmse)\n",
        "print(mlp_height_rmse)\n",
        "print(mlp_ice_conc_rmse)\n",
        "print(mlp_windspeed_rmse)\n",
        "print(mlp_precip_rmse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IrTJV7CC7RpY",
        "outputId": "654da417-66d9-4aa0-a479-da7aec4c44da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.719992827423478\n",
            "3.2394515056671507\n",
            "0.628204827224727\n",
            "1.4781625367664317\n",
            "0.1084606326913272\n",
            "0.3933217291539809\n",
            "0.6281131950664839\n",
            "0.29778291782179234\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lEbT58Ik7c_2"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}